{
  "hash": "31eaf58b60dc11b6367e00ed12493bab",
  "result": {
    "markdown": "---\ntitle: \"Standardize Your Way to Causal Inference\" \nsubtitle: \"Standardization and the Parametric G-Formula\"\nauthor: \"Ryan Batten\"\ndate: \"2023-05-25\"\ncategories: [Standardization, Parametric G-Formula]\nimage : \"standards.png\"\nbibliography: standardize.bib\ndraft: true\nformat: \n  html:\n    toc: true\n    toc-title: Contents\n    toc-location: right\n    toc-depth: 4\n    code-fold: true\n---\n\n\n## Comparing Apples and Oranges\n\nImagine that we have two groups of people that we are comparing the a*verage length of plants* grown: group A and group O. Group A are comprised of kids in grade 1 while group O are the parents. The kids (Group A) received a lesson about growing plants, while the parents (group O) did not. After receiving the lesson, each person got a plant and grew it. After 12 weeks, the plants length was measured. \\\n\\\nWhen the teacher looks at the plants grown, they conclude that group O are better at growing plants. That's when a parent runs up huffing and puffing \"You can't compare those! That's like comparing apples and oranges\" when a kid from group A chimes in \"If we didn't water them as much then we'd have done way better than them!\"\\\n\\\nNow they have a point, watering a plant will certainly have an impact on how the plants grow, as will the sunlight, soil and other important factors. \"So what do we do? Throw out this experiment?\" the teacher asks the parent. \"Not so fast! We can standardize these and then compare the groups!\" you exclaim. So how exactly do we standardize to compare Groups A and O?\n\n### How do we standardize?\n\nAlright so we have our question, \"Does getting a lesson in plant growing cause your plants to grow longer?\", but how exactly do we apply that in this case? Well first we need to have a look at the data and see what we have.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nset.seed(1513) # picked based on runif(1, min = 0, max = 2023)\n\ngrp_a <- data.frame(\n  age = rnorm(n = 116, mean = 10, sd = 3),\n  indoors = rbinom(n = 116, size = 1, prob = 0.3), \n  hrs_sunlight = runif(n = 116, min = 2, max = 6), \n  watered = rbinom(n = 116, size = 1, prob = 0.4),\n  group = 1\n) \n\ngrp_o <- data.frame(\n  age = rnorm(n = 116, mean = 30, sd = 7),\n  indoors = rbinom(n = 116, size = 1, prob = 0.8), \n  hrs_sunlight = runif(n = 116, min = 8, max = 12), \n  watered = rbinom(n = 116, size = 1, prob = 0.6),\n  group = 0\n) \n\ndf = rbind(grp_a, grp_o) |> \n  dplyr::mutate(\n    plant_length = rnorm(n = 232, mean = 1.5*group + 0.2*age + 0.3*indoors + 0.8*hrs_sunlight - 0.5*watered, sd = 1)\n  ) \n```\n:::\n\n\n+------------------------------+---------------+---------------+\n| Characteristic               | Group A       | Group O       |\n|                              |               |               |\n|                              | (n = 116)     | (n = 116)     |\n+:============================:+:=============:+:=============:+\n| Age, Mean (SD)               | 10 (2.74)     | 30.04 (6.12)  |\n+------------------------------+---------------+---------------+\n| Indoors, n (%)               | 34, (30.2%)   | 99 (85.3%)    |\n+------------------------------+---------------+---------------+\n| Hours in Sunlight, Mean (SD) | 3.77 (1.23)   | 10.02 (1.18)  |\n+------------------------------+---------------+---------------+\n| Adequately watered, n (%)    | 46, (39.7%)   | 59, (50.9%)   |\n+------------------------------+---------------+---------------+\n| Length of plant, Mean (SD)   | 6.52 (1.58)   | 14.2 (1.89)   |\n+------------------------------+---------------+---------------+\n\nAlright so now we have our data and know the age of each person, how many plants were grown indoors in each group, the hours in the sunlight of each plant, whether the plant was adequately watered and the length of each plant. So what? Well we need to standardize these groups. \\\n\\\nYou may be thinking of standardization in terms of other methods, that are not necessarily statistical. For example, say you weighted three rocks: one is 110 kilogram, one is 167 pounds and one is 25 stones, how could you possible compare these three? They are in different units! In this situation, you might be thinking \"Well, we just convert them all to the same units duh\"...exactly! You are standardizing the units. In this case we are doing something similar, except with a regression model.\n\n## Standardization for Causal Inference\n\nInverse probability weighting (IPW) is a commonly used method for causal inference however standardization is one alternative. I won't bore you here with extraneous details about IPW but you can find more information about IPW in another of my blog posts.\\\n\\\nBriefly, IPW uses a model for the treatment (aka the propensity score) whereas standardization models the outcome [@hernanwhatif]. More formally, if we were to write it as an equation, assuming that no individuals are censored (C=0) [@hernanwhatif, pp. 162] :\n\n$$\n{\\sum_{l}E[Y|A = a, C=0, L=l]}  \\times Pr[L=l]\n$$\n\nNow, in an ideal world we'd be able to calculate this nonparametrically. To do that, we could calculate the mean outcome in each stratum $l$ of the confounders $L$. So in our case we could look at one strata as the individuals who are 5 years old, grew their plant indoor, had their plant in the sunlight for 2 hours and adequately watered their plant. We could do this for each strata, then take the weighted mean sum using the above formula.\n\n::: callout-note\n## What if L is continuous?\n\nIf $L$ is continuous in the above formula, then we need to replace $Pr[L=l]$ with the probability density function $f_{L}[l]$ [@hernanwhatif, pp. 162].\n:::\n\nNow, as you can imagine that is a lot of work especially when the variable is continuous. You can probably imagine that this isn't always possible, especially when dealing with real-world data or many covariates. As the number of covariates increases, so does the number of strata. Lucky for us, this is where modelling can come in handy!\n\n## Standardizing the Mean Outcome to the Confounder Distribution\n\nFirstly, we need to go over the four steps, very briefly, involved in standardizing the mean outcome. The steps are [@hernanwhatif, pp. 164]:\n\n1.  Expansion of data set\n\n2.  Modelling\n\n3.  Prediction\n\n4.  Standardization by averaging\n\n### Expansion of data set\n\nFor the first step, we expand the dataset by creating three total datasets. The first will be our original dataset, the second is a dataset where the treatment is set to 0 and we have a missing outcome while the third is one where the treatment is set to 1 and we have a missing outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_og <- df\n\ndf_grp0 <- df |> \n  dplyr::select(\n    -plant_length, -group\n  ) |> \n  dplyr::mutate(\n    group = 0\n  )\n\ndf_grp1 <- df |> \n  dplyr::select(\n    -plant_length, -group\n  ) |> \n  dplyr::mutate(\n    group = 1\n  )\n```\n:::\n\n\n### Modelling\n\nFor this step, we will use the first dataset (the one where we have the outcomes). We will fit a model, including the covariates we want to adjust for (in this case, those that are confounders).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- glm(\n  plant_length ~ group + age + indoors + hrs_sunlight + watered, \n  family = gaussian(link = \"identity\"), \n  data = df\n)\n```\n:::\n\n\n### Prediction\n\nUsing our handy dandy model, we will now predict the outcome for each of the datasets. First we predict the outcome for the data set if everyone were in group O. Next, we predict the outcome for the data set if everyone were in group A.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred0 <- predict(fit, df_grp0) |> as.data.frame() |> \n  rename(plant_length = `predict(fit, df_grp0)`)\n\npred1 <- predict(fit, df_grp1) |> as.data.frame() |> \n  rename(plant_length = `predict(fit, df_grp1)`)\n```\n:::\n\n\n### Standardization by averaging\n\nNow that we have predicted the outcomes, we can calculate the average outcome for each data set. Before we calculate this, you might be asking about the uncertainty for this measurement, which I'm glad you asked! \\\n\\\nTo get 95% confidence intervals for this, we can use bootstrapping (with R code that complements the Causal Inference Book, a special thanks to Joy Shi, Sean McGrath and Tom Palmer for providing this). \\\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A special thanks to Joy Shi, Sean McGrath and Tom Palmer for providing this code for free. Link here: https://remlapmot.github.io/cibookex-r/standardization-and-the-parametric-g-formula.html#program-13.4\n\nlibrary(boot)\n\n# Function, altered from the above link for our use \n\nstandardization <- function(data, indices) {\n  \n  # We need to first make the datasets that we need\n  \n  # 1st copy: our original\n  \n  d <- data[indices, ] \n  \n  # 2nd copy: Same as original but with group = 0 and outcome as missing\n  \n  d0 <- d \n  d0$group <- 0 # setting group to 0\n  d0$plant_length <- NA # setting plant length (our outcome) to missing \n  \n  # 3rd copy: Same as original but with group = 1 and outcome as missing\n  \n  d1 <- d \n  d1$group <- 1 # setting group to 1\n  d1$plant_length <- NA # setting plant length (our outcome) to missing \n  \n  # Making one sample\n  \n  d.onesample <- rbind(d, d0, d1) # combining datasets\n  \n  # Fitting a model for each iteration\n  \n  fit <- glm(\n    plant_length ~ group + age + indoors + hrs_sunlight + watered,\n    data = d.onesample\n  )\n  \n  # Using model to predict the outcome\n  \n  d.onesample$predicted_meanY <- predict(fit, d.onesample)\n  \n  # Calculate the mean for each of the groups. The third calculation is for the difference in group O (0) vs group A (1)\n  \n  return(c(\n    mean(d.onesample$predicted_meanY[d.onesample$group == 0]),\n    mean(d.onesample$predicted_meanY[d.onesample$group == 1]),\n    # Treatment - No Treatment\n    \n    mean(d.onesample$predicted_meanY[d.onesample$group == 1]) -\n    mean(d.onesample$predicted_meanY[d.onesample$group == 0])\n  ))\n}\n\n\n# Now we can bootstrap this statistic using our dataset\n\nresults <- boot(data = df,\n                statistic = standardization,\n                R = 5)\n\n# Using the bootstrapped sample (titled result), we can calculate the confidence intervals. We take the standard deviation of the sampling distribution. This will give us our standard error\n\nse <- c(sd(results$t[, 1]),\n        sd(results$t[, 2]),\n        sd(results$t[,3]))\n\n\nmean <- results$t0 # calculate mean \nll <- mean - qnorm(0.975) * se # lower limits\nul <- mean + qnorm(0.975) * se # upper limits\n\nfinalresults <- data.frame(\n  result_title = c(\"Group O\", \"Group A\", \"Difference\"),\n  mean = round(mean,3),\n  ci = paste0(\"95% CI: \", round(ll, 3), \" - \", round(ul, 3))\n)\n```\n:::\n\n\nFinally we have our results! We end up with a mean difference of -1.43 (95% -0.96 to -1.90). Finally, we can put to rest that the people being put in Group O grew longer plants than those in group A (or those in group A grew shorter plants than those in Group O). Now begs the question of when are these valid?\n\n## Assumptions for Standardization\n\nSo when are these valid? We can group the assumptions into three groups: identifability conditions, measurement of variables and specification of model [@hernanwhatif, pp. 168]. The idenifability conditions are exchangeability, positivity and consistency. Positivity can be checked empirically but the other two are opinion based. For a more detailed version of these, check out my other blog post.\n\nThe second condition is the variables used in the analysis need to be correctly measured. Measurement error in the treatment, outcome or the confounders will generally result in bias (see chapter 9 of @hernanwhatif for more specifics). \\\n\\\nFinally, all models need to be correctly specified (see chapter 11 of @hernanwhatif for more specifics). Of course, all of these will never hold perfectly but some remain a matter of judgement which means it can be open to criticism. It's important to keep these assumptions in mind and the validity of them. For example, a critique in our example could be that our intervention is not well-defined (which I'd agree with and we could make our intervention definition more defined).\n\nWe need to make sure all of these conditions hold, at least in approximately since in the real-world it is difficult (i.e., in practice there is most likely some form of model misspecification). Some of these assumptions are based on judgement, which is important to note.\n\n::: callout-note\n## Positivity Assumption\n\nWhile both IP weighting and standardization require structural positivity, the implications of this assumptions not being valid can vary. For standardization, it is possible to use if this is assumption isn't met however there then needs to be a willingness to rely on parametric extrapolation (this can be done to fit a model that will smooth over the strata with structural zeroes) however this will introduce bias into the estimation. This will result in the nomial 95% confidence intervals around the estimates covering the true effect less than 95% of the time. See @hernanwhatif, pp. 162 for more details.\n:::\n\n## What about other measures? \n\nOur example used a continuous outcome, so we used risk difference as our causal estimand. Of course, there are other causal estimands of interest as well. I won't bore you with another example here (maybe in the future). I'd highly recommend checking out @lee2022 if you are interested in other estimands including relative risk and odds ratios. The process is very similar.\n\n## IP Weighting or Standardization?\n\nIf we were to do both without using any models (i.e., nonparametrically), then we would expect both methods to give the exact same result. This is because they are modelling different things (treatment for IPW, outcome for standardizaiton) and we can always expect some level of misspecification of a model in practice. So if they don't give the same answer then how do we pick? The short answer is we don't.\\\n\\\nLarge differences between them will let us know that there is some serious misspecificaiton in at least one of the estimates. Small differences may still indicate there's a problem but not as serious misspecification.\\\n\\\nBasically what I'm trying to say is when both methods can be used, just use both.\n\n::: callout-note\n## G-formula\n\nFun fact: both IPW and standardization are estimators of the g-formula. Standardization is called a *plug-in g-formula* estimator because it replaces the conditional mean outcome by its estimates. When those estimates come from parametric models, we refer to the method as the *parametric g-formula*.\\\n\\\nNote: when there is no time-varying confounders, the parametric g-formula doesn't require parametric modelling of the distribution of the confounders.\n:::\n\nBoth IP weighting and standardization will give the exact same result. So then why would we choose one over the other? Well, they only give the same answer when no models are used to estimate them.\\\n\\\nAt this point, you may be thinking \"Well that's not really an answer\" so lets try and explain. When you use a model, you are using two different models (in our case) with two different outcomes. For IP weighting, a logistic regression model is fit to estimate $Pr[A = a, C= 0|L]$ whereas in standardization, the conditional mean $E[Y|A=a C = 0, L = l]$\n\nHowever, some degree of misspecification is a part of all models, which will introduce some bias. Due to this, the estimates from IP weighting and standardization will differ, but large differences will alert us to potentially serious model misspecification.\n\n## The G-Formula\n\nEnough teasing already! What does the parametric g-formula have to do with this!! You may be screaming at your screen right now. Alright, so first the equation for the *g-formula* is formally expressed as [@naimi2017]:\n\n$$\nE(Y^{a_0, a_1}) = \\sum_{z_1}E(Y|A_1 = a_1, Z_1 = z1, A_0 = a_0)P(Z_1=z_1|A_0=a_0)\n$$\n\nAt this point you may be wondering \"What in the world does this formula have to do with standardization?\". If you are thinking that, props to you! (side note: I personally always like to ask these types of questions). Well, when we just input the values to this equation, we say that we are using a *plug-in g-formula* estimator since we just...you guessed it....plug in the values! Standardization is a *plug-in g-formula* estimator however since, in our case, the estimate came from a parametric model we call it the *parametric g-formula.*\n\n## What Next?\n\nNow you are free to go standardize away out there in the free world!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}