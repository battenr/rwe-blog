{
  "hash": "90e292d82c580339a4e8f74b6313c44f",
  "result": {
    "markdown": "---\nttitle: \"Inverse Probability Weighting\"\nauthor: \"Ryan Batten\"\ndate: \"2022-08-13\"\ncategories: [IPW, IPTW]\nimage: \"iptw.jpeg\"\nbibliography: ip_weighting.bib\ndraft: true\n---\n\n\n## Re-weighting? Like a scale?\n\nRe-weighting in this context has nothing to do with weight. Instead it is a statistical method that is used to adjust for effect modification and confounding to ensure exchangeability. This method can be helpful for answering questions about the ***marginal or conditional*** causal effect.\n\n::: callout-note\nThis post will draw heavily from Chapter 12 of What If [@hernanwhatif]\n:::\n\n## Propensity Score\n\nThe propensity score is the conditional probability of receiving treatment [@hernanwhatif], or in mathematical notation:\n\n\n$$\nPr[A = 1| L = l]\n$$\n\n\nWhere A is treatment and L is covariates.\n\n## Pseudo-Population\n\nThe goal of reweighing is to make a pseudo-population where exchangeability holds. To do this, we fit a logistic regression where the outcome is treatment.\n\n## General Formula for IP Weighting\n\nThe below formula is from [@hernanwhatif, pp.153].\n\n\n$$\nW^a = \\frac{p}{f[A|L]}\n$$\n\n\nwhere $p$ is the unconditional probability of treatment. Note: $0< p \\leq 1$, whereas $f[A|L]$ is the probability of treatment based on covariates L. A common choice is to use $Pr[A = 1]$ for $p$ in the treated and $Pr[A=0]$ for $p$ in the untreated. $Pr$ in this case means the proportion. More compactly:\n\n\n$$\n\\frac{f(A)}{f[A|L]}\n$$\n\n\n## Stabilized Weight\n\n\n$$\nSW^a = \\frac{f(A)}{f[A|L]}\n$$\n\n\nMean of the stabilized weights should be 1. This is important to check when conducting an analysis using stabilized weights.\n\n## Stabilized or Nonstabilized?\n\nAt this point, you may be wondering to yourself if we should be using stabilized or nonstabilized weights. One reason is stabilized weights result in narrower 95% CIs. You should use stabilized weights when your model is not saturated (i.e., there are the same amount of unknowns on both sides of the equations)\n\n## Example\n\nAn example always helps. Let's jump right into our made up dataset. We have 5 variables: sex, sunglasses, age, love for disney princess and ice cream eaters. Now for this dataset, we want to know if loving Disney movie causes people to become ice cream lovers. Below is a summary table of\n\n\n\n\n\n+-------------------------+---------------+------------------+\n| Variables               | Disney Lover  | Not a Disney Fan |\n|                         |               |                  |\n|                         | (n = 577)     | ( n = 206)       |\n+:=======================:+:=============:+:================:+\n| Age, mean (SD)          | 31.8 (16.3)   | 54.1 (16.0)      |\n+-------------------------+---------------+------------------+\n| Female, n (%)           | 110 (19.1%)   | 119, (57.8)      |\n+-------------------------+---------------+------------------+\n| Sunglasses, n (%)       | 209 (36.2%)   | 98 (47.6%)       |\n+-------------------------+---------------+------------------+\n| Ice Cream Lovers, n (%) | 126 (21.8%)   | 51 (24.8%)       |\n+-------------------------+---------------+------------------+\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}