{
  "hash": "8592aac7bbf394c789983f6dbe4d51fb",
  "result": {
    "markdown": "---\nttitle: \"P-Values and Power: Please Explain\"\nauthor: \"Ryan Batten\"\ndate: \"2022-08-02\"\ncategories: [P-Values, Hypothesis Tests]\nimage: \"p-value.jpeg\"\nbibliography: p-value_references.bib\n---\n\n\n# Overview\n\nP-values are a term commonly heard in scientific literature, however often they are misconstrued, misunderstood or misinterpreted. The p in p-value stands for probability, as a lot of statistics begins with. What exactly does it tell us? Let's find out\n\n## Formal Definitions\n\n### Hypotheses\n\nHypotheses are the best place to start for any science experiment or project. A hypothesis is what you are wanting to know. First you need to start with your research question. For example, are people who love lollipops older than people who do not? To do this, formally in statistics, we need a *null hypothesis* ($H_0$) and an *alternative hypothesis* ($H_A$). We will use two abbreviations here: LL for lollipop lickers and NLL for not lollipop lickers. We want to know \"Do lollipop lickers have more sore tongues than those who do not lick lollipops?\"\n\nRe-phrasing this in statistically terms, $H_0$ is that people do not have more sore tongues if they lick lollipops compared to if they do. The question now is what is our alternative hypothesis? Well there are three options to choose from:\n\n1.  We think LL have more sore tongues than NLL\n\n2.  We think LL have less sore tongues than NLL\n\n3.  We don't know, but we don't think they are the same!\n\n1 and 2 are what are called *one-sided* hypothesis. A one-sided hypothesis, means you are assuming the difference is bigger or smaller. A two-sided hypothesis means we simply don't know! We will assume we don't know and use $p$ to denote the proportion of lollipop sore tongues.\n\n\n$$\nH_0: p_{LL-ST} = p_{NLL-ST}\n$$\n\n$$\nH_A: p_{LL-ST} \\neq p_{NLL-ST}\n$$\n\n\nNow, before we continue we need to review how okay we are with being wrong. Being wrong is okay, but there are two different types of being wrong.\n\n### Type I and II Errors\n\nA type I error, sometimes referred to as $\\alpha$ error, is the error of saying something happened when it did not. For example, the error of me assuming you bought a balloon **when you did not**. A type II error, sometimes referred to as $\\beta$ error, is the error of saying something did not happen **when it did**. Using our balloon example, me assuming you didn't buy a balloon **when you did**.\n\n### Power\n\nThe power of a statistical test is the probability that it correctly rejects the null hypothesis [@gelman2014beyond]. It is tied to the type II error, through the below formula.\n\n\n$$\nPower = 1 - \\beta\n$$\n\n\nRemember our lollipop example? For that it would be the probability that we can say lollipop lickers do not have the same amount of sore tongues as non lollipop lickers.\n\n### Test Statistic\n\nAlright, I know at this point I'm boring you. One last definition and then we can get to the good stuff. A test statistic is exactly what it sounds like. A statistic used for tests! What kind of tests you may ask? Hypothesis tests like we have above. Now, the test statistic in itself may follow a variety of distributions depending on the test. I'll leave distributions to another post.\n\n### P-Value\n\nThe probability of such an extreme value of the test statistic occurring if the null hypothesis were true is often called the **P-Value @bland2015introduction.**\n\n### Significance Level\n\nRemember $\\alpha$ from before? The type I error is sometimes referred to as the significance level. The number that is picked for the type I error is usually 0.05, however it is completely arbitrary to pick 0.05. It could be 0.20 or 0.02.\n\n::: callout-important\nThere is no such thing as ***more*** significant. It is either significant at the level that was pre-specified before the analysis or not. For example: a p-value of 0.01 is not **more** significant than a p-value of 0.05\n:::\n\n## Back to our Problem  {#sec-problem}\n\nFinally, after all those boring definitions we are back to where we started. How are we going to determine whether people who are lollipop lickers have more sore tongues? This requires us to do a bit more thinking to determine the right statistical tool for the toolbox.\n\nLet's think about this question a bit more first. Lollipop lickers tend to be younger right? Do boys like lollipops more than girls? Let's assume that both of those things are true. We will want to *control* for both of those variables. Control in this sense, means adding the variable in the equation like below.\n\n\n$$\nST = LL + age + sex\n$$\n\n\nNow, the type of regression we will use is a *generalized linear model (GLM)*. Generalized here just means that you can apply to almost any situation, hence general. Another post will cover the differences between GLMs and regular old regression (*OLS*).\n\n## Data on Lollipop Lickers and Sore Tongues\n\nNow we know our question, have our tool and want to find out the answer. What are we missing.....the most important thing of all! Data! Luckily, there is a database that collects such data. It has data on 422 people. Below are the summary statistics for our groups\n\n\n\n\n\n+-----------------------------+---------------------+-------------------+\n|                             | Sore Tongues        | Not Sore Tongues  |\n|                             |                     |                   |\n|                             | (n = 181)           | (n = 241)         |\n+=============================+:===================:+:=================:+\n| **Lollipop Lickers, n (%)** | 146 (80.7%)         | 60 (24.9%)        |\n+-----------------------------+---------------------+-------------------+\n| **Age, mean (SD)**          | 17.9 (7.30)         | 48.0 (17.8)       |\n+-----------------------------+---------------------+-------------------+\n| **Female, n (%)**           | 107 (59.1%)         | 52 (21.6%)        |\n+-----------------------------+---------------------+-------------------+\n\n## Analysis Time! \n\nUsing our handy dandy formula from above, and our tool from our toolbox (GLM). Below are the results.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    6.31     1.20        5.25 1.53e- 7\n2 ll             2.77     0.484       5.73 1.00e- 8\n3 age           -0.317    0.0499     -6.36 2.04e-10\n4 sex            2.15     0.493       4.36 1.32e- 5\n```\n:::\n:::\n\n\n## Now for the Interpretation of the P-Value\n\nThe test statistic for regression is defined by $\\beta / SE(\\beta)$. Beta here is the coefficient. For example, the test statistic for age is\n\n\n$$\n-0.3393494/0.0543545 = -6.243\n$$\n\n\nNow, this number on it's own is pretty useless. We want to know, what is our p-value?! Well, to do that we turn to the normal distribution.\n\n## Normal Distribution\n\nUsing the normal distribution, since our sample size is bigger than 30 (a wonderfully random number, that I have no idea why it's 30), we can determine our almighty p-value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n2*pnorm(-6.243)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.292569e-10\n```\n:::\n:::\n\n\nTa-da! We have our p-value of 4.29e-10. This is less than our pre-specified value of 0.05. We can therefore use significant in our interpretation however there are some strong caveats we need to go over first.\n\n## Issues with the P-Value\n\nP-values are typically over emphasized. They are a piece of the puzzle, however they are not the whole puzzle. Arguably, a more important piece of the puzzle is the effect size. For our example, lollipop lickers have increased odds of a sore tongue compared to non-lollipop lickers (OR = 5.87, 95% CI: 2.53 to 13.87). The effect, in this case OR, should be another piece of the puzzle that is considered. Finally, there are two other errors we should go over that highlight how not confident we can be in the p-value.\n\n### Type M & Type S Error\n\nOften type I and type II errors are highlighted however type M and type S errors are important to note as well. A type S error is the probability of an estimate being in the wrong direction [@gelman2014beyond]. A type M error is the factor by which the magnitude of an effect might be overestimated [@gelman2014beyond]. Definitions are always great but an example helps to better understand.\n\nUsing our example from before, and the function outlined in @gelman2014beyond:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nretrodesign <- function(A, s, alpha=.05, df=Inf, n.sims=10000){\n  z <- qt(1-alpha/2, df)\n  p.hi <- 1 - pt(z-A/s, df)\n  p.lo <- pt(-z-A/s, df)\n  power <- p.hi + p.lo\n  typeS <- p.lo/power\n  estimate <- A + s*rt(n.sims,df)\n  significant <- abs(estimate) > s*z\n  exaggeration <- mean(abs(estimate)[significant])/A\n  return(list(power=power, typeS=typeS, exaggeration=exaggeration))\n}\n\nretrodesign(-0.0339, 0.0543545)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$power\n[1] 0.09561688\n\n$typeS\n[1] 0.9488784\n\n$exaggeration\n[1] -3.898453\n```\n:::\n:::\n\n\nThe type S error is 0.948, while the type M error is -3.81our type S error would be 0.949 and type M error of 3.87. This means that there is a 94.9% probability that the estimate is the wrong sign, and that the value is overestimated by 3.87 times. Doesn't sound good right?\n\n## So are p-values garbage? \n\nP-values have a place in science, however they should be interpreted with caution and as a piece to the puzzle. Other pieces include the effect size, study design, sample that was studied and a myriad of other components.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}