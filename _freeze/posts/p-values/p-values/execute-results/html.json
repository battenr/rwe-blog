{
  "hash": "56acb25c606b926a8a258471772527a8",
  "result": {
    "markdown": "---\nttitle: \"P-Values and Power: Please Explain\"\nauthor: \"Ryan Batten\"\ndate: \"2022-08-02\"\ncategories: [P-Values, Hypothesis Tests]\nimage: \"p-value.jpeg\"\nbibliography: p-value_references.bib\ndraft: true\n---\n\n\n# Overview\n\nP-values are a term commonly heard in scientific literature, however often they are misconstrued, misunderstood or misinterpreted. The p in p-value stands for probability, as a lot of statistics begins with. What exactly does it tell us? Let's find out\n\n## Formal Definitions\n\n### Hypotheses\n\nHypotheses are the best place to start for any science experiment or project. A hypothesis is what you are wanting to know. First you need to start with your research question. For example, are people who love lollipops older than people who do not? To do this, formally in statistics, we need a *null hypothesis* ($H_0$) and an *alternative hypothesis* ($H_A$). We will use two abbreviations here: LL for lollipop lickers and NLL for not lollipop lickers. We want to know \"Are lollipop lickers older than not lollipop lickers?\"\n\nRe-phrasing this in statistically terms, \\$H_0\\$, is that there is no difference in age between the two. How do we measure age? For this example, we will use the mean age. So our $H_0$ is no difference in age, how about our alternative hypothesis? Well this can be one of three options:\n\n1.  We think LL are older than NLL\n\n2.  We think LL are younger than NLL\n\n3.  We don't know, but we don't think they are the same!\n\n1 and 2 are what are called *one-sided* hypothesis. A one-sided hypothesis, means you are assuming the difference is bigger or smaller. A two-sided hypothesis means, we don't know!\n\n\n$$\nH_0: \\mu_{LL} = \\mu_{NLL}\n$$\n\n$$\nH_A: \\mu_{LL} \\neq \\mu_{NLL}\n$$\n\n\nNow, before we continue we need to review how okay we are with being wrong. Being wrong is okay, but there are two different types of being wrong.\n\n### Type I and II Errors\n\nA type I error, sometimes referred to as $\\alpha$ error, is the error of saying something happened when it did not. For example, the error of me assuming you bought a balloon **when you did not**. A type II error, sometimes referred to as $\\beta$ error, is the error of saying something did not happen **when you did**. Using our balloon example, me assuming you didn't buy a balloon when you did.\n\n### Power\n\nThe power of a statistical test is the probability that it correctly rejects the null hypothesis [@gelman2014beyond]. It is tied to the type II error, through the below formula.\n\n\n$$\nPower = 1 - \\beta\n$$\n\n\nRemember our lollipop example? For that it would be the probability that we can say lollipop lickers are not the same age as not lollipop lickers.\n\n### Test Statistic\n\nAlright, I know at this point I'm boring you. One last definition and then we can get to the good stuff. A test statistic is exactly what it sounds like. A statistic used for tests! What kind of tests you may ask? Hypothesis tests like we have above. Now, the test statistic in itself may follow\n\nCommon test statistics include:\n\n-   T-t\n\n-   sadf\n\n-   asf\n\n-   For regression coefficients: $\\beta_1 / SE(\\beta_1)$\n\n### P-Value\n\nThe probability of such an extreme value of the test statistic occuring if the null hypothesis were true is often called the **P Value @bland2015introduction.** Similarly, the significance level\n\n### Significance Level\n\nRemember $\\alpha$ from before? The type I error is sometimes referred to as the significance level. The number that is picked for the type I error is usually 0.05, however it is completely arbitrary to pick 0.05. It could be 0.20 or 0.02.\n\n:::{callout-important}\n\nThere is no such thing as more significant. It is either significant at the level that was pre-specified before the analysis or not. For example: a p-value of 0.01 is not **more** significant than a p-value of 0.05\n\n:::\n\n## Normal Distribution\n\nTo begin, we need to start with some data. Who doesn't love data right?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](p-values_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Issues with the P-Value\n\nP-values are typically over emphasized. They are a piece of the puzzle, however they are not the whole puzzle.\n\n### Type M & Type S Error\n\nOften type I and type II errors are highlighted however type M and type S errors are important to note as well. A type S error is the probability of an estimate being in the wrong direction [@gelman2014beyond]. A type M error is the factor by which the magnitude of an effect might be overestimated [@gelman2014beyond].\n\nDefinitions are always great but an example helps to better understand. Without any further\n",
    "supporting": [
      "p-values_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}