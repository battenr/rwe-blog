{
  "hash": "ae55469f68e238f2df593758f64f5c42",
  "result": {
    "markdown": "---\ntitle: \"Standardization\"\nauthor: \"Ryan Batten\"\ndate: \"2022-09-26\"\ncategories: [Stanardization]\n# image: \"iptw.jpeg\"\nbibliography: standardization.bib\ndraft: true\nformat: \n  html:\n    toc: true\n    toc-title: Contents\n    toc-location: right\n    toc-depth: 4\n    code-fold: true\n---\n\n\n## Why and how do we standardize?\n\nI think first it's important to go over some of high level points of standardizing in general. First, what is standardizing? I'm sure you're familiar with the phrase \"you can't compare these, that's like comparing apples and oranges!\" and there is some truth to that. What if you could compare them with respect to the amount of juice a single one produces? Or how far away from the color blue they are? (Alright, bad example I admit but you get the point). The reasons that we standardize is to allow for a observations to be on a common scale. For example, you can't compare apples to oranges but you **can** compare the volume of juice contained in an apple to that of an orange.\n\n## Why should we standardize something?\n\nWhat exactly is standardizing?\n\n## How do we standardize something?\n\nIn general, standardization is typically performed by standardizing the data\n\ndescribe standardization\n\nStandardization is the process of transforming data into a common format by adjusting the values of the observations to a common scale. This is done by subtracting the mean of the data from each observation and then dividing by the standard deviation. This process helps to make data more comparable and easier to interpret. Standardization is often used in machine learning algorithms to ensure that all features are on the same scale and have the same variance.\n\n## Standardization versus IPTW\n\n### IPTW\n\nI won't bore you here, I'll keep it brief (if you want a more in-depth review of IPTW please see my other post).\n\nbriefly explain difference between iptw and standardization\n\n## Standardization in Causal Inference\n\nThe standardized mean in the uncensored (C = 0) is calculated as\n\n$$\n{\\sum_{l}E[Y|A = a, C=0, L=l]}  \\times Pr[L=l]\n$$\n\nNow, in an ideal world we'd be able to calculate this nonparametrically. We could calculate the mean outcomes in the uncensored, treated in each stratum $l$ of the confounders $L$ (i.e., $E[Y|A =1, C=0, L=l]$ for each of the strata $l$. Then we would take the weighted mean sum. To do this, we'd use the above formula where the weights are the proportion of people in each straum L. Rather than just described, how about an example?\n\n::: callout-note\n## What if L is continuous?\n\nIf $L$ is continuous in the above formula, then we need to replace $Pr[L=l]$ with the probability density function $f_{L}[l]$.\n:::\n\n### Does Eating Candy Cause Cavities? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndata.frame <- \n```\n:::\n\n\n###  \n\n### Nonparametric Standardization\n\nUnfortunately, this isn't always possible, especially when we are dealing with real-world data. People drop out of databases or may have missing data, just to name two potential problems.\n\n### Mean Outcome Modelling\n\nIt can be \\\n\\\n\n## Assumptions for Standardization\n\n1.  Structural positivity. Similar to IP weighting, positivity is also necessary for standardization because when $Pr[A = a | L = l] = 0$ and $Pr[L = l] \\neq 0$ then the conditional mean outcome $E[Y|A,L]$ is undefined [@hernanwhatif, pp.162].\n\n2.  \n\n::: callout-note\n## Positivity Assumption\n\nWhile both IP weighting and standardization require structural positivity, the implications of this assumptions not being valid can vary. For standardization, it is possible to use if this is assumption isn't met however there then needs to be a willingness to rely on parametric extrapolation (this can be done to fit a model that will smooth over the strata with structural zeroes) however this will introduce bias into the estimation. This will result in the nomial 95% confidence intervals around the estimates covering the true effect less than 95% of the time. See @hernanwhatif, pp. 162 for more details.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}