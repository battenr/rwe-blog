{
  "hash": "163b98e03872dc30c53ef066aaf9fc93",
  "result": {
    "markdown": "---\ntitle: \"Mastering Statistics Through Make-Believe\" \nsubtitle: \"Simulated Data in R\"\nauthor: \"Ryan Batten\"\ndate: \"2023-09-30\"\ncategories: [Simulating Data]\nbibliography: sim_data.bib\nimage : sim_data.jpg\nformat: \n  html:\n    toc: true\n    toc-title: Contents\n    toc-location: right\n    toc-depth: 4\n    code-fold: true\n---\n\n\n## Why Simulate Data?\n\nSimulating data wasn't something that I was taught in school. I've learned it since graduating/during my PhD, in my pursuit of improving my stats knowledge (the more I learn the more I feel I don't know, weird feeling). It's been unbelievably useful.\n\nI wanted to write this post to help anyone else who isn't familiar with simulating data. Before, I just want to give a few use cases where I've found simulating data helpful:\n\n-   Showing bias (confounder, collider, etc)\n\n-   Understanding how methods work\n\n-   Comparing different methods (1:1 matching vs IPTW, etc)\n\n-   Understand data generating mechanisms\n\nEnough about how it's helpful, how do we do it!\n\n::: callout-note\n## R Code\n\nI primarily use R for my coding, so this post focuses on using R. However, the methodology apply to whatever program you use for analysis. If you prefer to use MS Excel, you can do this using Excel as well, although I'd suggest a different software.\n\nI'm more of a journeyman tradesman biostatistician. What I mean by that is that I've done a few statistics courses but don't have a degree in it (my MSc and PhD are in clinical epidemiology).\\\n\\\nIf you do have a background in statistics, hopefully this is a good refresher. If you don't, don't worry! I don't either, so hopefulyl\n:::\n\n## How to Simulate Data in R\n\nBefore we dive into some R code, which I do love, it's useful to first understand at a high level how this works. Before you scramble to close this, wait! I promise it isn't going to be technical or boring....or at least I'll try my best to not make it boring.\\\n\\\nFor simplicity, we'll categorize outcomes as continuous, binary or time-to-event (TTE). This post won't tackle TTE, but a future blog post will! Alright, let's get started.\n\n### Continuous Outcomes\n\nTo simulate data in R, we can use a family of functions that start with r. For example, if we want to simulate data from a normal distribution we can use the *rnorm* function. Let's simulate an outcome and exposure that we will assess with a generalized linear model.\n\nLet's use hours that a four month old slept as the outcome and number of pacifiers in their bed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nset.seed(123) # we need to set a seed prior to simulating data. This allows us to replicate the data. For more details check out this blog post: [insert blog post about using different seeds for simulations]\n\nn.babies = 128 # note to get this value I used runif(1, min = 100, max = 500) then picked closest number divisible by 2\n\nnum_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)\nage_months = rnorm(n.babies, mean = 9, sd = 2)\nhours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months \n\ndf = data.frame(\n  num_pacifiers, \n  age_months,\n  hours_slept\n)\n```\n:::\n\n\nOkay, so we have our data. Now let's do something with it! Let's try fitting a GLM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod = glm(hours_slept ~ num_pacifiers + age_months, \n          family = gaussian(), # we know this because we simulated the data. In reality, you have to use a combination of visualizing the data, understanding the data generating mechanism (aka what distribution it came from and the best model fit (i.e., Poisson vs negative binomial))\n          data = df \n          )\n\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = hours_slept ~ num_pacifiers + age_months, family = gaussian(), \n    data = df)\n\nCoefficients:\n               Estimate Std. Error   t value Pr(>|t|)    \n(Intercept)   4.000e+00  9.149e-16 4.372e+15   <2e-16 ***\nnum_pacifiers 5.000e-01  6.223e-17 8.034e+15   <2e-16 ***\nage_months    2.500e-01  8.405e-17 2.974e+15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 3.496232e-30)\n\n    Null deviance: 2.5793e+02  on 127  degrees of freedom\nResidual deviance: 4.3703e-28  on 125  degrees of freedom\nAIC: -8313.5\n\nNumber of Fisher Scoring iterations: 1\n```\n:::\n:::\n\n\nNow, as you can see our estimates were pretty accurate. Let's try this again, but this time we'll assume that pacifiers doesn't matter, it's only the age. We can still adjust for both. Let's try two models: one where we adjust for both variables and one where we adjust for only age.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)\nage_months = rnorm(n.babies, mean = 9, sd = 2)\nhours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months \n\ndf = data.frame(\n  num_pacifiers, \n  age_months,\n  hours_slept\n)\n\n# Could alternatively try adjusting for the wrong variable (i.e., pacifiers but not age)\n\nmod1 <- glm(hours_slept ~ age_months, \n            family = gaussian(link = \"identity\"), \n            data = df)\n\nmod2 <- glm(hours_slept ~ age_months + num_pacifiers, \n            family = gaussian(link = \"identity\"), \n            data = df)\n\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = hours_slept ~ age_months, family = gaussian(link = \"identity\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.85497    0.64176  12.240   <2e-16 ***\nage_months   0.27997    0.06918   4.047    9e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.382045)\n\n    Null deviance: 339.15  on 127  degrees of freedom\nResidual deviance: 300.14  on 126  degrees of freedom\nAIC: 478.33\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n\n```{.r .cell-code}\nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = hours_slept ~ age_months + num_pacifiers, family = gaussian(link = \"identity\"), \n    data = df)\n\nCoefficients:\n               Estimate Std. Error   t value Pr(>|t|)    \n(Intercept)   4.000e+00  4.115e-15 9.719e+14   <2e-16 ***\nage_months    2.500e-01  3.915e-16 6.386e+14   <2e-16 ***\nnum_pacifiers 5.000e-01  2.519e-16 1.985e+15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 7.615348e-29)\n\n    Null deviance: 3.3915e+02  on 127  degrees of freedom\nResidual deviance: 9.5192e-27  on 125  degrees of freedom\nAIC: -7919.1\n\nNumber of Fisher Scoring iterations: 1\n```\n:::\n:::\n\n\nNow you see how the impact of adjusting for a variable that doesn't affect the outcome, at least in this case. For our model where we adjust for only age, the result is . When we adjust for the \"correct\" variables , we end up with a coefficient of 0.25.\n\nThis is how you simulate a continuous outcome, but what about a binary outcome?\n\n::: callout-note\n## Continuous Distributions distributions\n\nFor the above example, we used a normal distribution. However, this doesn't need to be the case. If we are dealing with age we may want to use a uniform distribution (using runif) and specifying the minimum and maximum. For example, if we are thinking of a variable where it may not make sense to have a value below a certain value (i.e., age where people are between 18 and 65).\n\nFor simplicity here, and because a number of statistical methods assume normality (*cough* also the central limit theorem *cough*), we will use *rnorm*.\n:::\n\n### Binary Outcomes\n\nSimulating a binary outcome is similar to simulating a continuous outcome except we need to put these variables in the probability argument. We can convert linear predictors to probabilities for the logistic distribution (since we'll be fitting a logistic regression) using the *plogis* function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Double check this example (using ChatGPT and another way/blog post/expert)\n\nn.parents = n.babies*1.5\n\ncoffee_consumption = rnorm(n = n.parents, mean = 3, sd = 0.5)\nhours_baby_slept = rnorm(n = n.parents, mean = 4, sd = 0.25)\ncold_room = rbinom(n = n.parents, size = 1, prob = 0.5)\n\nlinpred = 0.25*hours_baby_slept \n\nprob_tired = plogis(linpred)\n\n# linpred = 3 + 0.25*hours_baby_slept + 0.005*cold_room + 0.05*coffee_consumption\n# \n# prob_tired = plogis(linpred)\n  \ntired_parents = rbinom(n = n.parents, size = 1, prob = prob_tired)\n\ndf = data.frame(\n  cold_room,\n  coffee_consumption,\n  hours_baby_slept,\n  tired_parents\n)\n```\n:::\n\n\nNow we can fit a logistic regression model!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod = glm(tired_parents ~ hours_baby_slept,\n          # tired_parents ~ coffee_consumption + hours_baby_slept + cold_room,\n          family = binomial(link = \"logit\"), \n          data = df)\n\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = tired_parents ~ hours_baby_slept, family = binomial(link = \"logit\"), \n    data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)\n(Intercept)        0.1552     2.5939   0.060    0.952\nhours_baby_slept   0.2344     0.6437   0.364    0.716\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 215.94  on 191  degrees of freedom\nResidual deviance: 215.80  on 190  degrees of freedom\nAIC: 219.8\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nBased on this, we can see that our model doesn't give a great example but if we increase the sample size, what happens?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Double check this example (using ChatGPT and another way/blog post/expert)\n\nn.parents.large = n.parents*10\n\ncoffee_consumption = rnorm(n = n.parents.large, mean = 3, sd = 0.5)\nhours_baby_slept = rnorm(n = n.parents.large, mean = 4, sd = 0.25)\ncold_room = rbinom(n = n.parents.large, size = 1, prob = 0.5)\n\nlinpred = 0.25*hours_baby_slept \n\nprob_tired = plogis(linpred)\n\n# linpred = 3 + 0.25*hours_baby_slept + 0.005*cold_room + 0.05*coffee_consumption\n# \n# prob_tired = plogis(linpred)\n  \ntired_parents = rbinom(n = n.parents.large, size = 1, prob = prob_tired)\n\ndf = data.frame(\n  cold_room,\n  coffee_consumption,\n  hours_baby_slept,\n  tired_parents\n)\n\n\nmod = glm(tired_parents ~ hours_baby_slept,\n          # tired_parents ~ coffee_consumption + hours_baby_slept + cold_room,\n          family = binomial(link = \"logit\"), \n          data = df)\n\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = tired_parents ~ hours_baby_slept, family = binomial(link = \"logit\"), \n    data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)\n(Intercept)       -0.3543     0.8335  -0.425    0.671\nhours_baby_slept   0.3389     0.2087   1.624    0.104\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2236.9  on 1919  degrees of freedom\nResidual deviance: 2234.3  on 1918  degrees of freedom\nAIC: 2238.3\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nSo if we include 10 times the people, our result gets closer! If we include 100 times the people, it gets even closer.\n\n::: callout-note\n## TTE Outcome\n\nAt this point, you may be expecting a section on simulating TTE outcome. This will be the topics of a future blog post, since it's not quite as straight forward as continuous and binary outcomes.\n:::\n\nYou might be thinking \"this is great and all, but how does this apply to causal inference?\"\n\n## Simulating Causal Concepts\n\n## Causal Concepts\n\nNow, this is great and all but this blog is about causal inference! So let's incorporate some into this post shall we? Instead of looking at GLMs, let's demonstrate how confounding and colliders can introduce bias. We'll calculate bias as [@morris2019]:\n\n$$\nE[\\hat{\\theta}] - \\theta\n$$\n\nwhere $E[\\hat{\\theta]}$ is the expected, or average, estimated value and $\\theta$ is the \"actual\" value. I'll explain why we use the average later on in the Over and Over and Over Again section \\[TRY TO ADD CROSS REFERENCE\\].\n\n::: callout-important\n## Causal Estimand\n\nWe need to make sure we are looking at the same causal estimand when comparing methods. For example, if we want to compare PSM to IPTW that would result in different answer...because they target different estimands!\n:::\n\n## Showing Confounding\n\nLet's assume that we want to demonstrate how confounding works. I'm an avid coffee lover, so let's use an example with coffee! Suppose that our research question is \"Does consuming coffee cause you to be happy?\" We can start by drawing a DAG with our three variables: happy, coffee and sleep.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggdag)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggdag'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n:::\n\n```{.r .cell-code}\ntheme_set(theme_dag())\n\ncoffee_dag <- ggdag::dagify(\n  happy ~ coffee + sleep,\n  coffee ~ sleep,\n  exposure = \"coffee\",\n  outcome = \"happy\",\n  labels = c(\n    coffee = \"Coffee\",\n    happy = \"Happiness\",\n    sleep = \"Sleep\"\n  )\n)\n\nggdag::ggdag(coffee_dag, text = FALSE, use_labels = \"label\")\n```\n\n::: {.cell-output-display}\n![](sim_data_files/figure-html/dags-1.png){width=672}\n:::\n:::\n\n\nNow we have our DAG, let's get to simulating some data!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look into Matthew Fox article for simulating data \n\n# Simulating Data, from ChatGPT\n\nset.seed(123) # Setting a seed for reproducibility\n\n# Number of observations\nn <- 1000\n\n# Simulating sleep hours (normal distribution with mean=7 and sd=1.5)\nsleep_hours <- rnorm(n, mean = 7, sd = 1.5)\n\n# Simulating coffee consumption based on sleep (negative correlation: less sleep -> more coffee)\ncoffee_consumption <- 5 - 0.5 * sleep_hours + rnorm(n, mean = 0, sd = 1)\n\n# Simulating happiness based on both sleep (positive correlation: more sleep -> more happiness)\n# and coffee consumption (positive correlation: more coffee -> more happiness)\nhappiness <- rbinom(n = n, size = 1, prob = plogis(0.3 * sleep_hours + 0.2 * coffee_consumption + rnorm(n, mean = 0, sd = 1)))\n\n# Creating a data frame to hold the variables\ndf <- data.frame(sleep_hours, coffee_consumption, happiness)\n\n# Displaying the first few rows of the data\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  sleep_hours coffee_consumption happiness\n1    6.159287          0.9245580         1\n2    6.654734          0.6326781         1\n3    9.338062          0.3129885         1\n4    7.105763          1.3149436         1\n5    7.193932         -1.1463086         1\n6    9.572597          1.2542747         1\n```\n:::\n:::\n\n\nNow we have our data, let's show some confounding. How? We'll fit two models: 1) not adjusting for the confounder, 2) adjusting for the confounder.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- glm(formula = happiness ~ coffee_consumption,\n            family = binomial(link = \"logit\"),\n            data = df)\n\nmod2 <- glm(formula = happiness ~ coffee_consumption + sleep_hours,\n            family = binomial(link = \"logit\"),\n            data = df)\n\nbias1 = coef(mod1)[2] - 0.2\nbias2 = coef(mod2)[2] - 0.2\n```\n:::\n\n\nWe can now calculate bias for each of these models. As we can see, the bias from model one (-0.1397736) is more than the bias from model two (0.0277288). But how can we trust this? We only did it once. What if a sample of the same size (N = 250) gave a different answer? To account for this, we need to repeat this multiple times. So, let's do that! Let's repeat it a thousand times\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # Setting a seed for reproducibility\n\nsimulation <- function() {\n  # Number of observations\n  n <- 250\n  \n  # Simulating sleep hours (normal distribution with mean=7 and sd=1.5)\n  sleep_hours <- rnorm(n, mean = 7, sd = 1.5)\n  \n  # Simulating coffee consumption based on sleep (negative correlation: less sleep -> more coffee)\n  coffee_consumption <- 5 - 0.5 * sleep_hours + rnorm(n, mean = 0, sd = 1)\n  \n  # Simulating happiness based on both sleep (positive correlation: more sleep -> more happiness)\n  # and coffee consumption (positive correlation: more coffee -> more happiness)\n  happiness <- rbinom(n = n, size = 1, prob = plogis(0.3 * sleep_hours + 0.2 * coffee_consumption + rnorm(n, mean = 0, sd = 1)))\n  \n  # Creating a data frame to hold the variables\n  df <- data.frame(sleep_hours, coffee_consumption, happiness)\n  \n  # Building the models\n  mod1 <- glm(formula = happiness ~ coffee_consumption,\n              family = binomial(link = \"logit\"),\n              data = df)\n  \n  mod2 <- glm(formula = happiness ~ coffee_consumption + sleep_hours,\n              family = binomial(link = \"logit\"),\n              data = df)\n  \n  # Calculating the biases\n  coef1 <- coef(mod1)[2] \n  coef2 <- coef(mod2)[2] \n  \n  coef_results <- data.frame(\n    coef1, coef2\n  )\n  \n  # Returning the biases as a named vector\n  return(\n    coef_results\n  )\n}\n\n# Replicating the simulation 1000 times\noutput_list <- replicate(n = 1000, expr = simulation(), simplify = FALSE) \n\n# Bind all data frames in the list into a single data frame\ndf.out <- do.call(rbind, output_list)\n\nbias1 = mean(df.out$coef1) - 0.2\nbias2 = mean(df.out$coef2) - 0.2\n```\n:::\n\n\nNow we can look at\n\n::: callout-note\n## Picking a number of simulations\n\nFor our example we picked 1000 repetitions but where did this number come from? Truthfully, it was completely arbitrary. In practice, we need to carefully choose how many we need. I highly suggest @morris2019 as a reference for more information if you need to pick the number of simulations.\n:::\n\n# Other Use Cases\n\nWe can use this to show other concepts as well, test new ideas or learn methods ourselves. Recently, I used it to demonstrate to myself how collider bias is. Another example, that I'm currently exploring, is how different the bias is from confounding versus a collider bias.\n\nor test them ourselves.\n\nFor example: collider bias.\n\n## \n",
    "supporting": [
      "sim_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}