{
  "hash": "5bac5881843607109e7c3852dc7562b8",
  "result": {
    "markdown": "---\ntitle: \"Mastering Statistics Through Make-Believe\" \nsubtitle: \"Simulated Data in R\"\nauthor: \"Ryan Batten\"\ndate: \"2023-09-02\"\ncategories: [Simulating Data]\nbibliography: sim_data.bib\n# image : \"standards.png\"\ndraft: true\nformat: \n  html:\n    toc: true\n    toc-title: Contents\n    toc-location: right\n    toc-depth: 4\n    code-fold: true\n---\n\n\n## Simulating Data, Why?\n\nSimulating data wasn't something that I was taught in school. I've learned it since graduating/during my PhD, in my pursuit of improving my stats knowledge (the more I learn the more I feel I don't know, weird feeling). It's been unbelievably useful.\n\nI wanted to write this post to help anyone else who isn't familiar with simulating data. Before, I just want to give a few use cases where I've found simulating data helpful:\n\n-   Showing bias (confounder, collider, etc)\n\n-   Understanding how methods work\n\n-   Comparing different methods (1:1 matching vs IPTW, etc)\n\n-   Understand data generating mechanisms\n\n::: callout-note\n## Stats for non-stats\n\nI'm a tradesman biostatistician \\[NOTE: RYAN CHECK THIS TERM FROM HIS TWEET\\] (credit to PhDemetri for this term). Basically, I've done a few statistics courses but don't have a degree in it (I do in clinical epidemiology which overlaps substantially with biostatistics).\n\nIf you do, hopefully this is a good refresher. If you don't, don't worry! I don't either so hopefully this helps out a little as an intro.\n:::\n\n## How to Simulate Data in R\n\nBefore we dive into some R code, which I do love, it's useful to first understand at a high level how this works. Before you scramble to close this, wait! I promise it isn't going to be technical or boring. Alright, let's get started.\n\n### Continuous Outcomes\n\nTo simulate data in R, we can use a family of functions that start with r. For example, if we want to simulate data from a normal distribution we can use the *rnorm* function. Let's simulate an outcome and exposure that we will assess with a generalized linear model.\n\nLet's use hours that a four month old slept as the outcome and number of pacifiers in their bed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nset.seed(123) # we need to set a seed prior to simulating data. This allows us to replicate the data. For more details check out this blog post: [insert blog post about using different seeds for simulations]\n\nn.babies = 128 # note to get this value I used runif(1, min = 100, max = 500) then picked closest number divisible by 2\n\nnum_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)\nage_months = rnorm(n.babies, mean = 9, sd = 2)\nhours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months \n\ndf = data.frame(\n  num_pacifiers, \n  age_months,\n  hours_slept\n)\n```\n:::\n\n\nOkay, so we have our data. Now let's do something with it! Let's try fitting a GLM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod = glm(hours_slept ~ num_pacifiers + age_months, \n          family = gaussian(), # we know this because we simulated the data. In reality, you have to use a combination of visualizing the data, understanding the data generating mechanism (aka what distribution it came from and the best model fit (i.e., Poisson vs negative binomial))\n          data = df \n          )\n\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = hours_slept ~ num_pacifiers + age_months, family = gaussian(), \n    data = df)\n\nCoefficients:\n               Estimate Std. Error   t value Pr(>|t|)    \n(Intercept)   4.000e+00  9.149e-16 4.372e+15   <2e-16 ***\nnum_pacifiers 5.000e-01  6.223e-17 8.034e+15   <2e-16 ***\nage_months    2.500e-01  8.405e-17 2.974e+15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 3.496232e-30)\n\n    Null deviance: 2.5793e+02  on 127  degrees of freedom\nResidual deviance: 4.3703e-28  on 125  degrees of freedom\nAIC: -8313.5\n\nNumber of Fisher Scoring iterations: 1\n```\n:::\n:::\n\n\nNow, as you can see our estimates were pretty accurate. Let's try this again, but this time we'll assume that pacifiers doesn't matter, it's only the age. We can still adjust for both. Let's try two models: one where we adjust for both variables and one where we adjust for only age.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)\nage_months = rnorm(n.babies, mean = 9, sd = 2)\nhours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months \n\ndf = data.frame(\n  num_pacifiers, \n  age_months,\n  hours_slept\n)\n\n# Could alternatively try adjusting for the wrong variable (i.e., pacifiers but not age)\n\nmod1 <- glm(hours_slept ~ age_months, \n            family = gaussian(link = \"identity\"), \n            data = df)\n\nmod2 <- glm(hours_slept ~ age_months + num_pacifiers, \n            family = gaussian(link = \"identity\"), \n            data = df)\n\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = hours_slept ~ age_months, family = gaussian(link = \"identity\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.85497    0.64176  12.240   <2e-16 ***\nage_months   0.27997    0.06918   4.047    9e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.382045)\n\n    Null deviance: 339.15  on 127  degrees of freedom\nResidual deviance: 300.14  on 126  degrees of freedom\nAIC: 478.33\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n\n```{.r .cell-code}\nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = hours_slept ~ age_months + num_pacifiers, family = gaussian(link = \"identity\"), \n    data = df)\n\nCoefficients:\n               Estimate Std. Error   t value Pr(>|t|)    \n(Intercept)   4.000e+00  4.115e-15 9.719e+14   <2e-16 ***\nage_months    2.500e-01  3.915e-16 6.386e+14   <2e-16 ***\nnum_pacifiers 5.000e-01  2.519e-16 1.985e+15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 7.615348e-29)\n\n    Null deviance: 3.3915e+02  on 127  degrees of freedom\nResidual deviance: 9.5192e-27  on 125  degrees of freedom\nAIC: -7919.1\n\nNumber of Fisher Scoring iterations: 1\n```\n:::\n:::\n\n\nNow you see how the impact of adjusting for a variable that doesn't affect the outcome, at least in this case. For our model where we adjust for only age, the result is . When we adjust for the \"correct\" variables , we end up with a coefficient of 0.25.\n\nThis is how you simulate a continuous outcome, but what about a binary outcome?\n\n::: callout-note\n## Continuous Distributions distributions\n\nFor the above example, we used a normal distribution. However, this doesn't need to be the case. If we are dealing with age we may want to use a uniform distribution (using runif) and specifying the minimum and maximum. For example, if we are thinking of a variable where it may not make sense to have a value below a certain value (i.e., age where people are between 18 and 65).\n\nFor simplicity here, and because a number of statistical methods assume normality (*cough* also the central limit theorem *cough*), we will use *rnorm*.\n:::\n\n### Binary Outcomes\n\nSimulating a binary outcome is similar to simulating a continuous outcome except we need to put these variables in the probability argument. We can convert linear predictors to probabilities for the logistic distribution (since we'll be fitting a logistic regression) using the *plogis* function.\n\n\n::: {.cell}\n\n:::\n\n\nTired parent: yes/no. Depends on: how many hours baby slept, cofffee consumption and cold room (yes/no)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)\nage_months = rnorm(n.babies, mean = 9, sd = 2)\nhours_slept = 4 + 0.25*age_months \n\ndf = data.frame(\n  num_pacifiers, \n  age_months,\n  hours_slept\n)\n```\n:::\n\n\n## Simulating Causal Concepts\n\n## Causal Concepts\n\nNow, this blog is about causal inference! So let's incorporate some into this post shall we? Instead of looking at GLMs, let's demonstrate how confounding and colliders can introduce bias. We'll calculate bias as [@morris2019]:\n\n$$\nE[\\hat{\\theta}] - \\theta\n$$\n\nwhere $E[\\hat{\\theta]}$ is the expected, or average, estimated value and $\\theta$ is the \"actual\" value. I'll explain why we use the average later on in the Over and Over and Over Again section \\[TRY TO ADD CROSS REFERENCE\\]\n\n## Showing Confounding\n\nLet's assume that we want to test how not adjusting for a confounder can lead to confounding bias. For this situation let's look at the number of pacifiers in a babies crib and the hours slept. We can draw a DAG, including our confounder which is weather.\n\n\\# Set seed for reproducibility\n\nset.seed(111)\n\n\\# Number of observations\n\nn \\<- 100\n\n\\# Generate confounder - Baby's Age in months\n\nbaby_age \\<- rnorm(n, mean = 6, sd = 2)\n\n\\# Generate number of pacifiers influenced by Baby's Age\n\nnum_pacifiers \\<- 1 + 0.3 \\* baby_age + rnorm(n, mean = 0, sd = 0.2)\n\n\\# Generate hours slept influenced by Baby's Age\n\nhours_slept \\<- 10 + 0.5 \\* baby_age + rnorm(n, mean = 0, sd = 1)\n\n\\# Combine into a data frame\n\ndata \\<- data.frame(baby_age, num_pacifiers, hours_slept)\n\nFor this, our question is \"does hot coffee cause rainbows?\". Rainy days lead to drinking more coffee, but rainy days also lead to rainbows.\\\n\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggdag)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggdag'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n:::\n\n```{.r .cell-code}\ntheme_set(theme_dag())\n\ndag = ggdag::dagify(\n  hot_coffee ~ rain, \n  rainbow ~ rain + hot_coffee, \n  exposure = \"hot_coffee\",\n  outcome = \"rainbow\",\n  labels = c(\n    hot_coffee = \"Hot\\nCoffee\",\n    rainbow = \"Rainbow\", \n    rain = \"Rain\"\n  )\n) \n\ndag %>% \n  ggdag::ggdag(layout = \"tree\", \n               text = FALSE,\n               use_labels = \"label\")\n```\n\n::: {.cell-output-display}\n![](sim_data_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\ng at how long goldfish grow as an example. There's a few factors that affect the size (in this example length and size will be used interchangeably): feeding_frequency (number of times per day), water temperate, and how much food.\n\n\\# Simulate data for 100 goldfish\n\nset.seed(123)\n\nn \\<- 100\n\n\\# Nutritional factors\n\nfood_quality \\<- rnorm(n, mean = 5, sd = 1)\n\nfeeding_frequency \\<- rnorm(n, mean = 3, sd = 0.5)\n\n\\# Environmental factors\n\nwater_quality \\<- rnorm(n, mean = 7, sd = 1)\n\nwater_temp \\<- rnorm(n, mean = 20, sd = 2)\n\n\\# Growth rate as the dependent variable\n\ngrowth_rate \\<- 0.5 + 0.2\\*food_quality + 0.1\\*feeding_frequency + 0.3\\*water_quality + 0.4\\*water_temp + rnorm(n, mean = 0, sd = 0.5)\n\n\\# Combine into a data frame\n\ngoldfish_data \\<- data.frame(food_quality, feeding_frequency, water_quality, water_temp, growth_rate)\n\nLet's use how long goldfish grow as an example. For ease of this example, let's\n\n### \n\nThis type of simulation is called a Monte Carlo Simulation. The reason it's so useful is because\n\nof coffee beans. These range in shape and size. Suppose we know that the average weight is 135 milligrams and 16 (10 to 22/64 of an inch). Now, does this mean that every bean will be that? Let's pull one out and see\n\n## Simulating Data\n\nWe'll start with continuous data. To simulate this we need two pieces of information, also known as parameters: mean and standard deviation.\n\n::: callout-note\n## TTE Outcome\n\nThis post will focus on continuous and binary outcomes. A future blog post will go over how to simulate time-to-event, sometimes called survival, data.\n:::\n\nNOTE TO SELF: explain that regression is finding the mean (keith mcnulty's post)\n\nNow, if we pick one\n\n# Old Text\n\nLearning to simulate data was one of the best things that I've stumbled into. It can help improve your knowledge of statistics and inform decision making. One of the things I found difficult when starting was for a resource that gave some examples of what I really wanted to know. Hopefully this post will provide a decent start to someone.\n\n## Why Simulate?\n\nAt first, I had heard about simulation studies but wasn't exactly sure the purpose of them.\n\n## Steps to Simulate\n\n\\@morris2019 use the acronym ADEMP: Aims, Data-generating mechanisms, Methods, Estimands and Performance measures. This is a great acronym which we'l'' break down a little bit .\n\n## Over and Over and Over Again\n\nNow we've simulated data. Great! But how can we trust this? We only did it once. What if the sample of the same size (N = X \\[RYAN: INSERT NUMBER\\]) gave a different answer? To account for this, we need to repeat this multiple times. So, let's do that!\n\n::: callout-note\n## Selecting n for Monte Carlo simulation\n\nTo select the n required, there are several resources. I won't go into detail here, but some personal references I use are \\[INSERT Morris et al. and others\\]\n:::\n\nMonte Carlo simulation is\n\n## Resources for Simulating\n\nThere are a ton of good resources when it comes to design aspects of simulation study. Two that help me think about simulation studies are \\@morris2019 and \\@burton2006.\n",
    "supporting": [
      "sim_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}