{
  "hash": "581b5d2d9f0bef946d3b6b75b6d8ceb3",
  "result": {
    "markdown": "---\ntitle: \"Resampling Magic\" # October Post\nsubtitle: \"Demystifying Bootstrapping\"\n#subtitle: \"The Wonders of Bootstrapping Explained\"\nauthor: \"Ryan Batten\"\ndate: \"2023-10-30\"\ncategories: [Resampling, Bootstrapping, Variance Estimation]\nbibliography: bootstrap.bib\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Certainly Uncertain\n\nA key part of any analysis is to determine the uncertainty associated with an estimate. Typically we use confidence intervals to show this. Sometimes deriving this can be tricky with certain methods or when assumptions aren't met. For example, if we use propensity score matching and then fit a generalized linear model in this matched sample, the assumption about the cases being independent isn't true. So what do we do? We need a different way to calculate the standard error. Enter bootstrapping!\n\n## What exactly *is* bootstrapping?\n\nNo, not like Bootstrap Bill (Pirates of the Caribbean reference). Bootstrapping is a statistical technique that can be quite useful. It comes from the term \"pull oneself up by their bootstraps\" in the sense that we use our sample that we already have. We can use this to estimate confidence intervals! But can also use it for other things. So how exactly does it work?\n\n## How does it work?\n\nBootstrapping works by drawing repeated samples, with replacement, from our sample. The idea is that by resampling data we can use this to imagine what a new sample (of the same size) would look like if we drew from our population again.\n\nThe beauty about bootstrapping is all we need is an estimate and a sampling distribution [@gelman2020regression]. A better way to understand it is with a quick example.\n\nImagine that go to an ice cream store and order a bowl of ice cream. We get a bowl of ice cream that's 80% vanilla and 20% chocolate but we're curious what the big tub in the back of the store looks like. Is it mostly vanilla? Chocolate? Is it a 70/30 split of either? Are there other flavours? If we asked for another bowl from the same tub would we get the same proportion?\n\nTo try and determine this, we take a spoonful from our bowl. In our bowl, there is 60% chocolate and 40% vanilla. We try taking another spoonful and notice it's 80% vanilla and 20% chocolate. If we do this over and over again, we can use this to try and get an idea of what the big tub in the back looks like.\\\n\\\nThis is what bootstrapping it. We take repeated samples to try to understand what the whole picture looks like. Let's try another example with some actual numbers!\n\n## Mini-Golf?\n\nLet's use mini-golf as an example. Imagine that you play mini-golf with your friends 100 times over a summer (yes you must really REALLY love mini-golf). Each time you play, you get a different score (the lower the better). Sometimes you do well, sometimes you play awful.\\\n\\\nYou start to think: \"I play a lot of mini-golf, but am I any good? What would my score be on average?\" being the keen person that you are, you also wonder what kind of confidence intervals go with this average. So you decide to use bootstrapping to answer this, using these steps:\n\n1.  Draw 10 scores, from any day (randomly) and write them down.\n2.  Put them back.\n3.  Draw another 10 scores and write them down.\n4.  Put them back.\n5.  Repeat steps 1-4\n\nNow we have our plan, let's start implementing it!\n\n## Mini-Golf - Average Score\n\nSo we know our average score, if we just take the mean of all the scores. But what is the confidence interval for this mean? To calculate this we often use the standard error while assuming a normal distribution. But what is the standard error? Is it the same as the standard deviation? Not quite.\n\nThe standard error is the standard deviation of the *sampling distribution*. The sampling distribution is the probability distribution of a statistic. This may sound like a bit of jargon, so instead let's use a visual.\n\nFor our example, we are taking 10 scores over and over. Each time we take 10 scores, we calculate the mean. So for each time, we have the mean. Let's plot this!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # Loading the tidyverse package\n\n# Setting seed for reproduceability, arbitrarily chosen in this case\n\nset.seed(123)\n\n\n# Original mini-golf scores, one per day for 100 days. \n\noriginal_scores <- sample(40:60, 100, replace = TRUE)\n\n# Initalizing a variable to store the bootstrap estimates in \n\nbootstrap_estimates <- numeric()\n\n# Number of bootstraps. For this case, arbitrarily choosing 1000. In reality, need to consider what number to use\n\nn_bootstraps <- 1000\n\n# Time to do the bootstrapping!\n\nfor (i in 1:n_bootstraps) {\n  \n  # Draw 10 samples from our original scores, with replacement. That \n  # way you have the same sample each time\n  \n  bootstrap_sample <- sample(original_scores, size = 10, replace = TRUE) \n  \n  # Calculate the mean for our new sample \n  \n  bootstrap_mean <- mean(bootstrap_sample)\n  \n  # Combine our new estimated mean \n  \n  bootstrap_estimates <- c(bootstrap_estimates, bootstrap_mean)\n}\n\n# We can calculate the 95% CI as either finding out the standard error (calculate the standard deviation of bootstrap_estimates) OR can use the corresponding quantiles. \n\n# Estimating the 95% CI using the 0.025 and 0.975 quantiles (two-sided)\n\nestimated_ci <- quantile(bootstrap_estimates, probs = c(0.025, 0.975))\n\nestimated_mean <- mean(original_scores) # Calculating the mean\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = bootstrap_estimates |> as.data.frame()\n\nggplot(data = df, aes(x = bootstrap_estimates)) + \n  geom_bar() +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  ) +\n  ggtitle(\"Sampling Distribution of Mini-Golf Scores for the Mean\") +\n  labs(x = \"Bootstrap Sample Number (1 through n of number of bootstrap replicates)\", y = \"Mean of Sample\")\n```\n\n::: {.cell-output-display}\n![](bootstrapping_files/figure-html/sampling distribution plot-1.png){width=672}\n:::\n:::\n\n\nEach of these is an estimate of the mean. This is a *sampling distribution.* We can now use this to determine our confidence intervals. If we're using a two-sided test and 95% CIs then can take the 0.025 and 0.975 percentiles! Doing this, we now know our average score is 50.66 and our confidence interval is 47.0975, 53.9025!\n\nHowever, this is the mean. It doesn't account for the number of obstacles on the course or if it was a weekend (perhaps we'd feel more rushed if we were playing on the weekend). So what if we want to fit a model and get 95% CIs for that model?\n\n## Mini-Golf: GLM Time\n\nWe'll use a generalized linear model for our example. Side note: there is no rationale for picking a GLM, other than it's a method that I'm familiar with. In reality, picking the model to use isn't always so straightforward. Now, let's simulate some data!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed for reproducibility, \n\nset.seed(123) # Setting seed for reproduceability, arbitrarily chosen in this case\n\n# Number of games\nn_games <- 100\n\n# Number of obstacles (simulated)\nobstacles <- round(runif(n_games, 5, 15))\n\n# Weekend (1 if weekend, 0 otherwise)\nweekend <- sample(0:1, n_games, replace = TRUE)\n\n# Mini-golf scores (simulated)\nscores <- 50 + 2 * obstacles + 5 * weekend + rnorm(n_games, 0, 5)\n\n# Data frame\nmini_golf_data <- data.frame(scores, obstacles, weekend)\n```\n:::\n\n\nFirs things first with our data, let's fit a GLM.\n\nNow that we have our data, we can create some bootstrap samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We'll use the tidymodels package here\n\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n:::\n\n```{.r .cell-code}\nboots <- bootstraps(mini_golf_data, times = 1000, apparent = TRUE)\n\n# Now we'll create a function for fitting our model. In this case a GLM\n\nfit_glm <- function(df){\n  glm(scores ~ obstacles + weekend, \n      family = gaussian(link = \"identity\"),\n      data = df \n      )\n}\n\n# Now let's fit this model for each bootstrap sample\n\nboot_glms <- boots %>% \n  dplyr::mutate(\n    model = map(splits, fit_glm),\n    coef_into = map(model, tidy)\n  )\n\n\nboot_coefs <- \n  boot_glms %>% \n  unnest(coef_into)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now we can calculate the confidence interval by taking the 0.025 and 0.95 quantiles. \n\nint_pctl(boot_glms, coef_into)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 6\n  term        .lower .estimate .upper .alpha .method   \n  <chr>        <dbl>     <dbl>  <dbl>  <dbl> <chr>     \n1 (Intercept)  45.5      49.2   52.9    0.05 percentile\n2 obstacles     1.72      2.05   2.37   0.05 percentile\n3 weekend       2.65      4.59   6.39   0.05 percentile\n```\n:::\n:::\n\n\n## Why not use bootstrapping all the time?\n\n\"So what you're telling me is I can just use bootstrapping all the time for every test?\" Well, no. It is a useful tool but like all tools, whether statistical or not, there is a time and a place to use them. Would you use a hammer as a fly swatter? I guess you could, but why not just use a fly swatter.\n\nLike any method, bootstrapping isn't perfect. There are benefits and drawbacks. For example, it can give different results depending on the starting seed. It can also only draw observations from your data that has been observed. In our ice cream example, no bubble gum flavour would show because our bowl only has vanilla and chocolate. However, it's not all bad. Bootstrapping can be a solution in a variety of situations and is quite flexible. It's important to keep all of this in mind when considering using it.\n\nBootstrapping is a powerful tool but should be used thoughtfully. Understanding its limitations is crucial for interpreting the results correctly, especially in complex analyses like causal inference. Given its pros and cons, it often serves as a useful complement to other statistical methods.\n\n## What's Next?\n\nBootstrapping is a wide area, this post was meant to be an introduction. I highly recommend the textbook by Hinkely et al. and to try using bootstrapping yourself!\n",
    "supporting": [
      "bootstrapping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}