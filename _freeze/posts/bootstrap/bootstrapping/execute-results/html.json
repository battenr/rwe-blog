{
  "hash": "0bcb615ebb51eee3f7ca8c0b27175c7d",
  "result": {
    "markdown": "---\ntitle: \"Resampling Magic\" # October Post\nsubtitle: \"Demystifying Bootstrapping\"\n#subtitle: \"The Wonders of Bootstrapping Explained\"\nauthor: \"Ryan Batten\"\ndate: \"2023-10-30\"\ncategories: [Resampling, Bootstrapping, Variance]\nbibliography: bootstrap.bib\ndraft: true\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Certainly Uncertain\n\nA key part of any analysis is to determine the uncertainty associated with an estimate. Typically we use confidence intervals to show this, which requires the standard error. Sometimes deriving this can be tricky with certain methods or when assumptions aren't met. For example, if we use propensity score matching and the fit a generalized linear model in this matched sample, the assumption about the cases being independent isn't true. So what do we do? We need a different way to calculate the standard error. Enter bootstrapping!\n\n## What is bootstrapping?\n\nNo, not like Bootstrap Bill (Pirates of the Caribbean reference), bootstrapping is a statistical technique that can be quite useful. We can use this to estate confidence intervals, for hypothesis testing, model validation and estimating treatment effects just to name a few things. So how exactly does it work?\n\n## How does it work?\n\nBootstrapping works by drawing repeated samples from our observations. The beauty about bootstrapping is all we need is an estimate and a sampling distribution [@gelman2020regression]. A better way to understand it is with a quick example.\n\nImagine that go to an ice cream store and order a bowl of ice cream. We get a bowl of ice cream, but we're curious what the big tub in the back of the store looks like. Is it mostly vanilla? Chocolate? Is it a 70/30 split of either? Are there other flavours? To try and determine this, we take a spoonful from our bowl. In our bowl, there is 60% chocolate and 40% vanilla. We try taking another spoonful and notice it's 80% vanilla and 20% chocolate. If we do this over and over again, we can get an idea of what the big tub in the back looks like. \\\n\\\nThis is what bootstrapping it. We take repeated samples to try to understand what the whole picture looks like. Let's try another example with some actual numbers!\n\n## Mini-Golf? \n\nLet's use mini-golf as an example. Imagine that you play mini-golf with your friends 100 times over a summer (yes you must really REALLY love mini-golf). Each time you play, you get a different score (the lower the better). Sometimes you do well, sometimes you play awful.\\\n\\\nYou start to think: \"I play a lot of mini-golf, but am I any good? What would my score be on average?\" being the keen person that you are, you also wonder what kind of confidence intervals go with this average. So you decide to use bootstrapping to answer this, using these steps:\n\n1.  Look at 10 scores on a random day and write them down.\n2.  Put them back.\n3.  Draw another 10 scores and write them down.\n4.  Put them back.\n5.  Repeat steps 1-4\n\nNow we have our plan, let's start implementing it!\n\n## Mini-Golf - Average Score\n\nSo we know our average score, if we just take the mean. But what is the confidence interval for this mean? To calculate this, we need the standard error. But what is the standard error? Is it the same as the standard deviation? Not quite.\n\nThe standard error is the standard deviation of the *sampling distribution*. The sampling distribution is the probability distribution of a statistic. This may sound like a bit of jargon, so instead let's use a visual.\n\nFor our example, we are taking 10 scores over and over. Each time we take 10 scores, we calculate the mean. So for each time, we have the mean. Now, let's plot this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\n# Your 100 mini-golf scores\noriginal_scores <- sample(40:60, 100, replace = TRUE)\n\n# Initialize a variable to store the bootstrap estimates\nbootstrap_estimates <- numeric()\n\n# Number of bootstraps\nn_bootstraps <- 1000\n\n# Bootstrapping\nfor (i in 1:n_bootstraps) {\n  bootstrap_sample <- sample(original_scores, size = 10, replace = TRUE)\n  bootstrap_mean <- mean(bootstrap_sample)\n  bootstrap_estimates <- c(bootstrap_estimates, bootstrap_mean)\n}\n\n# Estimate the 95% confidence interval\n\nestimated_ci <- quantile(bootstrap_estimates, probs = c(0.05, 0.95))\n\nestimated_mean <- mean(original_scores)\n```\n:::\n\n\nGreat, now let's plot this\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = bootstrap_estimates |> as.data.frame()\n\nggplot(data = df, aes(x = bootstrap_estimates)) + \n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](bootstrapping_files/figure-html/sampling distribution plot-1.png){width=672}\n:::\n:::\n\n\nEach of these is an estimate of the mean. This is a *sampling distribution.* We can now use this to determine our confidence intervals. If we're using a two-sided test and 95% CIs then can take the 2.5 and 97.5 quantiles! Now we know that our average score is 49.94 and our confidence interval is 46.7, 53.2\n\nHowever, this is the mean. It doesn't account for the number of obstacles on the course or if it was a weekend (perhaps we'd feel more rushed if we were playing on the weekend). So what if we want to fit a model and get 95% CIs for that model?\n\n## Mini-Golf: GLM Time\n\nWe'll use a generalized linear model for our example. Side note: there is no rationale for picking a GLM, other than it's a method that I'm familiar with. In reality, picking the model to use isn't always so straightforward.\\\n\\\nNow, let's simulate some data!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # For reproducibility\n\n# Number of games\nn_games <- 100\n\n# Number of obstacles (simulated)\nobstacles <- round(runif(n_games, 5, 15))\n\n# Weekend (1 if weekend, 0 otherwise)\nweekend <- sample(0:1, n_games, replace = TRUE)\n\n# Mini-golf scores (simulated)\nscores <- 50 + 2 * obstacles + 5 * weekend + rnorm(n_games, 0, 5)\n\n# Data frame\nmini_golf_data <- data.frame(scores, obstacles, weekend)\n```\n:::\n\n\nFirs things first with our data, let's fit a GLM.\n\nNow that we have our data, we can create some bootstrap samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We'll use the tidymodels package here\n\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n```\n:::\n\n```{.r .cell-code}\nboots <- bootstraps(mini_golf_data, times = 1000, apparent = TRUE)\n\n# Now we'll create a function for fitting our model. In this case a GLM\n\nfit_glm <- function(df){\n  glm(scores ~ obstacles + weekend, \n      family = gaussian(link = \"identity\"),\n      data = df \n      )\n}\n\n# Now let's fit this model for each bootstrap sample\n\nboot_glms <- boots %>% \n  dplyr::mutate(\n    model = map(splits, fit_glm),\n    coef_into = map(model, tidy)\n  )\n\n\nboot_coefs <- \n  boot_glms %>% \n  unnest(coef_into)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now we can calculate the confidence interval by taking the 0.025 and 0.95 quantiles. \n\nint_pctl(boot_glms, coef_into)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 6\n  term        .lower .estimate .upper .alpha .method   \n  <chr>        <dbl>     <dbl>  <dbl>  <dbl> <chr>     \n1 (Intercept)  45.5      49.2   52.9    0.05 percentile\n2 obstacles     1.72      2.05   2.37   0.05 percentile\n3 weekend       2.65      4.59   6.39   0.05 percentile\n```\n:::\n:::\n\n\n## Why not use bootstrapping all the time?\n\n\"So what you're telling me is I can just use bootstrapping all the time for every test?\" Well, no. It is a useful tool but like all tools, whether statistical or not, there is a time and a place to use them. Would you use a hammer as a fly swatter? I guess you could, but why not just use a fly swatter.\n\nLike any method, bootstrapping isn't perfect. There are benefits and drawbacks. For example, it can give different results depending on the seed that is chosen. Below are a list of pro/cons:\n\nPros:\n\n-   Simplicity in it's approach\n\n-   Can be used in a variety of situations\n\n-   Simple to understand\n\n-   Can be used for validating models\n\n-   Automatic\n\n-   Flexible\n\n-   Fast coverage\n\nCons:\n\n-   While it's a nonparametric method, we are still assumption that the distribution is a certain shape. If it is skewed, then our result won't be correct. One way to solve this is to use a bias-corrected and accelerated bootstrap\n-   Limited to only those values observed. In the ice cream example, our bowl may only have vanilla, chocolate and cotton candy. But what if in the \"big tub\" that we can't see there is bubblegum flavour? Bootstrapping wouldn't have the bubblegum flavor in any of those\n-   It can take a long time to perform, compared to other methods (i.e., sandwich estimators).\n-   Requires an additional level of thinking\n-   Not guaranteed to give the same results\n-   Computational intensive for large datasets [@hernanwhatif, pp. 174]\n-   Can be sensitive to outliers\n-   Can vary both in computation time and accuracy on the number of bootstraps.\n-   Requires the data to be independent\n-   Involves resampling with replacement so pseudo-replicates rather than true replicates, which might not fully capture the uncertainty in some cases.\n\nBootstrapping is a powerful tool but should be used thoughtfully. Understanding its limitations is crucial for interpreting the results correctly, especially in complex analyses like causal inference. Given its pros and cons, it often serves as a useful complement to other statistical methods.\n\n## What's Next?\n\nBootstrapping is a wide area, this post was meant to be an introduction. I highly recommend the textbook by Hinkely et al. and to try using bootstrapping yourself!\n",
    "supporting": [
      "bootstrapping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}