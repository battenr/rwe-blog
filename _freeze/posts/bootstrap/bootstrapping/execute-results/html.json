{
  "hash": "ca1e029c678cd7ef818d78d33e2272b3",
  "result": {
    "markdown": "---\ntitle: \"Resampling Magic\" # October Post\nsubtitle: \"Demystifying Bootstrapping\"\n#subtitle: \"The Wonders of Bootstrapping Explained\"\nauthor: \"Ryan Batten\"\ndate: \"2023-10-30\"\nimage: bootstraps.png\ncategories: [Resampling, Bootstrapping, Variance Estimation]\nbibliography: bootstrap.bib\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Certainly Uncertain\n\nA key part of any analysis is to determine the uncertainty associated with an estimate. Typically we use confidence intervals to show this. Sometimes deriving this can be tricky with certain methods or when assumptions aren't met. For example, if we use propensity score matching and then fit a generalized linear model in this matched sample, the assumption about the cases being independent isn't true. So what do we do? We need a different way to calculate the standard error. Enter bootstrapping!\n\n::: callout-note\n## Estimating Variance\n\nThis post will focus on bootstrapping but it's important to note that it's not the only solution. For example, a commonly used way to calculate variance with PS methods is the robust variance estimator [@austin2011introduction]. a\n:::\n\n## What exactly *is* bootstrapping?\n\nNo, not like Bootstrap Bill (Pirates of the Caribbean reference). Bootstrapping is a statistical technique that can be quite useful. It comes from the term \"pull oneself up by their bootstraps\" in the sense that we use our sample that we already have. We can use this to estimate confidence intervals! But can also use it for other things. So how exactly does it work?\n\n## How does it work?\n\nBootstrapping works by drawing repeated samples, with replacement aka putting the observations we drew back, from our sample. The idea is that by resampling the data we can use this to imagine what a new sample (of the same size) would look like if we drew from our population again.\n\nThe beauty about bootstrapping is all we need is an estimate and a sampling distribution [@gelman2020regression]. A better way to understand it is with a quick example.\n\nImagine that you go to an ice cream store and order a bowl of ice cream. You get a bowl of ice cream that's 80% vanilla and 20% chocolate but you're curious what the big tub in the back of the store looks like. Is it mostly vanilla? Chocolate? Is it a 70/30 split of either? Are there other flavours? If you asked for another bowl from the same tub would you get the same proportion?\n\nTo try and determine this, you take a spoonful of ice cream randomly and put it in another bowl (of the same size). You do this over and over until you have a new bowl of ice cream. This is magical ice cream though, so once you take a spoonful, that same flavour and amount appears in the bowl again (aka it's *replaced,* so it can appear more than once). This new bowl of ice cream is 60% chocolate and 40% vanilla. You put it back then do it all over again and notice it's 80% vanilla and 20% chocolate. If you do this over and over again, you can use this to try and get an idea of what the big tub in the back looks like.\\\n\\\nThis is bootstrapping. We take samples, with replacement, to try to understand what the whole picture looks like. Let's try another example with some actual numbers!\n\n## Mini-Golf?\n\nLet's use mini-golf as an example. Imagine that you play mini-golf with your friends 100 times over a summer (yes you must really REALLY love mini-golf). Each time you play, you get a different score (the lower the better). Sometimes you do well, sometimes you play awful.\\\n\\\nYou start to think: \"I play a lot of mini-golf, but am I any good? What would my score be on average?\" being the keen person that you are, you also wonder what kind of confidence intervals go with this average. So you decide to use bootstrapping to answer this, using these steps:\n\n1.  Draw 100 scores, from any day (randomly) and write them down. We are doing this with replacement, so the same observation can appear more than once.\n2.  Put them back.\n3.  Draw another 100 scores and write them down.\n4.  Put them back.\n5.  Repeat steps 1-4\n\nNow we have our plan, let's start implementing it!\n\n## Mini-Golf - Average Score\n\nSo we know our average score, if we just take the mean of all the scores. But what is the confidence interval for this mean? To calculate this we often use the standard error while assuming a normal distribution. But what is the standard error? Is it the same as the standard deviation? Not quite.\n\nThe standard error is the standard deviation of the *sampling distribution*. The sampling distribution is the probability distribution of a statistic. This may sound like a bit of jargon, so instead let's use a visual.\n\nFor our example, we are taking 100 scores over and over. Each time we take 100 scores, we calculate the mean. So for each time, we have the mean. Let's plot this!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # Loading the tidyverse package\n\n# Setting seed for reproduceability, arbitrarily chosen in this case\n\nset.seed(123)\n\n\n# Original mini-golf scores, one per day for 100 days. \n\noriginal_scores <- sample(40:60, 100, replace = TRUE)\n\n# Initalizing a variable to store the bootstrap estimates in \n\nbootstrap_estimates <- numeric()\n\n# Number of bootstraps. For this case, arbitrarily choosing 1000. In reality, need to consider what number to use\n\nn_bootstraps <- 1000\n\n# Time to do the bootstrapping!\n\nfor (i in 1:n_bootstraps) {\n  \n  # Draw 10 samples from our original scores, with replacement. That \n  # way you have the same sample each time\n  \n  bootstrap_sample <- sample(original_scores, size = 100, replace = TRUE) \n  \n  # Calculate the mean for our new sample \n  \n  bootstrap_mean <- mean(bootstrap_sample)\n  \n  # Combine our new estimated mean \n  \n  bootstrap_estimates <- c(bootstrap_estimates, bootstrap_mean)\n}\n\n# We can calculate the 95% CI as either finding out the standard error (calculate the standard deviation of bootstrap_estimates) OR can use the corresponding quantiles. \n\n# Estimating the 95% CI using the 0.025 and 0.975 quantiles (two-sided)\n\nestimated_ci <- quantile(bootstrap_estimates, probs = c(0.025, 0.975))\n\nestimated_mean <- mean(original_scores) # Calculating the mean\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = bootstrap_estimates |> as.data.frame()\n\nggplot(data = df, aes(x = bootstrap_estimates)) + \n  geom_bar() +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  ) +\n  ggtitle(\"Sampling Distribution of Mini-Golf Scores for the Mean\") +\n  labs(x = \"Bootstrap Sample Number (1 through n of number of bootstrap replicates)\", y = \"Mean of Sample\")\n```\n\n::: {.cell-output-display}\n![](bootstrapping_files/figure-html/sampling distribution plot-1.png){width=672}\n:::\n:::\n\n\nEach of these is an estimate of the mean. This is a *sampling distribution.* We can now use this to determine our confidence intervals. We can calculate the standard error by calculating the standard deviation of this distribution or, we can do it using percentiles. If we're using a two-sided test and 95% CIs then can take the 0.025 and 0.975 percentiles! Doing this, we now know our average score is 50.66 and our confidence interval is 49.44975, 51.64!\n\nHowever, this is the mean. It doesn't account for the number of obstacles on the course or if it was a weekend (perhaps we'd feel more rushed if we were playing on the weekend, making us play worse). So what if we want to fit a model and get 95% CIs for that model?\n\n## Mini-Golf: GLM Time\n\nWe'll use a generalized linear model for our example. Side note: there is no rationale for picking a GLM, other than it's a method that I'm familiar with. In reality, picking the model to use isn't always so straightforward. Now, let's simulate some data!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting seed for reproducibility, \n\nset.seed(123) # Setting seed for reproduceability, arbitrarily chosen in this case\n\n# Number of games\n\nn_games <- 100\n\n# Number of obstacles on the course that game\n\nobstacles <- round(runif(n_games, 5, 15))\n\n# Is it a weekend? Yes/no (1 if weekend, 0 otherwise)\n\nweekend <- sample(0:1, n_games, replace = TRUE)\n\n# Mini-golf scores, dependent on the number of obstacles and whether it was a weekend or not\n\nscores <- 50 + 2 * obstacles + 5 * weekend + rnorm(n_games, 0, 5)\n\n# Combining it all into a dataframe \n\nmini_golf_data <- data.frame(scores, obstacles, weekend)\n```\n:::\n\n\nWe've got some data! Now, we want to calculate our 95% CIs from a GLM. To do this, we first need to sample some data, then fit a GLM to that new dataset and repeat, repeat, repeat as many times as we have chosen. So, let's do it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We'll use the tidymodels package here to get our multiple samples then fit a model to each sample. \n\nlibrary(tidymodels)\n\n# First, let's get 1000 bootstrapped samples.\n\nboots <- bootstraps(mini_golf_data, times = 1000, apparent = TRUE)\n\n# Now we'll create a function for fitting our model. In this case a GLM\n\nfit_glm <- function(df){\n  \n  # Fitting the glm\n  \n  glm(scores ~ obstacles + weekend, \n      family = gaussian(link = \"identity\"),\n      data = df \n      )\n}\n\n# Now let's fit this model for each of our 1000 bootstrap samples\n\nboot_glms <- boots %>% \n  dplyr::mutate(\n    model = map(splits, fit_glm),\n    coef_into = map(model, tidy)\n  )\n\n# Now we can pull out our coefficients \n\nboot_coefs <- \n  boot_glms %>% \n  unnest(coef_into)\n```\n:::\n\n\nNow we can calculate the 95% confidence intervals based on the same percentile method as before.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now we can calculate the confidence interval by taking the 0.025 and 0.95 quantiles. \n\nint_pctl(boot_glms, coef_into)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 6\n  term        .lower .estimate .upper .alpha .method   \n  <chr>        <dbl>     <dbl>  <dbl>  <dbl> <chr>     \n1 (Intercept)  45.5      49.2   52.9    0.05 percentile\n2 obstacles     1.72      2.05   2.37   0.05 percentile\n3 weekend       2.65      4.59   6.39   0.05 percentile\n```\n:::\n:::\n\n\nJust like that, we have our 95% CIs for our terms. This method seemed to work pretty good and can be useful, so let's just use it all the time! Well, not quite.\n\n## Why not use bootstrapping all the time?\n\n\"So what you're telling me is I can just use bootstrapping all the time for every situtation?\" Well, no. It is a useful tool but like all tools, whether statistical or not, there is a time and a place to use them. Would you use a hammer as a fly swatter? I guess you could, but why not just use a fly swatter.\n\nLike any method, bootstrapping isn't perfect. There are benefits and drawbacks. For example, it can give different results depending on the starting seed. It can also only draw observations from your data that has been observed. In our ice cream example, no bubble gum flavour would show because our bowl only has vanilla and chocolate. However, it's not all bad. Bootstrapping can be a solution in a variety of situations and is quite flexible. It's important to keep the benefits and drawbacks in mind when choosing it (these are only a few pros/cons).\n\n## What's Next?\n\nBootstrapping is a wide area, this post was meant to be an introduction. There are a wide variety of use cases and types of bootstrapping, for example the bias-corrected and accelerated bootstrap. If you're interested in learning more, there is an entire textbook on bootstrapping [@davison1997bootstrap]. Happy bootstrapping!\n",
    "supporting": [
      "bootstrapping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}