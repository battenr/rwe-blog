{
  "hash": "7ce7c925adb0a1bb4b09e62a0056ef15",
  "result": {
    "markdown": "---\ntitle: \"Resampling Magic\"\nsubtitle: \"The Wonders of Bootstrapping and Jackknifing Explained\"\nauthor: \"Ryan Batten\"\ndate: \"2023-08-01\"\ncategories: [Resampling, Bootstrapping, Jackknifing]\nbibliography: bootstrap.bib\ndraft: true\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Certainly Uncertain\n\nA key part of any analysis is to determine the uncertainty associated with an estimate. This has to do with determining the variance of the estimate, which we can use to calculate the standard error. Sometimes this can be especially tricky with certain methods such as inverse probability weighting and standardization. One way to estimate this is using something called bootstrapping.\n\n## What is bootstrapping?\n\nNo, not like Bootstrap Bill (Pirates of the Caribbean), bootstrapping is a statistical technique that can be quite useful. We can use is to estmate confidence intervals, for hypothesis testing, model validation and estimating treatment effects just to name a few things. So how exactly does it work?\n\n## How does it work?\n\nBootstrapping works by drawing repeated samples, with replacement, from our observations. Imagine that we have have a big tub of ice cream that looks like a rainbow, with a bunch of colors all mixed together. We want to know what the whole tub looks like, so what we do is take one scoop and look at the colors in that scoop. Then we put it back, mix it up and take another scoop. We can do this over and over again to try and understand what the whole tub looks like!\\\n\\\nThis is what bootstrapping it. We take repeated samples to try to understand what the whole picture looks like. Let's take a different example and put some numbers to it!\n\n## Mini-Golf Examples\n\nLet's use mini-golf as an example. Imagine that you play mini-golf with your friends 100 times over a summer. Each time you play, you get a different score (the lower the better). Sometimes you do well, sometimes you play awful.\\\n\\\nNow you want to know: \"am I any good at mini-golf? What would my score be on average?\" So you decide to use bootstrapping to answer this. Here's the following steps\n\n1.  Look at 10 scores on random days and write them down.\n2.  Put them back.\n3.  Draw another 10 scores and write them down.\n4.  Put them back.\n5.  Repeat steps 1-4\n\nNow we have our plan, let's start implementing!\n\n## Mini-Golf - Average Score\n\nSo we know our average score, if we just take the mean. But what is the confidence interval for this mean? Well, from statistics we know that the standard error is the standard deviation of it's sampling distribution. The sampling distribution is the probability distribution of a statistic. For example, if we have a bunch of mean values (say 1000) then that is our sampling distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\n# Your 100 mini-golf scores\noriginal_scores <- sample(40:60, 100, replace = TRUE)\n\n# Initialize a variable to store the bootstrap estimates\nbootstrap_estimates <- numeric()\n\n# Number of bootstraps\nn_bootstraps <- 1000\n\n# Bootstrapping\nfor (i in 1:n_bootstraps) {\n  bootstrap_sample <- sample(original_scores, size = 10, replace = TRUE)\n  bootstrap_mean <- mean(bootstrap_sample)\n  bootstrap_estimates <- c(bootstrap_estimates, bootstrap_mean)\n}\n\n# Estimate the 95% confidence interval\nestimated_ci <- quantile(bootstrap_estimates, probs = c(0.05, 0.95))\n\nestimated_mean <- mean(original_scores)\n```\n:::\n\n\nNow we know that our average score is 49.16 and our confidence interval is 45.9, 52.5\n\n## Mini-Golf: GLM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # For reproducibility\n\n# Number of games\nn_games <- 100\n\n# Number of obstacles (simulated)\nobstacles <- round(runif(n_games, 5, 15))\n\n# Weekend (1 if weekend, 0 otherwise)\nweekend <- sample(0:1, n_games, replace = TRUE)\n\n# Mini-golf scores (simulated)\nscores <- 50 + 2 * obstacles + 5 * weekend + rnorm(n_games, 0, 5)\n\n# Data frame\nmini_golf_data <- data.frame(scores, obstacles, weekend)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initialize a matrix to store the bootstrap estimates\nn_bootstraps <- 1000\nglm_estimates <- matrix(0, nrow = n_bootstraps, ncol = 3) # Intercept, obstacles, weekend\n\n# Bootstrapping\nfor (i in 1:n_bootstraps) {\n  bootstrap_indices <- sample(1:n_games, size = n_games, replace = TRUE)\n  bootstrap_data <- mini_golf_data[bootstrap_indices, ]\n  \n  glm_fit <- glm(scores ~ obstacles + weekend, data = bootstrap_data)\n  glm_estimates[i, ] <- coef(glm_fit)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the 95% Confidence Interval\nconf_interval_glm <- apply(glm_estimates, 2, function(x) quantile(x, probs = c(0.025, 0.975)))\n# colnames(conf_interval_glm) <- c(\"2.5%\", \"97.5%\")\n# rownames(conf_interval_glm) <- c(\"Intercept\", \"Obstacles\", \"Weekend\")\n```\n:::\n\n\n## Why not use bootstrapping all the time?\n\nLike any method, bootstrapping isn't perfect. There's benefits and drawbacks to using it. For example, it can give different results depending on the seed that is chosen. Below are a list of pro/cons:\n\nPros:\n\n-   \n\nCons:\n\n-   While it's a nonparametric method, we are still assumption that the distribution is a certain shape. If it is skewed, then our result won't be correct. One way to solve this is to use a bias-corrected and accelerated bootstrap\n\n## Vary Away!\n\n\"So what you're telling me is I can just use bootstrapping all the time for every test?\" Well, no. It is a useful tool but like all tools, whether statistical or not, there is a time and a place to use them. Would you use a hammer as a fly swatter? I guess you could, but why not just use a fly swatter.\n\n## Coffee Beans\n\nImagine that we have a bag of coffee beans. They can vary in shape and size\n\n## What's Next?\n\nBootstrapping is a wide field. I highly recommend the textbook by Hinkely et al. and to try using bootstrapping yourself!\n\n## From ChatGPT \\[Need to Edit\\]\n\n## **Pros and Cons of Bootstrapping**\n\nBootstrapping is a widely-used technique in statistics for its simplicity and versatility. However, like any method, it has its strengths and weaknesses. Below, we explore some of the pros and cons of using bootstrapping.\n\n### **Pros**\n\n1.  **Simplicity**: Bootstrapping is conceptually simple and easy to implement. It doesn't require strong assumptions about the underlying distribution of the data.\n\n2.  **Versatility**: It can be applied to a wide range of statistics and models, making it useful for various types of data analysis, including causal inference.\n\n3.  **Small Sample Sizes**: Bootstrapping can be particularly useful when you have a small sample size, as it allows you to make more robust inferences.\n\n4.  **Non-Parametric**: It's a non-parametric method, meaning it doesn't assume a specific form for the distribution of the data. This makes it flexible and widely applicable.\n\n5.  **Computational Efficiency**: With modern computing power, bootstrapping can be done quickly, even for large datasets.\n\n6.  **Confidence Intervals**: Easily compute confidence intervals for complex estimators where analytical solutions may not be available.\n\n7.  **Model Validation**: Can be used for internal validation of models, assessing the stability of results.\n\n### **Cons**\n\n1.  **Not Always Accurate**: The accuracy of bootstrapping depends on the data and the statistic being estimated. It may not perform well for highly skewed data or for statistics that are sensitive to outliers.\n\n2.  **Computational Cost**: While generally efficient, bootstrapping can be computationally expensive if the dataset is large or if a large number of bootstrap samples are needed.\n\n3.  **Independence Assumption**: Assumes that the observations are independent. If this assumption is violated (e.g., in time-series data), then bootstrapping may give misleading results.\n\n4.  **Limited External Validity**: Bootstrapping only resamples from the existing data, so it can't capture variations or features not present in the original sample. This limits its external validity.\n\n5.  **Pseudo-Replication**: Since bootstrapping involves resampling with replacement, it creates \"pseudo-replicates\" rather than true replicates, which may not fully capture the uncertainty in some cases.\n\n6.  **Boundary Estimates**: For some statistics, bootstrapping can produce biased confidence intervals that do not contain the true parameter value, especially for small sample sizes.\n\n7.  **Not a Substitute for Real Data**: While bootstrapping can enhance an analysis, it's not a substitute for collecting more or better-quality data.\n\n### **Conclusion**\n\nBootstrapping is a powerful tool but should be used thoughtfully. Understanding its limitations is crucial for interpreting the results correctly, especially in complex analyses like causal inference. Given its pros and cons, it often serves as a useful complement to other statistical methods.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}