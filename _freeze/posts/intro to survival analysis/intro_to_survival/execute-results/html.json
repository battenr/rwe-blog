{
  "hash": "52419bdaa3a810739c697829eedfa16a",
  "result": {
    "markdown": "---\ntitle: \"Introducation to Survival Analysis\"\nauthor: \"Ryan Batten\"\ndate: \"2022-09-10\"\ncategories: [Survival Analysis]\n# image: \"iptw.jpeg\"\nbibliography: intro_to_survival.bib\ndraft: true\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Time-to-Event Analyses \n\nRe-weighting in this context has nothing to do with weight. Instead it is a statistical method that is used to adjust for confounding to ensure exchangeability. This method can be helpful for answering questions about the ***marginal*** or ***conditional*** causal effect.\n\nNow, what is a better way to get started than with probability yet again? In this case, we are going to talk about the probability of receiving treatment.\n\n::: callout-note\nThis post will draw heavily from Chapter 12 of What If [@hernanwhatif]\n:::\n\n## Propensity Score\n\nA term that is commonly used in clinical epidemiology is the ***propensity score***. The propensity score is the conditional probability of receiving treatment [@hernanwhatif], or in mathematical notation:\n\n\n$$\nPr[A = 1| L = l]\n$$\n\n\nWhere A is treatment and L is the covariate(s) of interest. While equations are good, examples are better. Imagine we have a clinical trial with two groups, wanting to determine if bouncing up and down on a trampoline causes a headache. The treatment group get to bounce on a trampoline for 10 minutes, while the control group have to sit down on a chair for 10 minutes. In this scenario, the propensity score would be the conditional probability of getting to bounce on the trampoline based on your age, sex and color shirt you are wearing.\n\n## Does jumping on a trampoline cause a headache?\n\nWhile theoretical discussions are helpful, numbers always help to really drive a point home. While we can't conduct a clinical trial, luckily there is a database that has already collected data on such a scenario!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2218) # August 18, 2022\n\nlibrary(tidyverse)\nlibrary(rwetasks)\n\ndf.tramp <- data.frame(\n  group = \"trampoline\",\n  age = runif(n = 260, min = 9, max = 25),\n  sex = rbinom(n = 260, size = 1, prob = 0.77),\n  t_shirt = sample(c(\"blue\", \"pink\", \"orange\", \"yellow\"),\n                   size = 260, \n                   replace = TRUE, \n                   prob = c(0.17, 0.46, 0.30, 0.07)),\n  brain_freeze = rbinom(260, size = 1, prob = 0.33), # brain freeze before group\n  time_standing = runif(n = 260, min = 0, max = 60), # time standing in minutes\n  headache = rbinom(260, size = 1, prob = 0.64)\n) \n\ndf.chair <- data.frame(\n  group = \"chair\",\n  age = runif(n = 469, min = 19, max = 88),\n  sex = rbinom(n = 469, size = 1, prob = 0.28),\n  t_shirt = sample(c(\"blue\", \"pink\", \"orange\", \"yellow\"),\n                   size = 469, \n                   replace = TRUE, \n                   prob = c(0.36, 0.16, 0.42, 0.23)),\n  brain_freeze = rbinom(469, size = 1, prob = 0.48), # brain freeze before group\n  time_standing = runif(n = 469, min = 0, max = 60), # time standing in minutes\n  headache = rbinom(469, size = 1, prob = 0.39)\n)\n\ndf <- rbind(df.tramp, df.chair) %>% \n  dplyr::mutate(\n    beta_age = runif(1, min = 0, max = 0.10), \n    beta_sex = runif(1, min = 0, max = 0.10),\n    beta_tshirt = runif(1, min = 0, max = 0.10),\n    beta_ts = runif(1, min = 0, max = 0.10), \n    prob_tramp = (beta_age*age + beta_sex*sex + beta_ts*time_standing)/10,\n    tramp_group = rbinom(729, size = 1, prob = prob_tramp),\n    prob_headache = (beta_age*age + 2*beta_sex*sex + 3*beta_ts*time_standing)/20,\n    headache = rbinom(729, size = 1, prob = prob_headache),\n  )\n\n# Trampoline Group Demographics\n\ndf.tramp.demo <- df %>% dplyr::filter(tramp_group == 1)\n\n# cbind(mean(df.tramp.demo$age), sd(df.tramp.demo$age)) # age\n# rwetasks::count_percent(df.tramp.demo, sex) # sex\n# rwetasks::count_percent(df.tramp.demo, t_shirt) # t-shirt\n# rwetasks::count_percent(df.tramp.demo, brain_freeze) # brain-freeze\n# cbind(mean(df.tramp.demo$time_standing), sd(df.tramp.demo$time_standing)) # time standing\n# rwetasks::count_percent(df.tramp.demo, headache) # headache\n\n# Chair Demographics\n\ndf.chair.demo <- df %>% dplyr::filter(tramp_group == 0)\n\n# cbind(mean(df.chair.demo$age), sd(df.chair.demo$age)) # age\n# rwetasks::count_percent(df.chair.demo, sex) # sex\n# rwetasks::count_percent(df.chair.demo, t_shirt) # t-shirt\n# rwetasks::count_percent(df.chair.demo, brain_freeze) # brain-freeze\n# cbind(mean(df.chair.demo$time_standing), sd(df.chair.demo$time_standing)) # time standing\n# rwetasks::count_percent(df.chair.demo, headache) # headache\n```\n:::\n\n\n+------------------------------------+----------------+----------------+\n|                                    | Trampoline     | Chair          |\n|                                    |                |                |\n|                                    | (n = 213)      | (n = 516)      |\n+====================================+:==============:+:==============:+\n| Age, mean (SD)                     | 54.2 (22.7)    | 34.2 (21.0)    |\n+------------------------------------+----------------+----------------+\n| Female, n (%)                      | 71 (33.3)      | 258 (50.0)     |\n+------------------------------------+----------------+----------------+\n| T-Shirt Color, n (%)               | 89 (41.8)      | 179 (34.7)     |\n|                                    |                |                |\n| Orange                             | 52 (24.4)      | 119 (23.1)     |\n|                                    |                |                |\n| Blue                               | 39 (18.3)      | 67 (13.0)      |\n|                                    |                |                |\n| Yellow                             | 33 (15.5)      | 151 (29.3)     |\n|                                    |                |                |\n| Pink                               |                |                |\n+------------------------------------+----------------+----------------+\n| Brain Freeze, n (%)                | 100 (46.9)     | 212 (41.1)     |\n+------------------------------------+----------------+----------------+\n| Time Standing (minutes), mean (SD) | 31.0 (16.8)    | 30.1 (17.9)    |\n+------------------------------------+----------------+----------------+\n| Headache                           | 116 (37.8)     | 102 (24.2)     |\n+------------------------------------+----------------+----------------+\n\n: Trampoline Jumpers vs Chair Sitters {#tbl-demo}\n\nNow, if our causal question is \"Does jumping on a trampoline cause a headache?\" we need to look at the two groups that we are comparing. If we review @tbl-demo, do the two groups look similar? Well for starters, the mean age is different. People in the trampoline group are an average age of 54 compared to 34 in the chair sitting group. The other characteristics seem quite different as well. If these two are so different how can we expect to even compare the two?! That's like comparing apples to coconuts!\n\nWell, luckily we can use a statistical method known as inverse probability weighting to make those coconuts look more like apples. Think of it like we are painting the coconuts, and focusing more on the ones that are a similar size and shape.\n\n## Pseudo-Population\n\nThe goal of reweighing is to make a pseudo-population where exchangeability holds. Essentially, in our \"make believe\" sample the two groups would be comparable. @fig-reweighted shows the two different groups and the proportion of male to females. Now, we can make these more similar by reweighting them to align with the overall sample (n = 729).\n\n![Before and After Reweighting](before-and-after-reweighting.png){#fig-reweighted}\n\nOne way to do this, is to fit a logistic regression (since the outcome would be a yes/no) to trampoline jumpers. Using this model, we can predict the probability of someone being a trampoline jumper, or conversely the probability that they are not a trampoline jumper. Once we do that, we can compare the two groups. First, we need to go over some basics of IP weighting.\n\n::: callout-important\nReweighting here is done to estimate the average treatment effect (ATE). Depending upon what the estimand of interest is, the weighting may be different. For example, if you want to estimate the average treatment effect in the treated (ATT), the people in the treated group would receive a weight of 1.0 while those in the control group are reweighted. For more information on choosing the estimand of interest, I recommend reading @greifer2021choosing.\n:::\n\n## IP Weighting\n\nThe pseudo-population is created by weighting each individual by the inverse of the conditional probability of receiving the treatment they did actually receive [@hernanwhatif, pp. 151]. The formula for this is:\n\n\n$$\nW^a = \\frac{1}{f[A|L]}\n$$\n\n\nFor our example, $f[A|L]$ is the probability of being a trampoline jumper conditional on the measured confounders. In mathematical notation:\n\n\n$$\nPr[A = \\text{trampoline jumpers} | \\text{L = age, sex, time standing}]\n$$\n\n\nNow, from probability we know that the total probability has to equal 1. So:\n\n\n$$\nPr[A = \\text{not trampoline jumpers}|L] = 1 - Pr[A = \\text{trampoline jumpers} | L]\n$$\n\n\nGreat! So basically all we need to do is calculate the probability of being a trampoline jumper given measured confounders then we can calculate the conditional probability of not being a trampoline jumper . Now how do we calculate this conditional probability?\n\n### Logistic Regression\n\nSince our two groups can be thought of as a binary variable, trampoline jumpers or not trampoline jumpers, we get a parametric estimate using logistic regression. Assuming that our model is correct, we can then predict/estimate $Pr[A=\\text{trampoline jumper}|L]$. If no confounding for the effect of A in the pseudo-population and the model is correctly specified, then association is causation and an unbiased estimator of the associational difference in the pseudo-population [@hernanwhatif, pp.151] :\n\n\n$$\nE[Headache | A = \\text{trampoline jumper}] - E[Headache | A = \\text{not a trampoline jumper}]\n$$\n\n\nis also an unbiased estimator of the causal difference:\n\n\n$$\nE[Headache^{a = \\text{trampoline jumper}}] - E[Headache^{a = \\text{not a trampoline jumper}}]\n$$\n\n\n## Estimating Weights\n\nNow we are ready to estimate some weights! We will estimate the weights using a logistic regression with the following confounders: age, sex, and time standing.\n\n::: callout-note\nFor this example we have selected age, sex and time standing to be confounders. We know this because it is a simulated dataset however there are approaches for selecting potential confounders. Methods for selecting confounders will be discussed in an upcoming post.\n:::\n\nUsing that model we will calculate the weights, for trampoline jumpers as:\\\n\n$$\n\\hat{W} = \\frac{1}{\\hat{Pr}[A = \\text{trampoline jumper} | L]}\n$$\n\n\nand for not trampoline jumpers as:\n\n\n$$\n\\hat{W} = \\frac{1}{1 - \\hat{Pr}[A = \\text{trampoline jumper} | L]}\n$$\n\n\n::: callout-note\n$\\hat{W}$ and $\\hat{Pr}$ are the estimated, or predicted, values. A logistic regression is used in this case because our outcome is binary (trampoline jumper: yes/no). Using this model we can predict the conditional probability which then is used to calculate the weights.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nmod.fit <- stats::glm(formula = tramp_group ~ age + as.factor(sex) + time_standing + I(age ^ 2) + I(time_standing ^ 2), \n                      family = binomial(link = \"logit\"),\n                      data = df)\n\ndf.weights <- df %>% \n  dplyr::mutate(\n    ps = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ip_weights = 1/ps,\n    half_ipw = 0.5/ps\n  )\n```\n:::\n\n\nNow we have the weights, let's check the summary statistics of them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(df.weights$ip_weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.074   1.163   1.430   1.997   2.039  12.130 \n```\n:::\n:::\n\n\nThe IP weights simulate a pseudo-population where all members of the sample are replaced by two copies of themselves. [@hernanwhatif, pp. 153] One copy receives the treatment value A = 1 and the other copy receives the value A = 0. [@hernanwhatif, pp. 153]. The expected mean of the weights should be 2 because all individuals are included both under treatment and under no treatment.\n\nIf we look back to our example, we can examine the summary statistics of these weights. The mean is sufficiently close to 2, 1.997, which is what we'd expect, however the maximum weight is ***12!!*** For one individual to be weighted as 12, that is rather large. Now there are a few options.\n\nOne would be to create a pseudo-population similar to what we have done, except using 0.5 for the numerator. That is, the unconditional probability of being a trampoline jumper is 0.5 and the unconditional probability of not being a trampoline jumper is 0.5. In this scenario the pseudo-population would be the same size as the study population, and would be equal to if we used $\\frac{1}{f(A|L}$ but divided all the weights by 2 [@hernanwhatif, pp.153]. We can write this more generally\n\n### General Formula\n\nA general formula for $W^a$ is: [@hernanwhatif, pp.153].\n\n\n$$\nW^a = \\frac{p}{f[A|L]}\n$$\n\n\nwhere $p$ is the unconditional probability of treatment. Note: $0< p \\leq 1$, whereas $f[A|L]$ is the probability of treatment based on covariates L. An alternative is for different people to have different probabilities [@hernanwhatif, pp. 153]. A common choice is to use $Pr[A = 1]$ for $p$ in the treated and $Pr[A=0]$ for $p$ in the untreated. $Pr$ in this case would just be the proportion. Using our example again, $Pr[A = \\text{trampoline jumpers}] = \\frac{213}{729} = 0.292$ and for the not trampoline jumpers, $Pr[A = \\text{not trampoline jumper}] = \\frac{516}{729} = 0.708$. If we use these values for the numerator in calculating the weights for our example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndf.weights <- df %>% \n  dplyr::mutate(\n    ps = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ip_weights = dplyr::case_when(\n      tramp_group == 1 ~ 0.292/ps,\n      tramp_group == 0 ~ 0.708/ps\n  )\n  )\n  \nsummary(df.weights$ip_weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4122  0.7843  0.8460  0.9997  1.0805  3.5420 \n```\n:::\n:::\n\n\nThis is notably different from when we used $\\frac{1}{f[A|L]}$. Now the weights range from 0.412 to 3.54 whereas before they ranged from 1.07 to 12.1. Not only that, but now in the pseudo-population, the ratio of trampoline jumpers to not trampoline jumpers is kept. The stabilizing factor, $f[A]$, is responsible for the narrower range [@hernanwhatif, pp. 153]. Weights that use the stabilizng factor are referred to as stabilized weights.\n\n## Stabilized Weights\n\n\n$$\nSW^a = \\frac{f(A)}{f[A|L]}\n$$\n\n\nFrom the above section, we saw that the stabilizing factor made our weights have a narrower range. The mean of the stabilized weights is also expected to be 1 because the size of the pseudo-population is equal to the study population [@hernanwhatif, pp. 153]. This is important to check when conducting an analysis using stabilized weights. An alternative to using the nonparametric estimator (i.e., the proportion of individuals), is to estimate $f[A]$ using the same model but with an intercept and no covariates. If we do that using our example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nmod.intercept <- stats::glm(formula = tramp_group ~ 1, \n                      family = binomial(link = \"logit\"),\n                      data = df)\n\ndf.weights <- df %>% \n  dplyr::mutate(\n    ps.denominator = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                    type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ps.numerator = predict(mod.intercept, type = \"response\"),\n    ip_weights = dplyr::case_when(\n      tramp_group == 1 ~ ps.numerator/ps.denominator,\n      tramp_group == 0 ~ (1-ps.numerator)/(ps.denominator)\n  )\n  )\n  \nsummary(df.weights$ip_weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4125  0.7841  0.8458  0.9997  1.0803  3.5442 \n```\n:::\n:::\n\n\nNow we have weights with a mean that is close to 1 with a maximum of 3.54. There is still one more thing that has to be checked whenever using these weights: the positivity assumption! We have to ensure that all participants have a greater than 0 probability of being a trampoline jumper! After checking this, we are confident that twe can use these weights.\n\n::: callout-note\nWhile checking the summary statistics and positivity assumption are important, it is always a good idea to additional check:\n\n1.  Balance has been achieved for the variables that were included in the model\n\n2.  The distribution of weights, typically done graphically\n\n3.  Check the higher order moments. For example, don't just check to see if mean is similar for a continuous variable, but that standard deviation is similar as well.\n:::\n\nNow which method would we prefer for estimating the weights? Well this comes down to opinion. For this example, both methods give very similar results for the weights. My personal preference is to use the parametric estimated weights, using logistic regression, for both the numerator and denominator rather than a parametric estimator for the denominator and nonparametric for the numerator.\n\n## Stabilized or Nonstabilized?\n\nAt this point, you may be wondering to yourself if we should be using stabilized or nonstabilized weights. One reason is stabilized weights result in narrower 95% CIs [@hernanwhatif, pp. 154]. However, this only occurs when the model is not saturated. A model is saturated when the number of parameters equals the number of quantities to be estimated. For example, $E[Y|A] = \\beta_0 + \\beta_1*A$ is a saturated model because it has two parameters, $\\beta_0$ and $\\beta_1$, and two quantities to estimate $E[Y|A = 1]$ and $E[Y|A = 0]$ [@hernanwhatif, pp. 151]. Keep in mind this example is for a binary variable, however this becomes nearly impossible to meet for continuous variables since you would need to have a parameter for every value of that variable.\n\n## Did it work?\n\nFor any adjustment technique that aims to achieve balance, such as IPTW, entropy balancing, weights used as part of matching adjusted indirect comparisons (MAICs), we need to check to see if balance has actually been met. In this case, we were trying to balance on the confounders age, sex and time standing. We can now check to see if this is achieved.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(Hmisc)\n\ntramp.wt <- df.weights %>% dplyr::filter(tramp_group == 1)\n\n# Age \nx <- tramp.wt$age\nwt <- tramp.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Sex\n\n# df.weights %>% \n  # dplyr::filter(tramp_group == 1) %>% \n  # count(sex, \n        #wt = ip_weights)\n\n# Time Standing\n\nx <- tramp.wt$time_standing\nwt <- tramp.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Chair Sitters\n\nchair.wt <- df.weights %>% dplyr::filter(tramp_group == 0)\n\n\n# Age \nx <- chair.wt$age\nwt <- chair.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Sex\n\n# df.weights %>% \n#   dplyr::filter(tramp_group == 0) %>% \n#   count(sex, \n#         wt = ip_weights)\n\n# Time Standing\n\nx <- chair.wt$time_standing\nwt <- chair.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n```\n:::\n\n\n+--------------------------+--------------------+---------------+----------------------+\n|                          | Trampoline Jumpers | Chair Sitters | Overall (unweighted) |\n|                          |                    |               |                      |\n|                          | (n = 213)          | (n = 516)     | (n = 729)            |\n+==========================+:==================:+:=============:+:====================:+\n| Age, mean (SD)           | 40.1 (23.5)        | 40.2 (23.4)   | 40.1 (23.3)          |\n+--------------------------+--------------------+---------------+----------------------+\n| Female, n (%)            | 92 (43.9)          | 231 (44.8)    | 329 (45.1)           |\n+--------------------------+--------------------+---------------+----------------------+\n| Time Standing, mean (SD) | 32.0 (17.7)        | 30.5 (17.6)   | 30.4 (17.6)          |\n+--------------------------+--------------------+---------------+----------------------+\n\n: Characteristics After Reweighting {#tbl-weighted}\n\n@tbl-weighted shows the characteristics that we included in the regression model after reweighting using stabilized weights. The mean of both groups is now similar, which consequently is also close to the overall unweighted sample. If these values don't look any different to you, then compare them to those show in @tbl-demo.\n\n## Drumroll Please!\n\nNow we are finally ready to answer our question! We will use a logistic regression since our outcome is binary, headache: yes/no, with our new fancy weights!\n\n::: callout-note\nTo determine the 95% CI we need to use a method that takes the IP weighting into account [@hernanwhatif, pp. 152]. One approach is to use nonparametric bootstrapping. Another approach is to use the robust variance estimator. Here we will use the robust variance estimator, however it is important to note that the robust variance estimator is conservative since it covers the super-population parameter more than 95% of the time. [@hernanwhatif, pp. 152]\n\n(Thanks to Giusi Moffa for pointing out I should be more clear about the CIs!)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(sandwich)\n\nmod.outcome <- stats::glm(\n  formula = headache ~ tramp_group,\n  family = binomial(link = \"logit\"),\n  data = df.weights,\n  weights = ip_weights\n)\n\nsandwich_se <- diag(sandwich::vcovHC(mod.outcome, type = \"HC\"))^0.5\n\noutput <- broom::tidy(mod.outcome) %>% \n  dplyr::filter(\n    term == \"tramp_group\"\n  ) %>% \n  dplyr::mutate(\n    log.upper.ci = estimate + qnorm(0.975)*0.221,\n    log.lower.ci = estimate - qnorm(0.975)*0.221,\n    OR = exp(estimate),\n    upper.ci = exp(log.upper.ci),\n    lower.ci = exp(log.lower.ci)\n  )\n\npaste0(\"OR, \", round(output$OR,3), \";\",\n       \" (95% CI, \", round(output$lower.ci, 2), \"-\", \n       round(output$upper.ci,2), \")\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"OR, 1.001; (95% CI, 0.65-1.54)\"\n```\n:::\n:::\n\n\nUsing our stabilized weights that we calculated earlier, we can now answer our causal question: \"Does jumping on a trampoline cause headaches?\". Based upon our model, the answer is no since our 95% CI includes unity, aka 1. We are now free to jump on trampolines without the worry of headaches!!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}