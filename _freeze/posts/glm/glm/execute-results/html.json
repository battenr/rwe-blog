{
  "hash": "edbc0caadffb58f8da71aedd4a66d197",
  "result": {
    "markdown": "---\nttitle: \"Generalized Linear Model\"\nauthor: \"Ryan Batten\"\ndate: \"2022-08-28\"\ncategories: [GLM, Regression]\n# image: \"iptw.jpeg\"\nbibliography: glm.bib\ndraft: true\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Regression: Swiss Army Knife of Statistics\n\nRegression is a commonly used tool in statistics. Typically, the starting point is with ordinary least squares (OLS) regression. This is where we will also start. Well? Let's get started!\n\n## Equation of a Line\n\nFrom high school math, you probably remember the equation of a line as:\n\n\n$$\ny = mx +b\n$$\n\n\nwhere $m$ is the slope of the line, $x$ is the value of the dependent variable and $b$ is the y-intercept of the line. We can build upon this simple equation for linear regression.\n\n## Ordinary Regression\n\nThe typical place to start with regression is ordinary least squares. This method tries to minimize the sum of squares of the differences between the observed variable and those predicted by the model (hence the *least squares*). Put otherwise: by minimizing the residuals. Another way to conceptualize this is by drawing the line of best fit.\\\n\\\nFor example, imagine we have a database and want to examine the association between age and the number of balloons that a person owns. Our database has four variables: age, sex, candy lover (yes/no) and the number of balloons the person owns. Now, if we draw a line of best fit through the data, we'll get the below figure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2228) # August 28, 2022\n\nlibrary(tidyverse)\nlibrary(ggxmean)\n\nn.id = 250\n\ndf <- data.frame(\n  age = runif(n = n.id, min = c(5, 18), max = c(30, 85)),\n  sex = rbinom(n = n.id, size = 1, prob = c(0.60,0.20)),\n  candy_lover = rbinom(n = n.id, size = 1, prob = c(0.64, 0.45)),\n  \n  # Beta Coefficients\n  \n  beta_age = runif(n = n.id, min = 0, max = 0.10),\n  beta_sex = runif(n = n.id, min = 0, max = 0.30),\n  beta_candy = runif(n = n.id, min = 0, max = 0.40)\n) %>% \n  dplyr::mutate(\n    num_ballons = round((beta_age*age + beta_sex*sex + beta_candy*candy_lover)*3,0) # number of ballons that you own\n  )\n\nggplot2::ggplot(data = df, mapping = aes(x = age, y = num_ballons)) +\n  ggplot2::geom_point(color = \"red\") +\n  geom_lm() +\n  ggxmean::geom_lm_residuals(linetype = \"dashed\") +\n  labs(x = \"Age\", y = \"Number of Ballons Owned\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](glm_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIf we look at the distance from the red dot to blue line for each point, this is the measurement from the observed value (red dot) to the predicted value (on the blue line). This is also known as the residuals (in the figure the dashed black line).\n\nOf course this is great in theory to draw the line of best fit, however there is also a mathematical equation that is more useful in practice for determining this.\n\n### Using Equations and Matrices\n\nWe can find the estimated values of the parameters (i.e., $\\beta_1$, $\\beta_2$, $\\beta_3$) using the below equation. Note that these variables are referring to the matrix, rather than a specific variable.\n\n\n$$\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n$$\n\n\nwhere $\\hat{\\beta}$ is the ordinary least squares estimator, $X$ is the matrix containing the predictor variables and $y$ is the vector of the response variable. For our example, $X$ is the matrix containing the values of an intercept and values for the following variables: age, sex and candy lover.\n\n::: callout-note\nFor those not familiar with linear algebra, the superscript T is called the transpose. Similarly, the -1 superscript is referred to as the inverse of a matrix.\n:::\n\nAbstract concepts can be helpful however an example is always better. Using our data (showing the first 6 rows):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX = as.matrix(cbind(1, df$age, df$sex, df$candy_lover))\n\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]     [,2] [,3] [,4]\n[1,]    1 29.23506    1    0\n[2,]    1 54.38400    0    1\n[3,]    1 27.16523    0    0\n[4,]    1 44.35563    0    0\n[5,]    1 29.51780    1    1\n[6,]    1 75.95165    0    1\n```\n:::\n:::\n\n\nAs you can see, the first column is all 1s. The second is the value for age, third is sex (1 = female, 0 = male) and fourth is the value for candy lover (1 = yes, 0 = no). Now for the y matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny = as.matrix(df$num_ballons)\n\nhead(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    2\n[2,]   10\n[3,]    2\n[4,]   12\n[5,]    6\n[6,]    2\n```\n:::\n:::\n\n\n#### Estimating Parameters\n\nWe can estimate the parameters using the below formula and plugging in our matrices.\n\n\n$$\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n$$\n\n\nIf we use this equation with our example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Doing it step by step:\n\nstep.1 <- t(X)%*%X\nstep.2 <- solve(step.1) # solve will return the inverse of a\nstep.3 <- step.2%*%t(X)\nstep.4 <- step.3%*%y\nbeta <- step.4\n\n# Alternatively, can do in one messy looking code: \n\nbeta <- solve(t(X)%*%X)%*%t(X)%*%y\n\n# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code all in one\n\nbeta\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,]  0.09488954\n[2,]  0.14316794\n[3,] -0.09891130\n[4,]  0.96749456\n```\n:::\n:::\n\n\nNow we are ready to move onto calculating the variance covariance matrix, which we can use to derive our standard errors.\n\n#### Estimating Variance\n\nThe first step is to calculate the residuals. We can do that using the below formula:\n\n\n$$\nResiduals = y - \\beta_1 - \\beta_2*age - \\beta_3*sex - \\beta_4*\\text{candy lover}\n$$\n\n\nOnce we know the residuals, as a matrix, we can calculate the variance-covariance matrix as\n\n\n$$\nVCov = \\frac{1}{n-k}(RES^TRES)*(X^TX)^{-1}\n$$\n\n\nwhere $n, k, RES, X$ are the number of observations, number of parameters estimated, residual matrix and matrix of values for the predictor variables. Back to our example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculating residuals as y-intercept-beta_1*X1-beta_2*X2\n\nres <- as.matrix(y-beta[1]-beta[2]*X[,2]-beta[3]*X[,3] - beta[4]*X[,4])\n\n# Note the above is really:\n# res <- as.matrix(y-beta[1]-beta[n]*X[,n]) for as many n's as there are \n\n# Variance-Covariance Matrix (VCV) \n\n# Formula for VCV: (1/df)*t(res)*res*[t(X)(X)]^-1\n\nn = nrow(df)\nk = ncol(X)\n\nVCV <- (1/(n-k))*as.numeric(t(res)%*%res)*solve(t(X)%*%X)\n```\n:::\n\n\n#### Standard Error\n\nWe can calculate the standard error from a variance covariance matrix as the square root of the diagonal values. Doing this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse <- sqrt(diag(VCV))\n```\n:::\n\n\nSo using the equations and matrices results in the following output\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_value <- rbind(\n  2*pnorm(abs(beta[1]/se[1]), lower.tail = FALSE),\n  2*pnorm(abs(beta[2]/se[2]), lower.tail = FALSE),\n  2*pnorm(abs(beta[3]/se[3]), lower.tail = FALSE),\n  2*pnorm(abs(beta[4]/se[4]), lower.tail = FALSE)\n)\n\n#... Output ----\n\noutput <- as.data.frame(\n  cbind(\n    c(\"Intercept\", \"Age\", \"Sex\"),\n    round(beta,5),\n    round(se, 5),\n    round(p_value, 4)\n)\n)\n\nnames(output) <- c(\n  \"Coefficients\",\n  \"Estimate\",\n  \"Std. Error\",\n  \"Pr(>{Z}\"\n)\n\noutput\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Coefficients Estimate Std. Error Pr(>{Z}\n1    Intercept  0.09489    0.61339  0.8771\n2          Age  0.14317    0.01154       0\n3          Sex -0.09891     0.5431  0.8555\n4    Intercept  0.96749     0.4874  0.0471\n```\n:::\n:::\n\n\n#### Comparing to lm()\n\nComparing these values to if we use the *lm()* function in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.fit <- lm(num_ballons ~ age + sex + candy_lover, \n              data = df)\n\nsummary(mod.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = num_ballons ~ age + sex + candy_lover, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9362 -2.0570  0.2046  1.6230 11.9744 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.09489    0.61339   0.155   0.8772    \nage          0.14317    0.01154  12.409   <2e-16 ***\nsex         -0.09891    0.54310  -0.182   0.8556    \ncandy_lover  0.96749    0.48740   1.985   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.732 on 246 degrees of freedom\nMultiple R-squared:  0.423,\tAdjusted R-squared:  0.4159 \nF-statistic:  60.1 on 3 and 246 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nNow that we have covered linear regression, we are ready to move onto generalized linear models (GLM).\n\n## Generalized Linear Model\n\n### Main Parts of GLM\n\nThere are three components to any GLM [@stat504]:\n\n1.  Random Component. This specifies the probability distribution of the response variable. Essentially, we are selecting the distribution for the\n\n2.  Systematic Component\n\n3.  Link Function\n\nAnother way to think of these three components are as the response variable (aka y), explanatory variables (aka x) and how they are connected.\n\nThere are some assumptions as well:\n\n-   The data are independently distributed\n\n-   The dependent variable typically assumes a distribution from an exponential family (i.e., normal, binomial, Poisson, etc.)\n\n-   A linear relationship between the transformed expected response in terms of the link function and explanatory variables (however ***not*** in terms of the response and explanatory variables)\n\n-   Errors need to be independent but not normally distributed\n\n### Maximum Likelihood Estimation\n\nOne of the key differences between OLS and GLM is the way that parameters (aka coefficients) are estimated. While OLS uses ordinary least squares, GLMs use something called maximum likelihood estimation (MLE). MLE is like what it sounds: it maximizing the likelihood function so that the model uses values that make the observed data most probable (aka most likely). There are different methods to determine this value for MLE including solving the derivative of the likelihood funciton where it is 0, hence the maxima, or more iterative procedures such as Gradient descent method or Newton-Raphson method.\n\nHere we will focus on iteratively reweighted least squares since that is what R uses by default.\n\n#### Iteratively Reweighted Least Squares\n\nIteratively reweighted least squares (IWLS), is an algorithm that is used to determine the parameters and standard errors of the parameters. We'll use logistic regression to walk through the steps, although for other link functions it is a similar process. The steps for IWLS are outlined below [@fox2014]\n\n1.  Set the regression coefficients to initial values. For our example, we will start with 0\n\n2.  For each iteration, *t,* calculate the fitted probabilities, $\\mu$, variance-function values, $v$, working-response values, $z$, and weights, $w$.\n\n    $\\mu_i^{(t)} = [1 + exp(-\\eta_i^{(t)})]^{-1}$\n\n    $v_i^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$\n\n    $z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)})/v_i^{(t)}$\n\n    $w_i^{(t)} = n_i*v_i$\n\n    Note: $n_i$ represents the binomial denominator for the ith observation. For binary data, all of the $n_i$ are 1.\n\n3.  Regress the working response on the predictors using weighted least squares, minimizing the weighted residual sum of squares\n\n    $\\sum\\limits_{i = 1}^{n}w_i^{(t)}(z_i^{(t)} - x_i^{'}\\beta)^2$\n\n    where $x_i^{'}$ is the *i*th row of the model matrix.\n\n4.  Repeat steps 2 and 3 until the regression coefficients stabilize at the maximum-likelihood estimator $\\hat\\beta$\n\n5.  Calculate the estimated asymptotic covariance matrix of the coefficients as\n\n    $\\hat{V}(\\hat{\\beta}) = (X^{'}WX)^{-1}$\n\n    where $W = \\text{diag}\\text{(}w_i\\text{})$ is the diagonal matrix of weights from the last iteration and $X$ is the model matrix.\n\nReading through steps can be helpful but an example is always better. Let's work through this in R. First we'll want to make a function to calculate IWLS implementing these steps (credit to Michael Clark for code for implementing IWLS, link in below code snippet)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Link to code that this is built off of: \n\n# https://m-clark.github.io/models-by-example/newton-irls.html#comparison-42\n\niwls <- function(X, y, tol = 1e-7, iter = 500){\n  \n  # Note: tol = 1e-7 is used by the lsfit function\n  \n  # First we need to start with some inital values\n  \n  int = log(mean(y)) / (1-mean(y)) # intercept\n  beta = c(int, rep(0, ncol(X) -1))\n  currtol = 1\n  it = 0\n  ll = 0 # log likelihood\n  \n  # As long as the tolerance calculate is greater than what we will allow we want the code to repeat\n  \n  while(currtol > tol && it < iter){\n    it = it + 1\n    ll_old = ll\n    \n    eta = X %*% beta\n    mu = plogis(eta)[,1]\n    s = mu*(1-mu)\n    S = diag(s)\n    z = eta + (y-mu)/s\n    beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z))\n    var = solve((t(X) %*% S %*% X))\n    \n    ll = sum(\n      dbinom(\n        y, \n        prob = plogis(X%*% beta), \n        size = 1, \n        log = TRUE)\n      )\n    \n    currtol = abs(ll - ll_old)\n  }\n  \n  list(\n    beta = beta, \n    var = var, \n    se = diag(sqrt(var)), # SE = sqrt(var) but we want diagnoals of the variance-covariance matrix \n    iter = it, \n    tol = currtol, \n    loglik = ll, \n    weights = plogis(X %*% beta) * (1 - plogis(X %*% beta))\n  )\n  \n}\n```\n:::\n\n\n### Comparing our method to glm()\n\nNow that we've written a function that calculates estimates of the parameters and standard errors, we need to see if it works! What's a better way to check than comparing with a well-established method? We'll compare our function to that from glm, although admittedly our output is not as clean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nX <- cbind(1, df$age, df$sex, df$num_ballons) %>% as.matrix()\ny <- df$candy_lover %>% as.matrix()\n\nour.way <- iwls(X, y)\n\nour.way$beta\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,]  0.23378250\n[2,] -0.01323819\n[3,]  0.68071575\n[4,]  0.06809751\n```\n:::\n\n```{.r .cell-code}\nsummary(our.way)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Length Class  Mode   \nbeta      4    -none- numeric\nvar      16    -none- numeric\nse        4    -none- numeric\niter      1    -none- numeric\ntol       1    -none- numeric\nloglik    1    -none- numeric\nweights 250    -none- numeric\n```\n:::\n\n```{.r .cell-code}\ndiag(our.way$se)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]        [,2]      [,3]      [,4]\n[1,] 0.300512 0.000000000 0.0000000 0.0000000\n[2,] 0.000000 0.008062909 0.0000000 0.0000000\n[3,] 0.000000 0.000000000 0.3057791 0.0000000\n[4,] 0.000000 0.000000000 0.0000000 0.0354048\n```\n:::\n\n```{.r .cell-code}\nglm.way <- glm(candy_lover ~ age + sex + num_ballons, \n               family = binomial(link = \"logit\"), \n               data = df)\n```\n:::\n\n\n+---------------------------+----------------------------------+------------------------------+\n| Parameter                 | Estimate (SE)                    | Estimate (SE)                |\n|                           |                                  |                              |\n|                           | *glm()*                          | our method                   |\n+:=========================:+:================================:+:============================:+\n| Intercept                 | 0.233782 (0.300512)              | 0.233783 (0.300512)          |\n+---------------------------+----------------------------------+------------------------------+\n| Age                       | -0.013238 (0.008063)             | -0.013238 (0.008063)         |\n+---------------------------+----------------------------------+------------------------------+\n| Sex                       | 0.680716 (0.305779)              | 0.680716 (0.305779)          |\n+---------------------------+----------------------------------+------------------------------+\n| Number of Balloons        | 0.068098 (0.35405)               | 0.068098 (0.035405)          |\n+---------------------------+----------------------------------+------------------------------+\n\n: Output from our function compared to glm()\n\nLooking at the estimates from the *glm()* function to our function...nearly identical results! Yippee! This is also what we'd expect. We can also look at the weights from the last iteration for both the *glm()* method and using our function. The code is in the below snippet, however rather than boring you with weights for 250 observations, I will leave that up to you to review if you are interested (tldr: they are quite similar).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult.weights <- cbind(glm.way$weights, our.way$weights) %>% as.data.frame()\n```\n:::\n\n\n## Conclusion\n\nSince every good story must come to an end, so too does our GLM by hand exercise...but fear not! You can now use this to foray into the world of GLM with a better understanding of how these parameters are calculated!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}