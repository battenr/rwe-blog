{
  "hash": "c8eff1612cbce24348490d0b324bf2a5",
  "result": {
    "markdown": "---\nttitle: \"Generalized Linear Model\"\nauthor: \"Ryan Batten\"\ndate: \"2022-08-28\"\ncategories: [GLM, Regression]\n# image: \"iptw.jpeg\"\n# bibliography: glm.bib\ndraft: true\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Regression: Swiss Army Knife of Statistics\n\nRegression is a commonly used tool in statistics. Typically, the starting point is with ordinary least squares (OLS) regression. This is where we will also start. Well? Let's get started!\n\n## Equation of a Line\n\nFrom high school math, you probably remember the equation of a line as:\n\n\n$$\ny = mx +b\n$$\n\n\nwhere $m$ is the slope of the line, $x$ is the value of the dependent variable and $b$ is the y-intercept of the line. We can build upon this simple equation for linear regression.\n\n## Ordinary Regression\n\nThe typical place to start with regression is ordinary least squares. This method tries to minimize the sum of squares of the differences between the observed variable and those predicted by the model (hence the *least squares*). Put otherwise: by minimizing the residuals. Another way to conceptualize this is by drawing the line of best fit.\\\n\\\nFor example, imagine we have a database and want to examine the association between age and the number of balloons that a person owns. Our database has four variables: age, sex, candy lover (yes/no) and the number of balloons the person owns. Now, if we draw a line of best fit through the data, we'll get the below figure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2228) # August 28, 2022\n\nlibrary(tidyverse)\nlibrary(ggxmean)\n\nn.id = 250\n\ndf <- data.frame(\n  age = runif(n = n.id, min = c(5, 18), max = c(30, 85)),\n  sex = rbinom(n = n.id, size = 1, prob = c(0.60,0.20)),\n  candy_lover = rbinom(n = n.id, size = 1, prob = c(0.64, 0.45)),\n  \n  # Beta Coefficients\n  \n  beta_age = runif(n = n.id, min = 0, max = 0.10),\n  beta_sex = runif(n = n.id, min = 0, max = 0.30),\n  beta_candy = runif(n = n.id, min = 0, max = 0.40)\n) %>% \n  dplyr::mutate(\n    num_ballons = round((beta_age*age + beta_sex*sex + beta_candy*candy_lover)*3,0) # number of ballons that you own\n  )\n\nggplot2::ggplot(data = df, mapping = aes(x = age, y = num_ballons)) +\n  ggplot2::geom_point(color = \"red\") +\n  geom_lm() +\n  ggxmean::geom_lm_residuals(linetype = \"dashed\") +\n  labs(x = \"Age\", y = \"Number of Ballons Owned\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](glm_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIf we look at the distance from the red dot to blue line for each point, this is the measurement from the observed value (red dot) to the predicted value (on the blue line). This is also known as the residuals (in the figure the dashed black line).\n\nOf course this is great in theory to draw the line of best fit, however there is also a mathematical equation that is more useful in practice for determining this.\n\n### Using Equations and Matrices\n\nWe can find the estimated values of the parameters (i.e., $\\beta_1$, $\\beta_2$, $\\beta_3$) using the below equation. Note that these variables are referring to the matrix, rather than a specific variable.\n\n\n$$\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n$$\n\n\nwhere $\\hat{\\beta}$ is the ordinary least squares estimator, $X$ is the matrix containing the predictor variables and $y$ is the vector of the response variable. For our example, $X$ is the matrix containing the values of an intercept and values for the following variables: age, sex and candy lover.\n\n::: callout-note\nFor those not familiar with linear algebra, the superscript T is called the transpose. Similarly, the -1 superscript is referred to as the inverse of a matrix.\n:::\n\nAbstract concepts can be helpful however an example is always better. Using our data (showing the first 6 rows):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX = as.matrix(cbind(1, df$age, df$sex, df$candy_lover))\n\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]     [,2] [,3] [,4]\n[1,]    1 29.23506    1    0\n[2,]    1 54.38400    0    1\n[3,]    1 27.16523    0    0\n[4,]    1 44.35563    0    0\n[5,]    1 29.51780    1    1\n[6,]    1 75.95165    0    1\n```\n:::\n:::\n\n\nAs you can see, the first column is all 1s. The second is the value for age, third is sex (1 = female, 0 = male) and fourth is the value for candy lover (1 = yes, 0 = no). Now for the y matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny = as.matrix(df$num_ballons)\n\nhead(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    2\n[2,]   10\n[3,]    2\n[4,]   12\n[5,]    6\n[6,]    2\n```\n:::\n:::\n\n\n#### Estimating Parameters\n\nWe can estimate the parameters using the below formula and plugging in our matrices.\n\n\n$$\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n$$\n\n\nIf we use this equation with our example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Doing it step by step:\n\nstep.1 <- t(X)%*%X\nstep.2 <- solve(step.1) # solve will return the inverse of a\nstep.3 <- step.2%*%t(X)\nstep.4 <- step.3%*%y\nbeta <- step.4\n\n# Alternatively, can do in one messy looking code: \n\nbeta <- solve(t(X)%*%X)%*%t(X)%*%y\n\n# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code all in one\n\nbeta\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,]  0.09488954\n[2,]  0.14316794\n[3,] -0.09891130\n[4,]  0.96749456\n```\n:::\n:::\n\n\nNow we are ready to move onto calculating the variance covariance matrix, which we can use to derive our standard errors.\n\n#### Estimating Variance\n\nThe first step is to calculate the residuals. We can do that using the below formula:\n\n\n$$\nResiduals = y - \\beta_1 - \\beta_2*age - \\beta_3*sex - \\beta_4*\\text{candy lover}\n$$\n\n\nOnce we know the residuals, as a matrix, we can calculate the variance-covariance matrix as\n\n\n$$\nVCov = \\frac{1}{n-k}(RES^TRES)*(X^TX)^{-1}\n$$\n\n\nwhere $n, k, RES, X$ are the number of observations, number of parameters estimated, residual matrix and matrix of values for the predictor variables. Back to our example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculating residuals as y-intercept-beta_1*X1-beta_2*X2\n\nres <- as.matrix(y-beta[1]-beta[2]*X[,2]-beta[3]*X[,3] - beta[4]*X[,4])\n\n# Note the above is really:\n# res <- as.matrix(y-beta[1]-beta[n]*X[,n]) for as many n's as there are \n\n# Variance-Covariance Matrix (VCV) \n\n# Formula for VCV: (1/df)*t(res)*res*[t(X)(X)]^-1\n\nn = nrow(df)\nk = ncol(X)\n\nVCV <- (1/(n-k))*as.numeric(t(res)%*%res)*solve(t(X)%*%X)\n```\n:::\n\n\n#### Standard Error\n\nWe can calculate the standard error from a variance covariance matrix as the square root of the diagonal values. Doing this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse <- sqrt(diag(VCV))\n```\n:::\n\n\nSo using the equations and matrices results in the following output\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_value <- rbind(\n  2*pnorm(abs(beta[1]/se[1]), lower.tail = FALSE),\n  2*pnorm(abs(beta[2]/se[2]), lower.tail = FALSE),\n  2*pnorm(abs(beta[3]/se[3]), lower.tail = FALSE),\n  2*pnorm(abs(beta[4]/se[4]), lower.tail = FALSE)\n)\n\n#... Output ----\n\noutput <- as.data.frame(\n  cbind(\n    c(\"Intercept\", \"Age\", \"Sex\"),\n    round(beta,5),\n    round(se, 5),\n    round(p_value, 4)\n)\n)\n\nnames(output) <- c(\n  \"Coefficients\",\n  \"Estimate\",\n  \"Std. Error\",\n  \"Pr(>{Z}\"\n)\n\noutput\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Coefficients Estimate Std. Error Pr(>{Z}\n1    Intercept  0.09489    0.61339  0.8771\n2          Age  0.14317    0.01154       0\n3          Sex -0.09891     0.5431  0.8555\n4    Intercept  0.96749     0.4874  0.0471\n```\n:::\n:::\n\n\n#### Comparing to lm()\n\nComparing these values to if we use the *lm()* function in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.fit <- lm(num_ballons ~ age + sex + candy_lover, \n              data = df)\n\nsummary(mod.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = num_ballons ~ age + sex + candy_lover, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9362 -2.0570  0.2046  1.6230 11.9744 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.09489    0.61339   0.155   0.8772    \nage          0.14317    0.01154  12.409   <2e-16 ***\nsex         -0.09891    0.54310  -0.182   0.8556    \ncandy_lover  0.96749    0.48740   1.985   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.732 on 246 degrees of freedom\nMultiple R-squared:  0.423,\tAdjusted R-squared:  0.4159 \nF-statistic:  60.1 on 3 and 246 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Generalized Linear Regression\n\n### Main Parts of GLM\n\nThere are three components to any GLM\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}