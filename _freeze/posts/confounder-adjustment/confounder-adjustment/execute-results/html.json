{
  "hash": "a9738173e795ad439c43a6d0a32921fa",
  "result": {
    "markdown": "---\ntitle: \"Damn Confounders. I need to adjust\" \n# subtitle: \"Standardization and the Parametric G-Formula\"\n# some potential title ideas: \nauthor: \"Ryan Batten\"\ndate: \"2023-03-02\"\ncategories: [DAGs, Causal Diagrams]\n# image : \"standards.png\"\ndraft: true\nformat: \n  html:\n    toc: true\n    toc-title: Contents\n    toc-location: right\n    toc-depth: 4\n    code-fold: true\n---\n\n\n## Table 2 Fallacy\n\nSo what should we report? There was a great paper that was published in\n\n## \"Darn, Another Graph?!\"\n\nSure, how about \"Don't Analyze Graphs\"? It's a bit tongue-in-cheek, implying that you should just trust the Directed Acyclic Graph (DAG) without over-analyzing it. Of course, in reality, careful analysis is often necessary to ensure the accuracy and validity of DAGs used in various applications such as machine learning.\n\nHow about \"Dramatic Action Graph\"? It's not necessarily hilarious, but it adds a bit of flair to the otherwise technical-sounding term DAG!\n\n### What is standardization and why do we care?\n\n\"You can't compare those! That's like comparing apples and oranges\". Standardization can help with this. Say that you have two different things that you want to compare, for example apples and oranges, the big question is how will you compare them?\n\nYou are actually already familiar with standardization, you just may not think about it. One common example is height. Depending on where you live you probably measure height in inches (feet and inches), or centimeters.\n\nNow this naturally leads in\n\n### How do we standardize?\n\nAlright so we've covered the what and why, but how do we apply that in this case? Well in a similar fashion, albeit a bit different.\n\nI think first it's important to go over some of high level points of standardizing in general. First, what is standardizing? I'm sure you're familiar with the phrase \"you can't compare these, that's like comparing apples and oranges!\" and there is some truth to that. What if you could compare them with respect to the amount of juice a single one produces? Or how far away from the color blue they are? (Alright, bad example I admit but you get the point). The reasons that we standardize is to allow for a observations to be on a common scale. For example, you can't compare apples to oranges but you **can** compare the volume of juice contained in an apple to that of an orange.\n\n## Why should we standardize something?\n\nWhat exactly is standardizing?\n\n## How do we standardize something?\n\nIn general, standardization is typically performed by standardizing the data\n\ndescribe standardization\n\nStandardization is the process of transforming data into a common format by adjusting the values of the observations to a common scale. This is done by subtracting the mean of the data from each observation and then dividing by the standard deviation. This process helps to make data more comparable and easier to interpret. Standardization is often used in machine learning algorithms to ensure that all features are on the same scale and have the same variance.\n\n## Standardization versus IPTW\n\n### IPTW\n\nI won't bore you here, I'll keep it brief (if you want a more in-depth review of IPTW please see my other post).\n\nbriefly explain difference between iptw and standardization\n\n## Standardization in Causal Inference\n\nThe standardized mean in the uncensored (C = 0) is calculated as\n\n$$\n{\\sum_{l}E[Y|A = a, C=0, L=l]}  \\times Pr[L=l]\n$$\n\nNow, in an ideal world we'd be able to calculate this nonparametrically. We could calculate the mean outcomes in the uncensored, treated in each stratum $l$ of the confounders $L$ (i.e., $E[Y|A =1, C=0, L=l]$ for each of the strata $l$. Then we would take the weighted mean sum. To do this, we'd use the above formula where the weights are the proportion of people in each straum L. Rather than just described, how about an example?\n\n::: callout-note\n## What if L is continuous?\n\nIf $L$ is continuous in the above formula, then we need to replace $Pr[L=l]$ with the probability density function $f_{L}[l]$.\n:::\n\n### Are the number of cavities we have caused by how much candy we eat?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ncandy_cavity <- data.frame(\n  cavity = \n)\n```\n:::\n\n\n### Nonparametric Standardization\n\nUnfortunately, this isn't always possible, especially when we are dealing with real-world data. People drop out of databases or may have missing data, just to name two potential problems. Sometimes if we also have high dimensional data with multiple confounders, it's not feasible. This is where modelling can come in handy!\n\n### Mean Outcome Modelling\n\nSo what exactly are we going to do for modelling? Well, we are\n\n## Standardizing the Mean Outcome to the Confounder Distribution\n\nThere are four steps involved to standardization:\n\n1.  Expansion of dataset\n\n2.  Modelling\n\n3.  Prediction\n\n4.  Standardization by averaging\n\n## \n\n## Assumptions for Standardization\n\n1.  Structural positivity. Similar to IP weighting, positivity is also necessary for standardization because when $Pr[A = a | L = l] = 0$ and $Pr[L = l] \\neq 0$ then the conditional mean outcome $E[Y|A,L]$ is undefined [@hernanwhatif, pp.162].\n\n2.  \n\n::: callout-note\n## Positivity Assumption\n\nWhile both IP weighting and standardization require structural positivity, the implications of this assumptions not being valid can vary. For standardization, it is possible to use if this is assumption isn't met however there then needs to be a willingness to rely on parametric extrapolation (this can be done to fit a model that will smooth over the strata with structural zeroes) however this will introduce bias into the estimation. This will result in the nomial 95% confidence intervals around the estimates covering the true effect less than 95% of the time. See @hernanwhatif, pp. 162 for more details.\n:::\n\n## Standardization for Binary Outcome\n\nOur example used a continuous outcome, however standardization can also be applied to a binary outcome. Odds ratios,\n\n[@lee2022]\n\n### Causal Estimand of Treatment\n\n[@lee2022] \\# Ryan to add formulas here that are in the article\n\n## IP Weighting or Standardization?\n\nnote to self: show that IPTW and standardization give the exact same result if estimated nonparametrically?\n\nBoth IP weighting and standardization will give the exact same result. So then why would we choose one over the other? Well, they only give the same answer when no models are used to estimate them.\\\n\\\nAt this point, you may be thinking \"Well that's not really an answer\" so lets try and explain. When you use a model, you are using two different models (in our case) with two different outcomes. For IP weighting, a logistic regression model is fit to estimate $Pr[A = a, C= 0|L]$ whereas in standardization, the conditional mean $E[Y|A=a C = 0, L = l]$\n\nHowever, some degree of misspecification is a part of all models, which will introduce some bias. Due to this, the estimates from IP weighting and standardization will differ, but large differences will alert us to potentially serious model misspecification.\n\n@hernanwhatif - Section 13.4\n\n::: callout-note\n## Doubly Robust\n\nA future post will dive more into this, however it's worth mentioning about the doubly robust estimator. In brief, a doubly robust estimator gives us two chances to get the model right [@hernanwhatif, pp. 167]\\\n\\\nNote to self: Make a minor note about the bias of doubly robust estimators (pg 170)\n:::\n\nWhat does this all mean? (Hurry up old man, we don't have all day! Which one is it?) Often there is no need to choose between them.\n\n## Parametric G-Formula\n\nSo what is the parametric g-formula? Well you already used it. It's the same as standardization. You see, standardization replaces the conditional mean outcome in the g-formula by it's estimates, in this case those estimates were estimated from a parametric model and...ta da! You get the parametric g-formula.\n\n## What Next?\n\nBoth IP weighting and standardization are estimators of the g-formula (also a topic of a subsequent post). Standardization however is a *plug-in* *g-formula* estimators. Since we used a parametric model, this is called the *parametric g-formula.*\n\nBoth methods (IPW and parametric g-formula) can be used to estimate average causal effects in either the entire population or a subset.\n",
    "supporting": [
      "confounder-adjustment_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}