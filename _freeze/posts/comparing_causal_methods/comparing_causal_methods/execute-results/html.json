{
  "hash": "f298a6f60b7096af455e998b8c11b342",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Comparison is the Thief of Joy\"\nsubtitle: \"Unless it's comparing causal methods!\"\nauthor: \"Ryan Batten\"\ndate: \"2024-07-16\"\ncategories: [Simulation, Causal Methods]\n#image: \"cc_methods.jpeg\"\nbibliography: cc_methods.bib\ndraft: true\nformat: \n  html:\n    code-fold: true\n---\n\n\n# Why Compare?\n\nThere are a ton of causal methods that are available and only increasing. To name a few: matching, inverse probability of treatment weighting, regression, machine learning methods, parametric g-formula, marginal structural model and many many more. This can make it tricky to figure out which to use. By comparing these quantitatively it can provide some additional evidence for choosing between methods for certain scenarios.\n\n::: callout-note\n## Side Note\n\nThis post will cover the basics of how to compare different methods used for causal inference. I can't recommend @morris2019using enough. It was extremely helpful when I was first looking into doing this and continues to be helpful. As a result, you'll see this article reference throughout this post. A big thank you to Anthony Hatswell for the recommendation!\n:::\n\n# First, Walk the DAG\n\n::: callout-note\n## Familiar with DAGs?\n\nIf you are new to causal inference, I recommend you check out \\[insert refs\\] to learn more about directed acyclic graphs (DAGs). For the purposes of this post, you can continue reading! Just know that this DAG guides which variables we want to adjust for (and which ones we don't) to best mitigate certain types of bias.\n:::\n\nFirst things first. We need to draw a directed acyclic graph (DAG). This will be needed to determine which variables to adjust for, which ones not to and more. Alright, let's get started.\n\nFirst we need a research question! The fun part is we get to decide what we want to look at! For this we'll look at the effect of ice cream on happiness level. We'll assume that happiness is a continuous measure (since whether someone is happy or not isn't really binary). We also get to decide what kind of variables we want to include! Let's go with:\n\n-   Ice Cream (the exposure)\n\n-   Happiness (the outcome)\n\n-   Sunshine\n\n-   Friends (Time with friends)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggdag)\n\ntheme_set(theme_dag())\n\ndag <- ggdag::dagify(\n  happiness ~ ice_cream + sunshine + time_with_friends + exercise, \n  time_with_friends ~ sunshine, \n  ice_cream ~ sunshine + exercise, \n  exposure = \"ice_cream\",\n  outcome = \"happiness\",\n  labels = c(\n    happiness = \"Happiness\",\n    ice_cream = \"Ice Cream\",\n    sunshine = \"Sunshine\",\n    time_with_friends = \"Time with Friends\",\n    exercise = \"Exercise\"\n  )\n)\n\nggdag::ggdag(dag, text = FALSE, use_labels = \"label\")\n```\n\n::: {.cell-output-display}\n![](comparing_causal_methods_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nGreat now we have our DAG! Now let's see what we have to adjust for\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggdag::ggdag_adjustment_set(dag)\n```\n\n::: {.cell-output-display}\n![](comparing_causal_methods_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nGreat! So now we know that we need to adjust for sunshine. Now we need to think about what exactly we want to estimate.\n\n# Causal Estimand?\n\nIndividual treatment effects are the ultimate goal but it is very difficult to determine them (at least at the time of this post). Instead, we use an average of the effects. The question becomes the average of who? The overall sample? People who are like those who received the treatment? Choosing this is an important step because it guides who we make inferences about. Quickly, there are basically four different options:\n\n-   Average treatment effect (ATE)\n\n-   Average treatment effect in the treated (ATT)\n\n-   Average treatment effect in the untreated (ATU)\n\n-   Average treatment effect in the overlap (ATO) (aka for those patients that we don't don't know\n\nFor this let's use the ATT. In the real-world, we'd put thought into which estimand we want to estimate and not just pick one willy nilly.\n\n::: callout-important\n## Not all methods can estimate all estimands\n\nIt's very important to consider this when comparing different methods. A great example of this is comparing propensity score matching (PSM) and inverse probability of treatment weighting (IPTW). PSM can only estimate the ATT because we're matching treated patients to those that are untreated. This results in a sample that looks similar to the treated patients.\n\nNow if we were to compare this with IPTW (using the default setting in some packages), we could estimate the ATE. Comparing this result with PSM is problematic. Why? Because they're estimating different things!\\\n\\\nFor IPTW you can estimate any of the four causal estimands. This isn't true for all methods.\n:::\n\n# \"Better\" Scientifically\n\nGreat, so we're ready to compare! Right? Not quite to the data just yet (but soon!). We need to decide how exactly we're going to quantify the difference. This essentially comes down to the research question we're asking. Do we want to know if one approach results in a model with a better fit? Or do we want to know which model has lower variance? Maybe we want to know which one has a higher power. For this example, we'll use the mean-squared error and relative percent increase in precision.\n\n::: callout-note\n## Choosing Measure\n\nI highly, HIGHLY recommend the article by @morris2019using for more about choices in comparing methods, considerations. It's my personal go-to when I'm doing anything involving simulation.\n:::\n\n# Methods\n\nMethods, methods, methods! Which methods do we want to compare? Now in reality, these don't usually happen sequentially. There may be two methods that you want to compare, so you then work backwards to draw a DAG a made-up situation (or one based on a real causal hypothesis). Regardless, it's important to know what two methods you are using and how they differ in how they work.\n\nFor example, using inverse probability of treatment weighting and the parametric g-formula it would be great to compare them. Both can estimate the average treatment effect, but they do so differently. IPTW models the treatment whereas the parametric g-formula uses the outcome model. It would expected that these wouldn't give the exact same result. Although they could still be compared since they should be similar. Essentially, it's important to know how the methods may differ and what would be expected vs unexpected.\n\n# Example\n\nAlright, let's put it all together with some simulation! We're going to use the DAG from before to compare IPTW and PSM.\n\n## Simulating Data\n\nTime to simulate some data! We'll use our handy dandy DAG from before to guide us on this\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# arbitrary sample size. This can sometimes be altered as well, if we want to compare the difference in methods as a function of sample size. For example, maybe we want to see if PSM and IPTW become the same (or simliar enough) as sample size increases\n\nn = 500 # arbitrary \ntheta = 3 # this is what the actual effect is (we'll use this later)\n\ndf = data.frame(\n  sunshine = rnorm(n = n, mean = 10, sd = 2),\n  exercise = rnorm(n = n, mean = 1, sd = 0.25)\n) %>% \n  dplyr::mutate(\n    time_with_friends = 2 + 1.5*sunshine, # assume total hours per week \n    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),\n    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise\n  ) \n```\n:::\n\n\nAlright now let's test fitting a version for each method. Personally, I do this first before getting into repeating it. This is just to make sure nothing is a miss\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(WeightIt)\nlibrary(MatchIt)\nlibrary(broom)\n\nmatched <- MatchIt::matchit(ice_cream ~ sunshine, \n                 data = df, \n                 method = \"nearest\", \n                 estimand = \"ATT\")\n\nmod <- stats::glm(\n  happiness ~ ice_cream, \n  data = match.data(matched)\n)\n\nbroom::tidy(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    48.3      0.598     80.7  6.66e-237\n2 ice_cream       3.32     0.846      3.92 1.04e-  4\n```\n\n\n:::\n\n```{.r .cell-code}\nipw <- WeightIt::weightit(ice_cream ~ sunshine, \n                          data = df,\n                          method = \"glm\", \n                          estimand = \"ATT\")\n\nmod_ipw <- stats::glm(\n  happiness ~ ice_cream, \n  data = df,\n  weights = ipw$weights\n)\n```\n:::\n\n\n## Repeat, Repeat, Repeat\n\nRepeating for each of these. First calculating the results for IPTW, then for PSM\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PSM\n\nsimulate_psm <- function(n, theta) {\n  df = data.frame(\n    sunshine = rnorm(n = n, mean = 10, sd = 2),\n    exercise = rnorm(n = n, mean = 1, sd = 0.25)\n) %>% \n  dplyr::mutate(\n    time_with_friends = 2 + 1.5*sunshine, # assume total hours per week \n    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),\n    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise\n  ) \n  \n  psm <- MatchIt::matchit(ice_cream ~ sunshine + exercise, \n                 data = df, \n                 method = \"nearest\", \n                 estimand = \"ATT\")\n  \n  outcome_model <- glm(happiness ~ ice_cream, \n                       data = match.data(psm))\n  \n  estimate <- broom::tidy(outcome_model) %>% \n    dplyr::filter(term == \"ice_cream\") %>% \n    pull(estimate)\n  \n  return(estimate)\n}\n\n\n\npsm_output <- replicate(1000, simulate_psm(500, 3), simplify = FALSE) \n\npsm_output <- do.call(rbind, psm_output) %>%  # reformatting \n  as.data.frame()\n\npsm_output$theta_hat <- psm_output$V1 \n\npsm_result <- psm_output %>% \n  mutate(\n    error = theta_hat - theta,\n    squared_error = error^2\n  )\n\navg_theta = mean(psm_result$theta_hat)\nmse = (1/1000)*sum(psm_result$squared_error)\n\npsm_se_result = psm_result %>% \n  mutate(\n    se_mse = mse,\n    squared_diff = (theta_hat - avg_theta)^2\n  )\n\npsm_emp_se <- sqrt((1/(1000-1))*sum(psm_se_result$squared_diff))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_ipw <- function(n, theta) {\n  df = data.frame(\n    sunshine = rnorm(n = n, mean = 10, sd = 2),\n    exercise = rnorm(n = n, mean = 1, sd = 0.25)\n) %>% \n  dplyr::mutate(\n    time_with_friends = 2 + 1.5*sunshine, # assume total hours per week \n    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),\n    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise\n  ) \n  \n  ipw <- WeightIt::weightit(ice_cream ~ sunshine + exercise, \n                 data = df, \n                 estimand = \"ATT\",\n                 method = \"glm\")\n  \n  outcome_model <- glm(happiness ~ ice_cream, \n                       data = df, \n                       weights = ipw$weights)\n  \n  estimate <- broom::tidy(outcome_model) %>% \n    dplyr::filter(term == \"ice_cream\") %>% \n    pull(estimate)\n  \n  return(estimate)\n}\n\n\n\nipw_output <- replicate(1000, simulate_ipw(500, 3), simplify = FALSE) \n\nipw_output <- do.call(rbind, ipw_output) %>%  # reformatting \n  as.data.frame()\n\nipw_output$theta_hat <- ipw_output$V1 \n\nipw_result <- ipw_output %>% \n  mutate(\n    error = theta_hat - theta,\n    squared_error = error^2\n  )\n\navg_theta = mean(ipw_result$theta_hat)\nmse = (1/1000)*sum(ipw_result$squared_error)\n\nipw_se_result = ipw_result %>% \n  mutate(\n    se_mse = mse,\n    squared_diff = (theta_hat - avg_theta)^2\n  )\n\nipw_emp_se <- sqrt((1/(1000-1))*sum(ipw_se_result$squared_diff))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nincrease_precision <- round(100*((ipw_emp_se/psm_emp_se)^2 - 1 ),2) # Increase in precision for IPTW vs PSM\n\ncorr = cor(ipw_result$theta_hat, psm_result$theta_hat)\n\nratio <- ipw_emp_se/psm_emp_se\n\nse_increase_precision <- round(200*(ratio^2)*sqrt((1-(corr^2))/(1000-1)), 2)\n```\n:::\n\n\nNow we have some results! The MSE for PSM is 0.34 and 0.1for IPTW. The increase in precision is -90.24 (0.62). We need to unpack these results since there are a few things to keep in mind.\n\nFirstly, we didn't assess the balance. When comparing two methods, we should also compare how the balance between the methods looks. Does one method result in better balance or no? This can be done graphically and/or using love plots (recommend the cobalt package for this). Secondly, the same size was 500. Perhaps, more importantly, is the number of patients getting matched. Do we have enough controls for our treatment? What if we didn't, what if we did? Thirdly, there were only two confounders. How would the results change if we included more?\n\nWhen comparing results it's important to not just look numerically. There are scientific limitations of certain methods. For example, PSM only keeps patients with a match. Would we be sure that these patients match our inclusion/exclusion criteria? Or is selection bias being induced? We need to incorporate these pieces when comparing methods.\n\nThis isn't to say that the above is necessarily wrong. It's just important to consider the limitations of what we did do. As put in @morris2019using, starting simple is the best and I agree. We can now build on this!\n\n::: callout-caution\n## Metrics That Rely on Standard Error\n\nWe should use caution when using using metrics relying on standard error. For PS-based methods, we need to use appropriate variance estimators. This can make it tricky when using certain metrics. Imagine we are comparing IPTW and PSM based on power for nominal coverage. Both of these need SE. If we're using robust variance estimation (HC0) for PSM and M-estimation for IPTW, are these really comparing IPTW vs PSM? Or would the different variance estimation techniques have something to do with the differences? It's important to consider these differences.\n:::\n\n# Why Bother?\n\nAt this point, you may be thinking \"why both to do this? This is a lot of work when I can just say one over the other\". The answer is two-fold. Firstly, it's not always clear what the best approach is. It can depend on the setting and outcome (continuous, binary, time-to-event). It's helpful to quantify this when discussing with a team. Secondly, when we compare methods it's helpful to know \"how much\". One method may have higher precision but how much is \"higher\". Using different metrics also helps to figure out what should be more emphasized (i.e., precision vs bias, etc).\n\nFor me, learning how to simulate data and do these types of comparisons has helped tremendously. Not only in making decisions, but also in understanding methods. Hope you found this post helpful! Happy causal method comparing!\n",
    "supporting": [
      "comparing_causal_methods_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}