{
  "hash": "f5ff0f9bed2ee7c1d8a133457ae48b16",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Comparison: The Thief of Joy\"\nsubtitle: \"Unless it's Comparing Causal Methods!\"\nauthor: \"Ryan Batten\"\ndate: \"2024-07-16\"\ncategories: [Simulation, Causal Methods]\nimage: \"monsters_icecream.png\"\nbibliography: cc_methods.bib\nformat: \n  html:\n    code-fold: true\n---\n\n\n# Why Compare?\n\nThere are a ton of causal methods that are available and it's only increasing. To name a few: matching, inverse probability of treatment weighting, regression, machine learning methods, parametric g-formula, marginal structural models and many many more. This can make it tricky to figure out which to use. By comparing these quantitatively it can provide some additional evidence for choosing between methods for certain scenarios.\n\n::: callout-note\n## Side Note\n\nThis post will cover the basics of how to compare different methods used for causal inference. I can't recommend @morris2019using enough. It was extremely helpful when I was first learning how to do this and I continue to use it. As a result, you'll see this article reference throughout this post. A big thank you to Anthony Hatswell for the recommendation!\n:::\n\n# First, Walk the DAG\n\n::: callout-note\n## Familiar with DAGs?\n\nIf you are new to causal inference, I recommend you check out [@rohrer2018thinking] or the ggdag R package to learn more about directed acyclic graphs (DAGs). For the purposes of this post, you can continue reading! Just know that this DAG guides which variables we want to adjust for (and which ones we don't) to best mitigate certain types of bias.\n:::\n\nFirst things first. We need to draw a directed acyclic graph (DAG). This will be needed to determine which variables to adjust for, which ones not to and more. Alright, let's get started!\n\nWait, we need a research question! The fun part is we get to decide what we want to look at! For this post, we'll look at the effect of ice cream on happiness level. We'll assume that happiness is a continuous measure (since whether someone is happy or not isn't really binary). We also get to decide what kind of variables we want to include! Let's go with:\n\n-   Ice Cream (the exposure)\n\n-   Happiness (the outcome)\n\n-   Sunshine (hours per day)\n\n-   Time with Friends (hours per week)\n\n-   Exercise (hours per day)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # ol faithful \nlibrary(ggdag) # used for the DAG \n\ntheme_set(theme_dag()) # setting the theme of the DAG \n\n# Actually creating the DAG. Adding labels so we know the name of each variable. \n\ndag <- ggdag::dagify(\n  happiness ~ ice_cream + sunshine + time_with_friends + exercise, \n  time_with_friends ~ sunshine, \n  ice_cream ~ sunshine + exercise, \n  exposure = \"ice_cream\",\n  outcome = \"happiness\",\n  labels = c(\n    happiness = \"Happiness\",\n    ice_cream = \"Ice Cream\",\n    sunshine = \"Sunshine\",\n    time_with_friends = \"Time with Friends\",\n    exercise = \"Exercise\"\n  )\n)\n\nggdag::ggdag(dag, text = FALSE, use_labels = \"label\") # actually showing the DAG\n```\n\n::: {.cell-output-display}\n![](comparing_causal_methods_files/figure-html/dag-1.png){width=672}\n:::\n:::\n\n\nGreat now we have our DAG! Now let's see what we have to adjust for. Luckily we can use the handy dandy ggdag package! Or if you don't use R, there's an online tool dagitty. Either way, let's have a look.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggdag::ggdag_adjustment_set(dag, text = FALSE, use_labels = \"label\")\n```\n\n::: {.cell-output-display}\n![](comparing_causal_methods_files/figure-html/variables_adjust-1.png){width=672}\n:::\n:::\n\n\nPerfect! Looks like we have to adjust for sunshine and exercise. The next step is to think about what exactly we want to estimate.\n\n# Why Causal Estimand...Why?\n\nIndividual treatment effects are the ultimate goal but it is very difficult to determine them (at least at the time of this post). Instead, we use an average of the effects. The question becomes the average of who? The overall sample? People who are like those who received the treatment? Choosing this is an important step because it guides who we make inferences about. Quickly, there are basically four different options:\n\n-   Average treatment effect (ATE)\n\n-   Average treatment effect in the treated (ATT)\n\n-   Average treatment effect in the untreated (ATU)\n\n-   Average treatment effect in the overlap (ATO) (aka for those patients that we don't don't know\n\nFor this let's use the ATT. In the real-world, we'd put thought into which estimand we want to estimate and not just pick one willy nilly. A great resource for this is @greifer2021choosing.\n\n::: callout-important\n## Not all estimands are applicable for all methods\n\nIt's very important to consider this when comparing different methods. A great example of this is comparing propensity score matching (PSM) and inverse probability of treatment weighting (IPTW). PSM can only estimate the ATT because we're matching treated patients to those that are untreated. This results in a sample that looks similar to the treated patients.\n\nIf we were to compare this with IPTW (using the default setting in some packages), we could accidentally end up estimating the ATE. Comparing this result with PSM is problematic. Why? Because they're estimating different things!\n\nNot all is lost, with IPTW you can estimate any of the four causal estimands. This isn't true for all methods, so we need to make sure it's the same causal estimand.\n:::\n\n# \"Better\" Scientifically\n\nGreat, so we're ready to compare! Right?! GIMME SOME DATA! Not quite yet but almost there! We need to decide how exactly we're going to quantify the difference. This essentially comes down to the research question we're asking. Do we want to know if one approach results in a model with a better fit? Or do we want to know which model has lower variance? Maybe we want to know which one has a higher power. For this example, we'll use the mean squared error (MSE) and relative percent increase in precision.\n\n::: callout-note\n## Choosing Measure\n\nI highly, HIGHLY recommend the article by @morris2019using for more about choices in comparing methods. It's my personal go-to when I'm doing anything involving simulation.\n:::\n\n# Methods, Methods, Methods!\n\nMethods, methods, methods! Unlike Beetlejuice, saying it three times won't make a method appear. So we need to pick the two methods that we want to compare. Now in reality, these don't usually happen sequentially. There may be two methods that you want to compare, so you then work backwards to draw a DAG a made-up situation (or one based on a real causal hypothesis). Regardless, it's important to know what two methods you are using and how they differ in how they work.\n\nFor example, it could be helpful to compare inverse probability of treatment weighting and the parametric g-formula. Both can estimate the average treatment effect, but they do so differently. IPTW models the treatment whereas the parametric g-formula models the outcome. It would be expected that these wouldn't give the exact same result. Although they could still be compared since they should be similar. Essentially, it's important to know how the methods may differ and what would be expected versus unexpected.\n\n# Finally an Example! With Data!\n\nAlright, let's put it all together with some simulation! We're going to use the DAG from before to compare IPTW and PSM.\n\n## Simulating Data\n\nTime to simulate some data! We'll use our handy dandy DAG from before to guide us on this. Personally, before continuing with running it multiple times, I like to do a test run. First I simulate some data to play around with the proportions. For example, what kind of coefficients do we need to get the desired ratio (roughly) of treated versus controls, etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(456) # setting seed for reproducibility\n\n# Picking an arbitrary sample size. This can sometimes be altered as well, if we want to compare the difference in methods as a function of sample size. For example, maybe we want to see if PSM and IPTW become the same (or simliar enough) as sample size increases\n\nn = 500 # arbitrary \ntheta = 3 # this is what the actual effect is (we'll use this later for some of the metrics)\n\ndf = data.frame(\n  sunshine = rnorm(n = n, mean = 10, sd = 2),\n  exercise = rnorm(n = n, mean = 1, sd = 0.25)\n) %>% \n  dplyr::mutate(\n    time_with_friends = 2 + 1.5*sunshine, \n    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),\n    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise\n  ) \n```\n:::\n\n\nAlright now let's test fitting a version for each method. Personally, I do this first before getting into repeating it. This is just to make sure nothing is a miss with the models and fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note: these results are muted here, but suggest using broom::tidy to check the outputs. \n\n# Packages\n\nlibrary(WeightIt) # we'll use this for IPTW\nlibrary(MatchIt) # we'll use this for PSM \n\n# Testing Matching the data \n\nmatched <- MatchIt::matchit(ice_cream ~ sunshine + exercise, \n                 data = df, \n                 method = \"nearest\", \n                 estimand = \"ATT\")\n\n# Outcome Model \n\nmod <- stats::glm(\n  happiness ~ ice_cream, \n  data = match.data(matched) # using the matched data \n)\n\n# IPTW Model \n\nipw <- WeightIt::weightit(ice_cream ~ sunshine + exercise, \n                          data = df,\n                          method = \"glm\", \n                          estimand = \"ATT\") # by default the estimand will be ATE\n\n# Outcome model with iptw\n\nmod_ipw <- stats::glm(\n  happiness ~ ice_cream, \n  data = df,\n  weights = ipw$weights # weights from PSM \n)\n```\n:::\n\n\n## Repeat, Repeat, Repeat...Then Repeat Again!\n\nNow that we have a sample, we can make these into functions and simulate. For the purposes of this post we'll just select 1000 for the number of times we'll repeat the process. In practice, choosing the number of times that we actually want to do this is more nuanced. Any guesses what reference I'll suggest? Yup! @morris2019using\n\nNow let's repeat for each of these. First, for PSM then for IPTW.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom) # we'll need this one for cleaning the model output \n\n# PSM\n\n# Creating a function that simulates data, then matches the treated (aka those that had ice cream) to the controls (those that did not). \n\nsimulate_psm <- function(n, \n                         theta) { # theta is the \"true\" effect\n  df = data.frame(\n    sunshine = rnorm(n = n, mean = 10, sd = 2),\n    exercise = rnorm(n = n, mean = 1, sd = 0.25)\n) %>% \n  dplyr::mutate(\n    time_with_friends = 2 + 1.5*sunshine, # assume total hours per week \n    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),\n    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise\n  ) \n  \n  # Matching data\n  \n  psm <- MatchIt::matchit(ice_cream ~ sunshine + exercise, \n                 data = df, \n                 method = \"nearest\", \n                 estimand = \"ATT\")\n  \n  # Fitting an outcome model \n  \n  outcome_model <- glm(happiness ~ ice_cream, \n                       data = match.data(psm))\n  \n  # Extracting only the term that we care about \n  \n  estimate <- broom::tidy(outcome_model) %>% \n    dplyr::filter(term == \"ice_cream\") %>% \n    pull(estimate)\n  \n  return(estimate)\n}\n\n# Repeat!\n\n# Repeating 1000 times. Arbitrarly choosing a sample size of 500 and \"true\" effect of 3\n\npsm_output <- replicate(1000, simulate_psm(500, 3), simplify = FALSE) \n\n# Reformatting \n\npsm_output <- do.call(rbind, psm_output) %>%  # reformatting \n  as.data.frame()\n\npsm_output$theta_hat <- psm_output$V1 # renaming the variable\n\n# Calculating some things that will be needed later. Namely, the error and squared error. The formula for these (and the metrics used later on) are in Morris et al. (2019)\n\npsm_result <- psm_output %>% \n  mutate(\n    error = theta_hat - theta,\n    squared_error = error^2\n  )\n\navg_theta = mean(psm_result$theta_hat) # calculating the average of the estimated effect\npsm_mse = (1/1000)*sum(psm_result$squared_error) # calculating the mean squared error\n\n# Calculating the empirical standard error\n\npsm_se_result = psm_result %>% \n  mutate(\n    se_mse_num = (squared_error - psm_mse)^2,\n    squared_diff = (theta_hat - avg_theta)^2\n  )\n\npsm_mse_se <- sqrt(sum(psm_se_result$se_mse_num) / (1000*(1000-1)))\n\npsm_emp_se <- sqrt((1/(1000-1))*sum(psm_se_result$squared_diff))\n```\n:::\n\n\nGreat! PSM is all done. Now let's do the same thing for IPTW.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Doing the same as above for PSM. Rather to the above code chunk for comments on the process\n\nsimulate_ipw <- function(n, theta) {\n  df = data.frame(\n    sunshine = rnorm(n = n, mean = 10, sd = 2),\n    exercise = rnorm(n = n, mean = 1, sd = 0.25)\n) %>% \n  dplyr::mutate(\n    time_with_friends = 2 + 1.5*sunshine, \n    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),\n    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise\n  ) \n  \n  ipw <- WeightIt::weightit(ice_cream ~ sunshine + exercise, \n                 data = df, \n                 estimand = \"ATT\",\n                 method = \"glm\")\n  \n  outcome_model <- glm(happiness ~ ice_cream, \n                       data = df, \n                       weights = ipw$weights)\n  \n  estimate <- broom::tidy(outcome_model) %>% \n    dplyr::filter(term == \"ice_cream\") %>% \n    pull(estimate)\n  \n  return(estimate)\n}\n\n\n\nipw_output <- replicate(1000, simulate_ipw(500, 3), simplify = FALSE) \n\nipw_output <- do.call(rbind, ipw_output) %>%  # reformatting \n  as.data.frame()\n\nipw_output$theta_hat <- ipw_output$V1 \n\nipw_result <- ipw_output %>% \n  mutate(\n    error = theta_hat - theta,\n    squared_error = error^2\n  )\n\navg_theta = mean(ipw_result$theta_hat)\nipw_mse = (1/1000)*sum(ipw_result$squared_error)\n\nipw_se_result = ipw_result %>% \n  mutate(\n    se_mse_num = (squared_error - ipw_mse)^2,\n    squared_diff = (theta_hat - avg_theta)^2\n  )\n\nipw_mse_se <- sqrt(sum(ipw_se_result$se_mse_num) / (1000*(1000-1)))\n\nipw_emp_se <- sqrt((1/(1000-1))*sum(ipw_se_result$squared_diff))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculating these metrics using Morris et al. (2019). \n\nincrease_precision <- round(100*((ipw_emp_se/psm_emp_se)^2 - 1 ),2) # Increase in precision for IPTW vs PSM\n\ncorr = cor(ipw_result$theta_hat, psm_result$theta_hat) # correlation between the two \n\nratio <- ipw_emp_se/psm_emp_se\n\nse_increase_precision <- round(200*(ratio^2)*sqrt((1-(corr^2))/(1000-1)), 2)\n```\n:::\n\n\nNow we have some results! The MSE for PSM is 0.14 (0.01) and 0.01 (0.00075) for IPTW. The increase in precision is -88.52 (0.73). We need to unpack these results since there are a few things to keep in mind.\n\nFirstly, we didn't assess the balance. The whole point of using these methods is to ensure exchangeability. When comparing two methods, we should also compare how the balance between the methods looks. Does one method result in better balance or no? This can be done graphically and/or using love plots (recommend the cobalt package for this). Secondly, the same size was 500. Perhaps, more importantly, is the number of patients getting matched. Do we have enough controls for our treatment? What if we didn't? What if we did? Thirdly, there were only two confounders. How would the results change if we included more? A last point (to not bore you to death): we included the two variables we know were confounders. What would happen if we left one out? (aka if the model was misspecified?).\n\nWhen comparing results it's important to not just look numerically. There are other limitations of that may not be as easy to quantify. For example, PSM only keeps patients with a match. Would we be sure that these patients match our inclusion/exclusion criteria? Or is it a subset of these criteria? Or is selection bias being induced? We need to incorporate these pieces when comparing methods.\n\nThis isn't to say that the above is necessarily wrong. It's just important to consider the limitations of what we did do. As put in @morris2019using, starting simple is the best and I agree. We can now build on this!\n\n::: callout-caution\n## Metrics That Rely on Standard Error\n\nWe should use caution when using using metrics relying on standard error. For PS-based methods, we need to use appropriate variance estimators. This can make it tricky when using certain metrics. Imagine we are comparing IPTW and PSM based on power for nominal coverage. Both of these need SE. If we're using robust variance estimation (HC0) for PSM and M-estimation for IPTW, are these really comparing IPTW vs PSM? Or would the different variance estimation techniques have something to do with the differences? It's important to consider this.\n:::\n\n# Why Bother?\n\nAt this point, you may be thinking \"why both doing this? This is a lot of work when I can just say one over the other\". The answer is two-fold. Firstly, it's not always clear what the best approach is. It can depend on the setting and outcome (continuous, binary, time-to-event). It's helpful to quantify this when discussing with a team. Secondly, when we compare methods it's helpful to know \"how much\". One method may have higher precision but how much is \"higher\". Using different metrics also helps to figure out what should be more emphasized (i.e., precision vs bias, etc).\n\nUsing this approach, based on real world data, can partially help making a decision. I've learnt a lot from using this approach and continue to use it when comparing new methods. Recently, I've been exploring usingTMLE, BART and energy balancing. Hope you find this approach as useful as I do!\n",
    "supporting": [
      "comparing_causal_methods_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}