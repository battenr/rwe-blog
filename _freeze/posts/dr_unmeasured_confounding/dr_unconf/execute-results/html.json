{
  "hash": "43479f6b9e6d8a5b6e17517a1155b810",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Double Trouble\" # October Post\nsubtitle: \"Doubly Robust Methods For Unmeasured Confounding\"\nauthor: \"Ryan Batten\"\ndate: \"2025-09-16\"\nimage: dr.png\ndraft: true \ncategories: [Resampling, Bootstrapping, Variance Estimation]\nbibliography: dr_unconf.bib\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Power of Two\n\nDoubly robust methods can be very useful for causal inference. Why?\n\nThey combine two models: one for the outcome, and one for the treatment. This creates a model that is *doubly* **robust**. The question is....robust to what?\n\n## Model Misspecification\n\nEvery model ever made is misspecified. Why? Because it's not exact, it's an approximation of the real world. This is problematic because it can lead to bias.How much bias depends on how much the model is misspecified. If close enough, there is minimal bias. If grossly misspecified, a lot more bias.\n\nDoubly robust methods can be quite helpful with this. If either of the two models is correctly specified (or close), then our bias will be less than either model alone.\n\nThis begs the question....what else can we use doubly robust methods for?\n\n## Unmeasured Confounding\n\nThe villain to every observational study ever conducted. The ever lurking....unmeasured confounding.\n\nThere are several ways we can mitigate this villain's powers. The most obvious tool is randomization, however in observational research we can't do that. We can use sensitivity analyses to see how it would impact us.\n\nBut....what about doubly robust methods? Would these be powerful enough to pushback against our villain?\n\nLet's find out!\n\n## Enter the Simulation\n\nTo see whether this hair-brained scheme could work, we'll simulate some data. Let's lay out some ground rules for our simulation:\n\n-   Going to run it 1000 times (chosen due to run time/don't want our laptops to implode)\n\n-   Sample size of 250 (arbitrary)\n\n-   Two measured confounders (one continuous, one binary)\n\n-   Two unmeasured confounders (both continuous)\n\n-   Binary treatment\n\n-   Continuous outcome\n\n-   Target estimand will be the average treatment effect (ATE)\n\nMethods we'll explore:\n\n-   Individual probability weighting only\n\n-   Generalized linear model (outcome model only)\n\n-   Doubly robust: using both IP weighting, and those covariates in the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Title: This code is the setup that we will use later. It include the libraries and \n# any functions that we may need \n\n\nlibrary(tidyverse) # ol' faithful\nlibrary(WeightIt) # for IP weighting\nlibrary(kableExtra) # for formatting table output\n\n#... Functions ----\n\nsim_data <- function(n = 250, # sample size \n                     beta_trt = 1.5, # treatment effect\n                     # Parameters for Z1 \n                     z1_mean = 5, z1_sd = 1, \n                     # Parameters for Z2\n                     z2_size = 1, z2_prob = 0.5, \n                     # Parameters for U1\n                     u1_mean = 10, u1_sd = 3,\n                     u2_mean = 2, u2_sd = 0.5,\n                     # Confounder - Effect on X\n                     z1_on_x = 0.05, z2_on_x = 0.5, \n                     u1_on_x = 0.08, u2_on_x = 0.8, \n                     # Confounder - Effect on Y\n                     z1_on_y = 0.5, z2_on_y = 0.7,\n                     u1_on_y = 0.4, u2_on_y = 2\n){\n  \n  # Creating the Dataframe\n  \n  df <- data.frame(\n    z1 = rnorm(n = n, mean = z1_mean, sd = z1_sd), \n    z2 = rbinom(n = n, size = z2_size, prob = z2_prob),\n    u1 = rnorm(n = n, mean = u1_mean, sd = u1_sd),\n    u2 = rnorm(n = n, mean = u2_mean, sd = u2_sd)\n  ) %>% \n    dplyr::mutate(\n      beta_trt = 1.5, \n      prob = plogis(u1_on_x*u1 - u2_on_x*u2 + z1_on_x*z1 + z2_on_x*z2), \n      x = rbinom(n = n, size = 1, prob = prob), \n      y = beta_trt*x + z1_on_y*z1 + z2_on_y*z2 + u1_on_y*u1 + u2_on_y*u2 + rnorm(n = n, mean = 0, sd = 1)\n      #y = beta_trt*x + z1_on_y*z1 + z2_on_y*z2 + rnorm(n = n, mean = 0, sd = 1)\n    )\n  \n  # Return the dataframe\n  \n  return(df)\n}\n```\n:::\n\n\n## Perfectly Imperfect \n\nFor our first scenario, let's assume that we perfectly specify both models. To be exact, I mean we include the measured confounders that we do have (z1 & z2). How does it look for each method?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# IPW ----\n\nipw <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1 + z2, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, ipw(sample.size = 250), simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `tidy()` method for objects of class `glm_weightit` is not maintained by the broom team, and is only supported through the `glm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n```\n\n\n:::\n\n```{.r .cell-code}\ndf.out.ipw <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\nipw.bias <- mean(df.out.ipw$bias_for_one) # bias  \nipw.se.of.bias <- sqrt(sum(df.out.ipw$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Outcome Model ----\n\nout_mod <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting outcome model \n  \n  mod <- glm(y ~ x + z1 + z2, # note: not doubly robust for this example\n             family = gaussian(link = \"identity\"), \n             data = df)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, out_mod(sample.size = 250), simplify = FALSE)\n\ndf.out.outmod <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\noutmod.bias <- mean(df.out.outmod$bias_for_one) # bias  \noutmod.se.of.bias <- sqrt(sum(df.out.outmod$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Doubly Robust ----\n\ndr <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1 + z2, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x + z1 + z2, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, dr(sample.size = 250), simplify = FALSE)\n\ndf.out.dr <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\ndr.bias <- mean(df.out.dr$bias_for_one) # bias  \ndr.se.of.bias <- sqrt(sum(df.out.dr$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n\n# Results ----\n\nresults <- data.frame(\n  `Method` = c(\"IPW\", \"Outcome Model\", \"Doubly Robust\"), \n  `Bias` = c(ipw.bias, outmod.bias, dr.bias), \n  se_of_bias = c(ipw.se.of.bias, outmod.se.of.bias, dr.se.of.bias)\n) %>% \n  rename(`SE of Bias` = se_of_bias)\n\nknitr::kable(results) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; \">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Method </th>\n   <th style=\"text-align:right;\"> Bias </th>\n   <th style=\"text-align:right;\"> SE of Bias </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> IPW </td>\n   <td style=\"text-align:right;\"> -0.1091725 </td>\n   <td style=\"text-align:right;\"> 0.0070240 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Outcome Model </td>\n   <td style=\"text-align:right;\"> -0.1031545 </td>\n   <td style=\"text-align:right;\"> 0.0074152 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Doubly Robust </td>\n   <td style=\"text-align:right;\"> -0.1025139 </td>\n   <td style=\"text-align:right;\"> 0.0076859 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAs you can see, if we correctly specify the model (which we know is true since we simulated the data), then there is no real harm in this. The question becomes, what happens if we misspecify both models?\n\n## Imperfect Model in an Imperfect World\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# IPW ----\n\nipw <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, ipw(sample.size = 250), simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `tidy()` method for objects of class `glm_weightit` is not maintained by the broom team, and is only supported through the `glm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n```\n\n\n:::\n\n```{.r .cell-code}\ndf.out.ipw <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\nipw.bias <- mean(df.out.ipw$bias_for_one) # bias  \nipw.se.of.bias <- sqrt(sum(df.out.ipw$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Outcome Model ----\n\nout_mod <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting outcome model \n  \n  mod <- glm(y ~ x + z2, # note: not doubly robust for this example\n             family = gaussian(link = \"identity\"), \n             data = df)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, out_mod(sample.size = 250), simplify = FALSE)\n\ndf.out.outmod <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\noutmod.bias <- mean(df.out.outmod$bias_for_one) # bias  \noutmod.se.of.bias <- sqrt(sum(df.out.outmod$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Doubly Robust ----\n\ndr <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x + z2, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, dr(sample.size = 250), simplify = FALSE)\n\ndf.out.dr <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\ndr.bias <- mean(df.out.dr$bias_for_one) # bias  \ndr.se.of.bias <- sqrt(sum(df.out.dr$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n\n# Results ----\n\nresults <- data.frame(\n  method = c(\"IPW\", \"Outcome Model\", \"Doubly Robust\"), \n  bias = c(ipw.bias, outmod.bias, dr.bias), \n  se_of_bias = c(ipw.se.of.bias, outmod.se.of.bias, dr.se.of.bias)\n)\n\nknitr::kable(results) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; \">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> method </th>\n   <th style=\"text-align:right;\"> bias </th>\n   <th style=\"text-align:right;\"> se_of_bias </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> IPW </td>\n   <td style=\"text-align:right;\"> -0.0257925 </td>\n   <td style=\"text-align:right;\"> 0.0079486 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Outcome Model </td>\n   <td style=\"text-align:right;\"> -0.0871226 </td>\n   <td style=\"text-align:right;\"> 0.0081363 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Doubly Robust </td>\n   <td style=\"text-align:right;\"> -0.1174972 </td>\n   <td style=\"text-align:right;\"> 0.0076668 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Measured Confounding \n\nAt this point you may be thinking, but \"duh' both models are misspecified. Of course this won't work. Well, let's take a look at if there is only measured confounding\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#... Functions ----\n\nsim_data <- function(n = 250, # sample size \n                     beta_trt = 1.5, # treatment effect\n                     # Parameters for Z1 \n                     z1_mean = 5, z1_sd = 1, \n                     # Parameters for Z2\n                     z2_size = 1, z2_prob = 0.5, \n                     # Parameters for U1\n                     u1_mean = 10, u1_sd = 3,\n                     u2_mean = 2, u2_sd = 0.5,\n                     # Confounder - Effect on X\n                     z1_on_x = 0.05, z2_on_x = 0.5, \n                     u1_on_x = 0.08, u2_on_x = 0.8, \n                     # Confounder - Effect on Y\n                     z1_on_y = 0.5, z2_on_y = 0.7,\n                     u1_on_y = 0.4, u2_on_y = 2\n){\n  \n  # Creating the Dataframe\n  \n  df <- data.frame(\n    z1 = rnorm(n = n, mean = z1_mean, sd = z1_sd), \n    z2 = rbinom(n = n, size = z2_size, prob = z2_prob)\n    #u1 = rnorm(n = n, mean = u1_mean, sd = u1_sd),\n    #u2 = rnorm(n = n, mean = u2_mean, sd = u2_sd)\n  ) %>% \n    dplyr::mutate(\n      beta_trt = 1.5, \n      prob = plogis(z1_on_x*z1 + z2_on_x*z2), \n      x = rbinom(n = n, size = 1, prob = prob), \n      #y = beta_trt*x + z1_on_y*z1 + z2_on_y*z2 + u1_on_y*u1 + u2_on_y*u2 + rnorm(n = n, mean = 0, sd = 1)\n      y = beta_trt*x + z1_on_y*z1 + z2_on_y*z2 + rnorm(n = n, mean = 0, sd = 1)\n    )\n  \n  # Return the dataframe\n  \n  return(df)\n}\n\n\n\n\n# IPW ----\n\nipw <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, ipw(sample.size = 250), simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `tidy()` method for objects of class `glm_weightit` is not maintained by the broom team, and is only supported through the `glm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n```\n\n\n:::\n\n```{.r .cell-code}\ndf.out.ipw <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\nipw.bias <- mean(df.out.ipw$bias_for_one) # bias  \nipw.se.of.bias <- sqrt(sum(df.out.ipw$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Outcome Model ----\n\nout_mod <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting outcome model \n  \n  mod <- glm(y ~ x + z2, # note: not doubly robust for this example\n             family = gaussian(link = \"identity\"), \n             data = df)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, out_mod(sample.size = 250), simplify = FALSE)\n\ndf.out.outmod <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\noutmod.bias <- mean(df.out.outmod$bias_for_one) # bias  \noutmod.se.of.bias <- sqrt(sum(df.out.outmod$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Doubly Robust ----\n\ndr <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x + z2, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, dr(sample.size = 250), simplify = FALSE)\n\ndf.out.dr <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\ndr.bias <- mean(df.out.dr$bias_for_one) # bias  \ndr.se.of.bias <- sqrt(sum(df.out.dr$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n\n# Results ----\n\nresults <- data.frame(\n  method = c(\"IPW\", \"Outcome Model\", \"Doubly Robust\"), \n  bias = c(ipw.bias, outmod.bias, dr.bias), \n  se_of_bias = c(ipw.se.of.bias, outmod.se.of.bias, dr.se.of.bias)\n)\n\nknitr::kable(results) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; \">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> method </th>\n   <th style=\"text-align:right;\"> bias </th>\n   <th style=\"text-align:right;\"> se_of_bias </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> IPW </td>\n   <td style=\"text-align:right;\"> 0.0872926 </td>\n   <td style=\"text-align:right;\"> 0.0045014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Outcome Model </td>\n   <td style=\"text-align:right;\"> 0.0272389 </td>\n   <td style=\"text-align:right;\"> 0.0046918 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Doubly Robust </td>\n   <td style=\"text-align:right;\"> -0.0050662 </td>\n   <td style=\"text-align:right;\"> 0.0043592 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nSo this begs the question.....what's going on?\n\n## Bias....Bias....MORE BIAS! \n\nAs you can see, now we have several situations that we can draw from. So let's dive in.\n\nFirst, the bias of a doubly robust method is the product of the bias from each model [@funk2011doubly]:\n\n$$\nBias_{Doubly\\ Robust\\ Method} = Bias_{Treatment \\ Model}*Bias_{Outcome \\ Model}\n$$\n\nThis means that if *either* of the models are correctly specified, then we're good to go! However, if both models are misspecified....we've got issues. So what does this have to do with confounding? I'm glad you asked.\n\n## Dissecting Our Results\n\nConfounding causes bias. It doesn't matter if it's measured or unmeasured. If we're trying to fit a model and there is unmeasured confounding, we could have some problems.\n\n## What's Next? \n\nThis was an introductory thought. I'd be curious to hear your opinion as well!\n\nTo me, a takeaway would be to consider if there is unmeasured confounding in your study, and how much there is. Additionally, how well you think you can specify either model.\n\nThis example shows that unmeasured confounding can complicate things. Doubly robust methods reduce bias due to model misspecification, however not for unmeasured confounding. If you think there's a lot of unmeasured confounding, it may be beneficial to consider other methods.\n\n## \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}