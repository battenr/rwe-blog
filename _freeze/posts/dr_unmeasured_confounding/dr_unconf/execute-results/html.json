{
  "hash": "fb924777ac9f89c69468fda12f283466",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Double Trouble\" # October Post\nsubtitle: \"Doubly Robust Methods For Unmeasured Confounding\"\nauthor: \"Ryan Batten\"\ndate: \"2025-09-25\"\nimage: dr_unmeasured.png\ncategories: [Doubly Robust Methods, Confounding]\nbibliography: dr_unconf.bib\nformat: \n  html:\n    code-fold: true\n---\n\n\n## Power of Two\n\nDoubly robust methods can be very useful for causal inference. Why? They combine two models: one for the outcome, and one for the treatment. This creates a model that is *doubly* **robust**. The question is....robust to what?\n\n## Model Misspecification\n\nEvery model ever made is misspecified. Why? Because it's not exact, it's an approximation of the real world. This can lead to bias. How much bias depends on how much the model is misspecified. If close enough, there is minimal bias. If grossly misspecified, a lot more bias.\n\nDoubly robust methods can be quite helpful with this. If either of the two models is correctly specified (or close), then our bias will be less than either model alone.\n\nThis begs the question....what else can we use doubly robust methods for?\n\n## Unmeasured Confounding\n\nThe villain to every observational study ever conducted. The ever lurking....unmeasured confounding.\n\nThere are several ways we can mitigate this villain's powers. The most obvious tool is randomization, however in observational research we can't do that. One option is to use sensitivity analyses to better understand how it would impact us.\n\nHowever....what about doubly robust methods? Would these be powerful enough to mitigate our villian's powers?\n\nLet's find out!\n\n## Enter the Simulation\n\nTo see whether this hair-brained scheme could work, we'll simulate some data. Let's lay out some ground rules for our simulation:\n\n-   Going to run it 1000 times (chosen due to run time/don't want our laptops to implode)\n\n-   Sample size of 250 (arbitrary)\n\n-   Two measured confounders (one continuous, one binary)\n\n-   Two unmeasured confounders (both continuous)\n\n-   Binary treatment\n\n-   Continuous outcome\n\n-   Target estimand will be the average treatment effect (ATE)\n\nMethods we'll explore:\n\n-   Individual probability weighting only\n\n-   Generalized linear model (outcome model only)\n\n-   Doubly robust: using the same covariates in both the propensity score model and the outcome model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Title: Setup \n\n# Description: This code is the setup for what we will be using later. It includes the libraries and any functions that we may need. \n\nlibrary(tidyverse) # ol' faithful\nlibrary(WeightIt) # for IP weighting\nlibrary(kableExtra) # for formatting table output\nlibrary(ggdag) # for drawing a DAG \n\n#... Functions ----\n\n# We will use this function to simulate data \n\nsim_data <- function(n = 250, # sample size \n                     beta_trt = 1.5, # treatment effect\n                     # Parameters for Z1 \n                     z1_mean = 5, z1_sd = 1, \n                     # Parameters for Z2\n                     z2_size = 1, z2_prob = 0.5, \n                     # Parameters for U1\n                     u1_mean = 10, u1_sd = 3,\n                     u2_mean = 2, u2_sd = 0.5,\n                     # Confounder - Effect on X\n                     z1_on_x = 0.05, z2_on_x = 0.5, \n                     u1_on_x = 0.08, u2_on_x = 0.8, \n                     # Confounder - Effect on Y\n                     z1_on_y = 0.5, z2_on_y = 0.7,\n                     u1_on_y = 0.4, u2_on_y = 2\n){\n  \n  # Creating the Dataframe\n  \n  df <- data.frame(\n    z1 = rnorm(n = n, mean = z1_mean, sd = z1_sd), # measured continuous confounder\n    z2 = rbinom(n = n, size = z2_size, prob = z2_prob), # measured binary confounder\n    u1 = rnorm(n = n, mean = u1_mean, sd = u1_sd), # unmeasured confounder \n    u2 = rnorm(n = n, mean = u2_mean, sd = u2_sd) # unmeasured confounder \n  ) %>% \n    dplyr::mutate(\n      beta_trt = 1.5, \n      prob = plogis(u1_on_x*u1 - u2_on_x*u2 + z1_on_x*z1 + z2_on_x*z2), \n      x = rbinom(n = n, size = 1, prob = prob), \n      y = beta_trt*x + z1_on_y*z1 + z2_on_y*z2 + u1_on_y*u1 + u2_on_y*u2 + rnorm(n = n, mean = 0, sd = 1)\n      #y = beta_trt*x + z1_on_y*z1 + z2_on_y*z2 + rnorm(n = n, mean = 0, sd = 1)\n    )\n  \n  # Return the dataframe\n  \n  return(df)\n}\n```\n:::\n\n\nAnd of course, we need a directed acyclic graph (DAG)!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_dag())\n\ndag_example <- ggdag::dagify(\n  x ~ z1 + z2 + u1 + u2, \n  y ~ z1 + z2 + u1 + u2 + x, \n  exposure = \"x\",\n  outcome = \"y\"\n)\n\nggdag::ggdag(dag_example, layout = \"nicely\")\n```\n\n::: {.cell-output-display}\n![](dr_unconf_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Perfectly Imperfect\n\nFor our first scenario, let's assume that we perfectly specify both models based on the data we have. We will include both of the measured confounders (z1 & z2), in each model. However, the model will still be imperfect since there is unmeasured confounding.\n\nHow does it look for each method?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inverse Probability Weighting (IPW) ----\n\nipw <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1 + z2, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x, # note: not doubly robust, so only including exposure variable\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, ipw(sample.size = 250), simplify = FALSE)\n\ndf.out.ipw <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\nipw.bias <- mean(df.out.ipw$bias_for_one) # bias  \nipw.se.of.bias <- sqrt(sum(df.out.ipw$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Outcome Model ----\n\nout_mod <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting outcome model \n  \n  mod <- glm(y ~ x + z1 + z2, # note: not doubly robust for this example\n             family = gaussian(link = \"identity\"), \n             data = df)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, out_mod(sample.size = 250), simplify = FALSE)\n\ndf.out.outmod <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\noutmod.bias <- mean(df.out.outmod$bias_for_one) # bias  \noutmod.se.of.bias <- sqrt(sum(df.out.outmod$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Doubly Robust ----\n\ndr <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1 + z2, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x + z1 + z2, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, dr(sample.size = 250), simplify = FALSE)\n\ndf.out.dr <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\ndr.bias <- mean(df.out.dr$bias_for_one) # bias  \ndr.se.of.bias <- sqrt(sum(df.out.dr$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n\n# Results ----\n\nresults <- data.frame(\n  `Method` = c(\"IPW\", \"Outcome Model\", \"Doubly Robust\"), \n  `Bias` = c(ipw.bias, outmod.bias, dr.bias), \n  se_of_bias = c(ipw.se.of.bias, outmod.se.of.bias, dr.se.of.bias)\n) %>% \n  rename(`Monte Carlo SE of Bias` = se_of_bias)\n\nknitr::kable(results) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; \">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Method </th>\n   <th style=\"text-align:right;\"> Bias </th>\n   <th style=\"text-align:right;\"> Monte Carlo SE of Bias </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> IPW </td>\n   <td style=\"text-align:right;\"> -0.1112812 </td>\n   <td style=\"text-align:right;\"> 0.0074219 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Outcome Model </td>\n   <td style=\"text-align:right;\"> -0.0956331 </td>\n   <td style=\"text-align:right;\"> 0.0074621 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Doubly Robust </td>\n   <td style=\"text-align:right;\"> -0.1074268 </td>\n   <td style=\"text-align:right;\"> 0.0075944 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nInteresting! So we can see from above that if we use a doubly robust method, there is some benefit although it might be minimal. This begs another question, what if we misspecify both models? Would a doubly robust method be better there?\n\n## Imperfect Model in an Imperfect World\n\nFor this example, we will include only one confounder for each model. For the propensity score model, we'll include z1. For the outcome model, we'll include z2. How does this look?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# IPW ----\n\nipw <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, ipw(sample.size = 250), simplify = FALSE)\n\ndf.out.ipw <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\nipw.bias <- mean(df.out.ipw$bias_for_one) # bias  \nipw.se.of.bias <- sqrt(sum(df.out.ipw$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Outcome Model ----\n\nout_mod <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting outcome model \n  \n  mod <- glm(y ~ x + z2, # note: not doubly robust for this example\n             family = gaussian(link = \"identity\"), \n             data = df)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, out_mod(sample.size = 250), simplify = FALSE)\n\ndf.out.outmod <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\noutmod.bias <- mean(df.out.outmod$bias_for_one) # bias  \noutmod.se.of.bias <- sqrt(sum(df.out.outmod$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Doubly Robust ----\n\ndr <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x + z2, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, dr(sample.size = 250), simplify = FALSE)\n\ndf.out.dr <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\ndr.bias <- mean(df.out.dr$bias_for_one) # bias  \ndr.se.of.bias <- sqrt(sum(df.out.dr$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n\n# Results ----\n\nresults <- data.frame(\n  `Method` = c(\"IPW\", \"Outcome Model\", \"Doubly Robust\"), \n  `Bias` = c(ipw.bias, outmod.bias, dr.bias), \n  se_of_bias = c(ipw.se.of.bias, outmod.se.of.bias, dr.se.of.bias)\n) %>% \n  rename(`Monte Carlo SE of Bias` = se_of_bias)\n\nknitr::kable(results) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; \">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Method </th>\n   <th style=\"text-align:right;\"> Bias </th>\n   <th style=\"text-align:right;\"> Monte Carlo SE of Bias </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> IPW </td>\n   <td style=\"text-align:right;\"> -0.0352957 </td>\n   <td style=\"text-align:right;\"> 0.0076658 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Outcome Model </td>\n   <td style=\"text-align:right;\"> -0.0852447 </td>\n   <td style=\"text-align:right;\"> 0.0075845 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Doubly Robust </td>\n   <td style=\"text-align:right;\"> -0.1120202 </td>\n   <td style=\"text-align:right;\"> 0.0075194 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe doubly robust method is actually **MORE** misspecified ðŸ˜². Now, you may be thinking \"duh, both models are misspecified\". This is true, but both were technically misspecified in the previous example (since unmeasured confounding wasn't included). What would happen if we applied the same methodology, but in an example where there is only measured confounding?\n\n## Measured Confounding\n\nLet's use the same approach as before where both the propensity score and outcome model are missing a measured covariate, with a twist: this time no unmeasured confounding.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function for simulating data ----\n\n# This function is the same as what we've been using previously with one crucial point: \n# There is no unmeasured confounding.\n\nsim_data <- function(n = 250, # sample size \n                     beta_trt = 1.5, # treatment effect\n                     # Parameters for Z1 \n                     z1_mean = 5, z1_sd = 1, \n                     # Parameters for Z2\n                     z2_size = 1, z2_prob = 0.5, \n                     # Parameters for U1\n                     u1_mean = 10, u1_sd = 3,\n                     u2_mean = 2, u2_sd = 0.5,\n                     # Confounder - Effect on X\n                     z1_on_x = 0.05, z2_on_x = 0.5, \n                     u1_on_x = 0.08, u2_on_x = 0.8, \n                     # Confounder - Effect on Y\n                     z1_on_y = 0.5, z2_on_y = 0.7,\n                     u1_on_y = 0.4, u2_on_y = 2\n){\n  \n  # Creating the Dataframe\n  \n  df <- data.frame(\n    z1 = rnorm(n = n, mean = z1_mean, sd = z1_sd), \n    z2 = rbinom(n = n, size = z2_size, prob = z2_prob)\n    #u1 = rnorm(n = n, mean = u1_mean, sd = u1_sd),\n    #u2 = rnorm(n = n, mean = u2_mean, sd = u2_sd)\n  ) %>% \n    dplyr::mutate(\n      beta_trt = 1.5, \n      prob = plogis(z1_on_x*z1 + z2_on_x*z2), # removed u1 & u2\n      x = rbinom(n = n, size = 1, prob = prob), \n      y = beta_trt*x + z1_on_y*z1 + z2_on_y*z2 + rnorm(n = n, mean = 0, sd = 1) # removed u1 & u2\n    )\n  \n  # Return the dataframe\n  \n  return(df)\n}\n\n\n# IPW ----\n\nipw <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, ipw(sample.size = 250), simplify = FALSE)\n\ndf.out.ipw <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\nipw.bias <- mean(df.out.ipw$bias_for_one) # bias  \nipw.se.of.bias <- sqrt(sum(df.out.ipw$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Outcome Model ----\n\nout_mod <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting outcome model \n  \n  mod <- glm(y ~ x + z2, # note: not doubly robust for this example\n             family = gaussian(link = \"identity\"), \n             data = df)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, out_mod(sample.size = 250), simplify = FALSE)\n\ndf.out.outmod <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\noutmod.bias <- mean(df.out.outmod$bias_for_one) # bias  \noutmod.se.of.bias <- sqrt(sum(df.out.outmod$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n# Doubly Robust ----\n\ndr <- function(sample.size){\n  \n  trt_effect <- 1.5 # same as in sim_data() function \n  \n  # Simulating data\n  \n  df <- sim_data()\n  \n  # Fitting PS model - estimating the average treatment effect and stabilizing the weights\n  \n  ps.mod <- WeightIt::weightit(x ~ z1, \n                               data = df, \n                               method = \"glm\", \n                               estimand = \"ATE\", \n                               stabilize = TRUE)\n  \n  # Fitting outcome model \n  \n  mod <- glm_weightit(y ~ x + z2, # note: not doubly robust for this example\n                      family = gaussian(link = \"identity\"), \n                      data = df,\n                      weights = ps.mod$weights)\n  \n  estimate <- broom::tidy(mod)$estimate[2] # this is the estimate\n  \n  bias_for_one = estimate - trt_effect # comparing the estimate to the \"true\" effect \n  \n  df_bias = data.frame(\n    bias_for_one,\n    estimate # need for estimating relative precision later \n  )\n  \n  return(df_bias)\n  \n}\n\n# Repeating 1000 times using a sample size of 250\n\noutput_list <- replicate(1000, dr(sample.size = 250), simplify = FALSE)\n\ndf.out.dr <- do.call(rbind, output_list) %>% # reformatting\n  \n  # Adding a new column that will be used to estimate the Monte Carlo SE of the estimate\n  mutate(\n    squared = (bias_for_one - mean(bias_for_one))^2,\n    residuals_squared = (estimate - mean(estimate))^2\n  )\n\n\n# Calculating the mean bias and Monte Carlo SE of estimate\n\n# See Morris et al. (2019) for details on calculating these\n\ndr.bias <- mean(df.out.dr$bias_for_one) # bias  \ndr.se.of.bias <- sqrt(sum(df.out.dr$squared)*(1 / (1000*999))) # Monte Carlo SE of bias (1000 is number of simulations, 999 is n - 1)\n\n\n# Results ----\n\nresults <- data.frame(\n  `Method` = c(\"IPW\", \"Outcome Model\", \"Doubly Robust\"), \n  `Bias` = c(ipw.bias, outmod.bias, dr.bias), \n  se_of_bias = c(ipw.se.of.bias, outmod.se.of.bias, dr.se.of.bias)\n) %>% \n  rename(`Monte Carlo SE of Bias` = se_of_bias)\n\nknitr::kable(results) %>% \n  kable_styling(bootstrap_options = \"striped\", full_width = F, position = \"left\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"width: auto !important; \">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Method </th>\n   <th style=\"text-align:right;\"> Bias </th>\n   <th style=\"text-align:right;\"> Monte Carlo SE of Bias </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> IPW </td>\n   <td style=\"text-align:right;\"> 0.0845856 </td>\n   <td style=\"text-align:right;\"> 0.0043921 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Outcome Model </td>\n   <td style=\"text-align:right;\"> 0.0241229 </td>\n   <td style=\"text-align:right;\"> 0.0046696 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Doubly Robust </td>\n   <td style=\"text-align:right;\"> -0.0004500 </td>\n   <td style=\"text-align:right;\"> 0.0041988 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nHuh? Doubly robust DOES help in this case! So what in tarnation is going on?!\n\n## Bias....Bias....And....MORE BIAS!\n\nAt this point, we have three different scenarios we can use to try and piece together an explanation. Let's dive in.\n\nFirst, the bias of a doubly robust method is the product of the bias from each model [@funk2011doubly]:\n\n$$\nBias_{Doubly\\ Robust\\ Method} = Bias_{Treatment \\ Model}*Bias_{Outcome \\ Model}\n$$\n\nThis means that if *either* of the models are correctly specified, then we're good to go! However, if both models are misspecified....we've got issues. The problem is it depends on the *degree* of bias of each model.\n\nSo what does this have to do with confounding? I'm glad you asked.\n\nConfounding causes bias. It doesn't matter if it's measured or unmeasured. If we're trying to fit a model and there is unmeasured confounding, we could have some problems. Even if we correctly model the measured confounding there is bias from unmeasured confounding. How much depends on the quantity and magnitude of confounders.\n\nThis bias could be magnified in a doubly robust method, if both models have a large amount of bias.\n\nSo what do we with this information?\n\n## What's Next?\n\nConsider if there is unmeasured confounding in your study, and if you are confident in the model being correctly specified. If we have the measured confounding model correctly, based on our examples, there seems to be minimal benefit but not much of a detriment.\n\nHowever, if you believe the model isn't correct, or there is a large amount of unmeasured confounding it's worth considering if a doubly robust method is appropriate.\n\nThis is intended to be an introductory post. The specifics of your analysis may differ.\n\nI'd love to hear your thoughts on doubly robust methods for unmeasured confounding! Hope this post gave you something to consider going forward.\n\n## \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}