[
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting | Ryan Batten",
    "section": "",
    "text": "My goal is to ensure that the research you want to conduct provides meaningful, actionable real-world evidence while being methodologically sound. Whether it’s designing a study, analyzing data, or explaining complex concepts in an easy-to-understand manner, I’m committed to delivering high-quality work that meets your specific need."
  },
  {
    "objectID": "consulting.html#services-i-offer",
    "href": "consulting.html#services-i-offer",
    "title": "Consulting | Ryan Batten",
    "section": "Services I Offer",
    "text": "Services I Offer\nStudy Design: Crafting an effective research study requires a thorough understanding of your specific objectives. I specialize in designing robust, efficient studies that consider all these aspects, utilizing appropriate methodologies and rigorous statistical models to ensure the integrity of the research and its findings.\nStatistical Analysis: I offer comprehensive statistical analysis services, applying appropriate statistical methods to real-world data to identify patterns and extract meaningful insights. This approach to data analysis allows for accurate interpretation that can inform decisions.\nClear Communication of Complex Topics: Understanding complex statistical data and epidemiological findings is crucial in translating research into actionable insights. However, the complexity of these topics can often be a barrier. I have a talent for conveying intricate subjects in a concise and easy to understand manner. This facilitates better understanding and application of research findings."
  },
  {
    "objectID": "consulting.html#reach-out",
    "href": "consulting.html#reach-out",
    "title": "Consulting | Ryan Batten",
    "section": "Reach Out!",
    "text": "Reach Out!\nFree 15-Minute Consultation"
  },
  {
    "objectID": "posts/dags/dags.html",
    "href": "posts/dags/dags.html",
    "title": "Hot Diggity DAG",
    "section": "",
    "text": "Yes..well no…well kinda. DAG in this case, stands for Directed Acyclic Graph. It’s a graph that is directed and acyclic…alright that’s not much of an explanation. Let’s try again: a causal DAG is used to show the causal relationship between your variables.\n\nOf course to show these variables, we are inherently making assumptions. For example, assuming that X causes Y. We always make assumptions in science, since the real world is complicated. These assumptions are sometimes made implicitly, making it difficult to address for reviewers or people reading your work.\n\nAlternatively, if we show our assumptions explicitly then it makes the assumptions more obvious. To quote Miguel Hernán, since I couldn’t put it better myself, “Draw your assumptions before your conclusions”.\n\n\n\n\n\n\nThis post is meant to be an introduction\n\n\n\nA minor note before we continue: this post is meant to be an introduction. DAGs can get quite complicated, especially is you add a lot fo variables, and have a variety of uses (i.e., selection of variables, identifying potential biases, guiding simulations, for missingness, etc). Future posts will delve more into these use cases but for now, it’s just meant to be a gentle intro."
  },
  {
    "objectID": "posts/dags/dags.html#dont-analyze-graphs",
    "href": "posts/dags/dags.html#dont-analyze-graphs",
    "title": "Hot Diggity DAG",
    "section": "",
    "text": "Yes..well no…well kinda. DAG in this case, stands for Directed Acyclic Graph. It’s a graph that is directed and acyclic…alright that’s not much of an explanation. Let’s try again: a causal DAG is used to show the causal relationship between your variables.\n\nOf course to show these variables, we are inherently making assumptions. For example, assuming that X causes Y. We always make assumptions in science, since the real world is complicated. These assumptions are sometimes made implicitly, making it difficult to address for reviewers or people reading your work.\n\nAlternatively, if we show our assumptions explicitly then it makes the assumptions more obvious. To quote Miguel Hernán, since I couldn’t put it better myself, “Draw your assumptions before your conclusions”.\n\n\n\n\n\n\nThis post is meant to be an introduction\n\n\n\nA minor note before we continue: this post is meant to be an introduction. DAGs can get quite complicated, especially is you add a lot fo variables, and have a variety of uses (i.e., selection of variables, identifying potential biases, guiding simulations, for missingness, etc). Future posts will delve more into these use cases but for now, it’s just meant to be a gentle intro."
  },
  {
    "objectID": "posts/dags/dags.html#why-should-i",
    "href": "posts/dags/dags.html#why-should-i",
    "title": "Hot Diggity DAG",
    "section": "Why should I?",
    "text": "Why should I?\nA perfectly valid question at this point is “Why should I? I don’t need these. I know what assumptions I’m making”. While this may be true, a picture is worth a thousand words. It’s much, MUCH easier to identify biases visually than abstractly.\n\nAn equally, perhaps even more important point, is by using DAGs it is more clear for others to interpret/assess your assumptions. For example, confounding bias occurs when there’s a variable that affects both the exposure and the outcome. This is pretty straightforward to view.\nA slightly more challenging example is collider bias. This occurs when ends of two different arrows meet at a variable. Collider bias is easy to identify visually but can be more difficult to identify abstractly, especially the more variables you add. Rather than just speaking, time for an example!"
  },
  {
    "objectID": "posts/dags/dags.html#does-coffee-increase-my-alertness",
    "href": "posts/dags/dags.html#does-coffee-increase-my-alertness",
    "title": "Hot Diggity DAG",
    "section": "Does Coffee Increase My Alertness?",
    "text": "Does Coffee Increase My Alertness?\nI’m an avid coffee drinker. I love hot coffee, iced coffee, cold brew, nitro cold brew…you name it, I’m in (unless it’s bad coffee). Something I often wonder is if drinking a lot of coffee actually increases my alertness or not. That is, does coffee cause increased alertness? Let’s start here. We’ll assume that coffee does cause alertness and draw a line between the two.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggdag)\ntheme_set(theme_dag())\n\ncoffee_dag &lt;- ggdag::dagify(\n  alert ~ coffee, \n  exposure = \"coffee\",\n  outcome = \"alert\",\n  labels = c(\n    coffee = \"Coffee\",\n    alert = \"Alertness\"\n  )\n)\n\nggdag::ggdag(coffee_dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\nThis looks great but there’s one glaring problem….there are many, many, MANY variables that affect coffee and alertness. How much you’ve slept, stress and hunger just to name a few. Not to mention that each of those variables are related. So our DAG needs some editing, but first we need to go over some assumptions or “house rules” per say."
  },
  {
    "objectID": "posts/dags/dags.html#assumptions-of-dags",
    "href": "posts/dags/dags.html#assumptions-of-dags",
    "title": "Hot Diggity DAG",
    "section": "Assumptions of DAGs",
    "text": "Assumptions of DAGs\nLet’s start with the acronym. They are called DAGs because they have a direction and cannot have a variable that causes itself, either directly or through another variable (Hernan and Robins 2021, 72).\nAnother key component is the causal Markov assumption, which put simply is that a variable is independent of any variable that it isn’t a cause for (for more details about this check out Hernan and Robins (2021), pp. 72). This assumption means that in a causal DAG, the common causes of any pair of variables in the graph must also be in the graph.\n\nD-Separation\nThe d in d-separation stands for directionally separated. If all paths between two variables are blocked then we say that variables are d-separated (others they are d-connected). To decide if a path is blocked or open, we use the following rules (Hernan and Robins 2021, 78):\n\nIf no variables are being conditioned on, a path is blocked if and only if two arrowheads on the path collide at some variable on the path.\nAny path that contains a non-collider that has been conditioned on is blocked.\nA collider that has been conditioned on does not block a path.\nA collider that has a descendant that has been conditioned on does not block a path.\n\n\n\n\n\n\n\nCausal Graph Theory\n\n\n\nThere is some complex math, nonparametric structural equation models to be exact, behind these rules that I won’t go into here. If you are interested, I recommend checking out the work of Judea Pearl, or Chapter 6 of Hernan and Robins (2021).\n\n\nTo summarize d-seperation in a few sentences: a path is blocked if and only if, it contains a non-collider that has been conditioned on, or it contains a collider that has not been conditioned on and has no descendants that have been conditioned on (Hernan and Robins 2021, 78)."
  },
  {
    "objectID": "posts/dags/dags.html#water-flowing-in-a-pipe",
    "href": "posts/dags/dags.html#water-flowing-in-a-pipe",
    "title": "Hot Diggity DAG",
    "section": "Water Flowing in a Pipe?",
    "text": "Water Flowing in a Pipe?\nA useful way to think of this, to borrow a concept from Judea Pearl, is to think of water flowing in a pipe. For associations, this can flow both ways however when determining causality it cannot. Applying this thinking to the rules for d-separation think about it like water. If there is a collider, then water cannot flow like there is a dam. If the collider is conditioned on, its like we moved the dam and water can now flow."
  },
  {
    "objectID": "posts/dags/dags.html#variables-to-include",
    "href": "posts/dags/dags.html#variables-to-include",
    "title": "Hot Diggity DAG",
    "section": "Variables to Include",
    "text": "Variables to Include\nLet’s return to our original DAG with just coffee and alertness. We need to include some more variables here, but which ones? Let’s assume that we have some other things we think can be important to include: deep breathing and sleep.\nSleep definitely affects coffee consumption and alertness. This we need to add to our graph because it’s a common cause of coffee consumption and alertness.\n\n\nCode\ncoffee_dag &lt;- ggdag::dagify(\n  alert ~ coffee + sleep, \n  coffee ~ sleep,\n  exposure = \"coffee\",\n  outcome = \"alert\",\n  labels = c(\n    coffee = \"Coffee\",\n    alert = \"Alertness\",\n    sleep = \"Sleep\"\n  )\n)\n\nggdag::ggdag(coffee_dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\nNow, we can see here that sleep is a confounder. We can use our fancy new DAG to identify which variables are d-connected.\n\n\nCode\nggdag::ggdag_paths(coffee_dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\nWe can see that there are two paths between the variables. There is the first path between coffee and alertness, which we’d expect. The second path, is between coffee and alertness through sleep. This is not what we’d want and in this case would result in confounding bias if we don’t fix it. How do we fix it? By including it as an adjustment variable.\nWe can use the ggdag_adjustment_set() function from the ggdag package.\n\n\nCode\nggdag::ggdag_adjustment_set(coffee_dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\nWe can see that we need to adjust for sleep to close the opened path, which is what we’d expect. Now we can move onto deep breathing and find out whether we need to adjust for deep breathing or not.\n\nDeep breathing only affects alertness, or so we assume, this is an example of effect modification (spoiler alert: we can include if we want but the DAG is perfectly valid if not since deep breathing isn’t a common cause of A and Y)"
  },
  {
    "objectID": "posts/dags/dags.html#effect-modification",
    "href": "posts/dags/dags.html#effect-modification",
    "title": "Hot Diggity DAG",
    "section": "Effect Modification",
    "text": "Effect Modification\n\n\nCode\ncoffee_dag &lt;- ggdag::dagify(\n  alert ~ coffee + deep_breaths, \n  exposure = \"coffee\",\n  outcome = \"alert\",\n  labels = c(\n    coffee = \"Coffee\",\n    alert = \"Alertness\",\n    deep_breaths = \"Deep Breathing\"\n  )\n)\n\nggdag::ggdag(coffee_dag, \n             layout = \"circle\", \n             text = FALSE, use_labels = \"label\")\n\n\n\n\n\nWe see that deep breathing affects alertness but not drinking coffee. Now, if we didn’t include deep breathing it would still be a valid causal diagram because deep breathing isn’t a common cause of coffee and alertness. We’d only need to include if it was part of our causal question (e.g., what is the average causal effect of coffee on alertness within levels of deep breathing?).\nNow, another point is that including it in our causal diagram doesn’t distinguish which of the following three ways that alertness could modify the effect of coffee on alertness:\n\nThe causal effect of coffee on alertness is in the same direction in both deep breathing = yes and deep breathing = no\nThe direction of the causal effect of coffee on alertness in deep breathing = yes is opposite of that in stratum deep breathing = no\nCoffee has a causal effect on alertness in one stratum of deep breathing but no causal effect in the other stratum.\n\nIn the DAG, it fails to distinguish which of these is the type of effect modification. Additionally, many effect modifiers do not have a causal effect on the outcome. Instead, they are surrogates for variables that have a causal effect on the outcome.\nFor example, let’s look at deep breathing again.\n\n\nCode\ncoffee_dag &lt;- ggdag::dagify(\n  alert ~ coffee + stress, \n  deep_breaths ~ stress,\n  exposure = \"coffee\",\n  outcome = \"alert\",\n  labels = c(\n    coffee = \"Coffee\",\n    alert = \"Alertness\",\n    deep_breaths = \"Deep Breathing\",\n    stress = \"Stress\"\n  )\n)\n\nggdag::ggdag(coffee_dag, \n             layout = \"circle\", \n             text = FALSE, use_labels = \"label\")\n\n\n\n\n\nIn this DAG, we see deep breathing is actually a surrogate for a variable that has a causal effect. Deep breathing is a surrogate effect modifier whereas stress is a causal effect modifier. Both of these are often indistinguishable in practice, so the concept of effect modification encompasses both (Hernan and Robins 2021, 81)."
  },
  {
    "objectID": "posts/dags/dags.html#whats-next",
    "href": "posts/dags/dags.html#whats-next",
    "title": "Hot Diggity DAG",
    "section": "What’s Next?",
    "text": "What’s Next?\nNow you’ve grasped the basics of DAGs! You can use these to help answer all your causally related questions! Personally, I mostly use them for variable selection. It’s extremely helpful in analyses for real-world data when deciding which variables require adjustment. Hope you had as much fun reading it as I did writing it!"
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "",
    "text": "You may have seen real-world evidence (RWE) being used increasingly in research settings. It has tremendous potential for answering important research questions, however what exactly is RWE? RWE is evidence generated from real-world data (RWD). RWD is defined by the FDA as data relating to patient health status and/or the delivery of health care routinely collected from a variety of sources (FDA December 2018). This data could be from electronic health records, medical claims and billing, electronic medical records, data from product registries, wearable devices, mobile devices and other sources. Sounds pretty good right? Well the quality of the evidence is only as a good as the quality of the study conducted."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#what-is-rwe",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#what-is-rwe",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "",
    "text": "You may have seen real-world evidence (RWE) being used increasingly in research settings. It has tremendous potential for answering important research questions, however what exactly is RWE? RWE is evidence generated from real-world data (RWD). RWD is defined by the FDA as data relating to patient health status and/or the delivery of health care routinely collected from a variety of sources (FDA December 2018). This data could be from electronic health records, medical claims and billing, electronic medical records, data from product registries, wearable devices, mobile devices and other sources. Sounds pretty good right? Well the quality of the evidence is only as a good as the quality of the study conducted."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#hierarchy-of-research",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#hierarchy-of-research",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "Hierarchy of Research",
    "text": "Hierarchy of Research\nHistorically, the hierarchy of research has been that systematic reviews with a meta-analysis are at the top of the evidence pyramid, followed by randomized controlled trials (RCTs), cohort studies, case-control studies and anecdotal research.\n\n\n\nFigure 1: Hierarchy\n\n\nRCTs are near the top of this pyramid for good reason. A well-conducted RCT can provide insight into efficacy of a treatment in a controlled environment so that no data is missing, both observed and unobserved confounders are balanced between groups and you can measure the variables you need for analysis. There are however, drawbacks/potential limitations: RCTs can be expensive to conduct, can require a long duration of follow-up, are not feasible to conduct ethically, limited sample size for assessing safety and limited generalizability. Luckily, observational study designs can help fill in the gaps in these areas."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#observational-studies",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#observational-studies",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "Observational Studies",
    "text": "Observational Studies\nWhen people discuss about the hierarchy of study design, such as in Figure 1, the quality of the study is not mentioned. For example, is a poorly conducted RCT better than a rigorously conducted case-control study using observational data? That is up for debate however, observational studies can be used to supplement findings from RCTs or where RCTs are infeasible.\n\nOne of the key issues with observational studies is the variability in methodological rigor. Due to this, results can vary widely from study to study and really depend upon the researchers conducting the study. Luckily, recent strides have been made in this field. To mention a few, the STaRT-RWE template (Wang et al. 2021) and the principles behind emulating a target trial proposed by Hernan & Robins (Hernán and Robins 2016)."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#examples-of-rwe",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#examples-of-rwe",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "Examples of RWE",
    "text": "Examples of RWE\nDiscussing about RWE in theory is useful but what are some use cases? Recently, regulatory bodies are accepting RWE as part of submissions. These can vary from accepting burden of illness studies to help contextualize the current treatment and cost of available treatments to external control arms (ECA). ECAs can be beneficial when a single arm clinical trial is conducted. In certain disease areas, such as cancer, single arm trials are common but this makes it difficult for regulatory reviewers to ascertain the efficacy of a treatment since there is no comparator arm. Demonstration of safety is another area that RWE is well-suited due to the larger sample size and the generalizability of findings to real-world settings.\nPurpura et al. (Purpura et al. 2022) reported 116 FDA-approvals of New Drug and Biologics License Applications. Of these 116, 83 used RWE to support therapeutic context and 88 used RWE to support safety and/or effectiveness. Over the coming years, these numbers are expected to rise as the methodological rigor of these studies increases."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#whats-next",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#whats-next",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "What’s Next?",
    "text": "What’s Next?\nRWE is starting to be adopted more by regulatory bodies, with the FDA accepting RWE as part of submissions (FDA December 2018), as well as the National Institute for Health and Care Excellence (NICE) issuing guidance for the development of RWE. It is a very exciting time for developing RWE given the vast data that is available! However, always keep in mind that these studies need to be conducted to similar standards that we would hope a clinical trial is."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html",
    "href": "posts/marginal-conditional/marginal_conditional.html",
    "title": "Marginal vs Conditional Effects",
    "section": "",
    "text": "This post is going to be about marginal compared to conditional effects. First, we need to understand what in the world these terms even mean. To do that, can you guess what we are going to start with? Yup! Probability again."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#marginally-important",
    "href": "posts/marginal-conditional/marginal_conditional.html#marginally-important",
    "title": "Marginal vs Conditional Effects",
    "section": "",
    "text": "This post is going to be about marginal compared to conditional effects. First, we need to understand what in the world these terms even mean. To do that, can you guess what we are going to start with? Yup! Probability again."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#conditional-vs-unconditional-probability",
    "href": "posts/marginal-conditional/marginal_conditional.html#conditional-vs-unconditional-probability",
    "title": "Marginal vs Conditional Effects",
    "section": "Conditional vs Unconditional Probability",
    "text": "Conditional vs Unconditional Probability\nFirst things first. Let’s review some basic probability terms. Unconditional probability, denoted as \\(P(A)\\), is the probability that something will happen. Unconditional probability sounds like a mouthful, so we will use marginal instead. For example, let’s assume in a made-up world that the probability of you getting a visit from the toothfairy is 0.30 on any given night. In mathematical terms:\n\\[\nP(\\text{toothfairy visit}) = 0.30\n\\]\nNow, the toothfairy randomly visiting you seems a bit bizarre doesn’t it? She’d just drop by any night for no reason at all? Maybe if you wore pink pyjamas to bed she’d visit. This can be worded as “the probability that the toothfairy will visit, given that you are wearing pink pyjamas to bed”. This probability is conditional, since it is the probability of something happening depending on another variable. Using mathematical notation again:\n\\[\nP(\\text{toothfairy visit} | \\text{wearing pink pyjamas})\n\\]\nThis same logic goes for when we are conducting analyses. At this point, it is important to highlight that exchangeability and positivity are two requirements for causal inference. If the effect is marginal, then exchangeability and positivity need to hold in all levels of the variable (Hernan and Robins 2021, 51). If the effect of interest is conditional, then exchangeability and positivity need to hold in a subset of the sample. Before going any further however, we need to review the conditional mean."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#conditional-mean",
    "href": "posts/marginal-conditional/marginal_conditional.html#conditional-mean",
    "title": "Marginal vs Conditional Effects",
    "section": "Conditional Mean",
    "text": "Conditional Mean\nA conditional mean is similar to the concept of conditional probability, except the measure is a mean instead of a probability. To determine the conditional mean, we can use either parametric estimators or nonparametric estimators. Parameters can be thought of as the coefficients of a variable. These are typically denoted by \\(\\theta\\) or by \\(\\beta\\). These parameters are estimated from the data, which can then be used to make predictions or to determine the expected value, denoted by \\(E\\).\n\nParametric Estimators of the Conditional Mean\nLet’s assume that we use the toothfairy visit example again. We want to know the average number of toothfairy visits among people wearing pink pyjamas. In math terms, that would be\n\\[\nE[\\text{Toothfairy visits} | \\text {Pyjama color}]\n\\]\nTo figure this out, we need a model. Using mathematical notation again:\n\\[\nE[\\text{Toothfairy visits} | \\text {Pyjama Color}] = \\theta_0 + \\theta_1*\\text{pyjama color}\n\\]\nThe equation above is called a parametric conditional mean model (Hernan and Robins 2021, 141), because it describes the conditional mean function in terms of a finite number of parameters (note: \\(\\theta_0\\) and \\(\\theta_1\\) are referred to as parameters of the model). Once we’ve fitted some data to the model, we can determine the predicted value, \\(\\hat{E}\\), for each value of pyjama color. In mathematical terms:\n\\[\n\\hat{E}[\\text{Toothfairy visits} | \\text{pyjama color = pink}] = \\hat{\\theta_0} + \\hat{\\theta_1}*pink\n\\]\nParametric estimators, those based on a parametric conditional mean model, allow us to estimate quantities that cannot be estimated otherwise. However, the inferences are only correct if the restrictions are correct (i.e., the model is correctly specified) (Hernan and Robins 2021, 152). In the above equation, we have to restrict the shape of the relation, also known as the functional form (Hernan and Robins 2021, 141). For the above example, we are assuming that it is a linear relationship between the color of pyjamas and toothfairy visits.\nImagine that in our database, we have no people that wear pink pyjamas BUT we have people that wear blue and green pyjamas! We could use the data on the people wearing blue and green pyjamas to estimate how many toothfairy visits people wearing pink pyjamas get.\n\n\nNonparametric Estimators of the Conditional Mean\nNonparametric estimators of the conditional mean are those that produce estimates from the data without any prior restrictions on the conditional mean function (Hernan and Robins 2021, 143). To use our example above, the only way to have a nonparametric estimator of the conditional mean would be to have that value measured. For our data on toothfairy visits and pink pyjamas, if there were no participants wearing pink pyjamas then there would be no nonparametric estimator for the conditional mean of the toothfairy visiting."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#conditional-to-marginal",
    "href": "posts/marginal-conditional/marginal_conditional.html#conditional-to-marginal",
    "title": "Marginal vs Conditional Effects",
    "section": "Conditional to Marginal",
    "text": "Conditional to Marginal\nNow, we can calculate the unconditional expectation using the law of total expectation, however when we are talking about conditional versus marginal in clinical epidemiology, there is an additional layer we need to discuss: effect modification.\n\\[\nE[Y] = \\sum_x E[Y|X = x]Pr[X=x]\n\\]"
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#effect-modification",
    "href": "posts/marginal-conditional/marginal_conditional.html#effect-modification",
    "title": "Marginal vs Conditional Effects",
    "section": "Effect Modification",
    "text": "Effect Modification\nAn effect modifier is when the average causal effect varies across levels of that variable (Hernan and Robins 2021, 42). Since our causal effect of interest is pyjama color on toothfairy visits, if we were to stratify by age, we would notice that people who are older than 20 receive less toothfairy visits than people younger than 20. In this case, age would be\nNow, there are four different techniques that we’ll elaborate on subsequent posts that can be used to adjust for effect modification: standardization, IP weighting, stratification/restriction and matching. Standardization and IP weighting can be used to compute either marginal or conditional effects. Stratification/restriction and matching can only be used to compute conditional effects in certain subsets of the population (Hernan and Robins 2021, 51). Logically this makes sense because standardization and IP weighting can be done using the entire sample, while stratification/matching only use a subset of the population!"
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#why-do-we-care",
    "href": "posts/marginal-conditional/marginal_conditional.html#why-do-we-care",
    "title": "Marginal vs Conditional Effects",
    "section": "Why do we care?",
    "text": "Why do we care?\nFinally, why do we care about marginal vs conditional effects at all? Well it matters when we are defining our research question! In order to have a well-defined research question, you need to clearly define what you want to know. For us, it was “does wearing pink pyjamas cause the toothfairy to visit?”."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html",
    "href": "posts/ip-weighting/ip_weighting.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "Re-weighting in this context has nothing to do with weight. Instead it is a statistical method that is used to adjust for confounding to ensure exchangeability. This method can be helpful for answering questions about the marginal or conditional causal effect.\nNow, what is a better way to get started than with probability yet again? In this case, we are going to talk about the probability of receiving treatment.\n\n\n\n\n\n\nNote\n\n\n\nThis post will draw heavily from Chapter 12 of What If (Hernan and Robins 2021)"
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#re-weighting-like-a-scale",
    "href": "posts/ip-weighting/ip_weighting.html#re-weighting-like-a-scale",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "Re-weighting in this context has nothing to do with weight. Instead it is a statistical method that is used to adjust for confounding to ensure exchangeability. This method can be helpful for answering questions about the marginal or conditional causal effect.\nNow, what is a better way to get started than with probability yet again? In this case, we are going to talk about the probability of receiving treatment.\n\n\n\n\n\n\nNote\n\n\n\nThis post will draw heavily from Chapter 12 of What If (Hernan and Robins 2021)"
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#propensity-score",
    "href": "posts/ip-weighting/ip_weighting.html#propensity-score",
    "title": "Inverse Probability Weighting",
    "section": "Propensity Score",
    "text": "Propensity Score\nA term that is commonly used in clinical epidemiology is the propensity score. The propensity score is the conditional probability of receiving treatment (Hernan and Robins 2021), or in mathematical notation:\n\\[\nPr[A = 1| L = l]\n\\]\nWhere A is treatment and L is the covariate(s) of interest. While equations are good, examples are better. Imagine we have a clinical trial with two groups, wanting to determine if bouncing up and down on a trampoline causes a headache. The treatment group get to bounce on a trampoline for 10 minutes, while the control group have to sit down on a chair for 10 minutes. In this scenario, the propensity score would be the conditional probability of getting to bounce on the trampoline based on your age, sex and color shirt you are wearing."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#does-jumping-on-a-trampoline-cause-a-headache",
    "href": "posts/ip-weighting/ip_weighting.html#does-jumping-on-a-trampoline-cause-a-headache",
    "title": "Inverse Probability Weighting",
    "section": "Does jumping on a trampoline cause a headache?",
    "text": "Does jumping on a trampoline cause a headache?\nWhile theoretical discussions are helpful, numbers always help to really drive a point home. While we can’t conduct a clinical trial, luckily there is a database that has already collected data on such a scenario!\n\n\nCode\nset.seed(2218) # August 18, 2022\n\nlibrary(tidyverse)\nlibrary(rwetasks)\n\ndf.tramp &lt;- data.frame(\n  group = \"trampoline\",\n  age = runif(n = 260, min = 9, max = 25),\n  sex = rbinom(n = 260, size = 1, prob = 0.77),\n  t_shirt = sample(c(\"blue\", \"pink\", \"orange\", \"yellow\"),\n                   size = 260, \n                   replace = TRUE, \n                   prob = c(0.17, 0.46, 0.30, 0.07)),\n  brain_freeze = rbinom(260, size = 1, prob = 0.33), # brain freeze before group\n  time_standing = runif(n = 260, min = 0, max = 60), # time standing in minutes\n  headache = rbinom(260, size = 1, prob = 0.64)\n) \n\ndf.chair &lt;- data.frame(\n  group = \"chair\",\n  age = runif(n = 469, min = 19, max = 88),\n  sex = rbinom(n = 469, size = 1, prob = 0.28),\n  t_shirt = sample(c(\"blue\", \"pink\", \"orange\", \"yellow\"),\n                   size = 469, \n                   replace = TRUE, \n                   prob = c(0.36, 0.16, 0.42, 0.23)),\n  brain_freeze = rbinom(469, size = 1, prob = 0.48), # brain freeze before group\n  time_standing = runif(n = 469, min = 0, max = 60), # time standing in minutes\n  headache = rbinom(469, size = 1, prob = 0.39)\n)\n\ndf &lt;- rbind(df.tramp, df.chair) %&gt;% \n  dplyr::mutate(\n    beta_age = runif(1, min = 0, max = 0.10), \n    beta_sex = runif(1, min = 0, max = 0.10),\n    beta_tshirt = runif(1, min = 0, max = 0.10),\n    beta_ts = runif(1, min = 0, max = 0.10), \n    prob_tramp = (beta_age*age + beta_sex*sex + beta_ts*time_standing)/10,\n    tramp_group = rbinom(729, size = 1, prob = prob_tramp),\n    prob_headache = (beta_age*age + 2*beta_sex*sex + 3*beta_ts*time_standing)/20,\n    headache = rbinom(729, size = 1, prob = prob_headache),\n  )\n\n# Trampoline Group Demographics\n\ndf.tramp.demo &lt;- df %&gt;% dplyr::filter(tramp_group == 1)\n\n# cbind(mean(df.tramp.demo$age), sd(df.tramp.demo$age)) # age\n# rwetasks::count_percent(df.tramp.demo, sex) # sex\n# rwetasks::count_percent(df.tramp.demo, t_shirt) # t-shirt\n# rwetasks::count_percent(df.tramp.demo, brain_freeze) # brain-freeze\n# cbind(mean(df.tramp.demo$time_standing), sd(df.tramp.demo$time_standing)) # time standing\n# rwetasks::count_percent(df.tramp.demo, headache) # headache\n\n# Chair Demographics\n\ndf.chair.demo &lt;- df %&gt;% dplyr::filter(tramp_group == 0)\n\n# cbind(mean(df.chair.demo$age), sd(df.chair.demo$age)) # age\n# rwetasks::count_percent(df.chair.demo, sex) # sex\n# rwetasks::count_percent(df.chair.demo, t_shirt) # t-shirt\n# rwetasks::count_percent(df.chair.demo, brain_freeze) # brain-freeze\n# cbind(mean(df.chair.demo$time_standing), sd(df.chair.demo$time_standing)) # time standing\n# rwetasks::count_percent(df.chair.demo, headache) # headache\n\n\n\n\nTable 1: Trampoline Jumpers vs Chair Sitters\n\n\n\n\n\n\n\n\nTrampoline\n(n = 213)\nChair\n(n = 516)\n\n\n\n\nAge, mean (SD)\n54.2 (22.7)\n34.2 (21.0)\n\n\nFemale, n (%)\n71 (33.3)\n258 (50.0)\n\n\nT-Shirt Color, n (%)\nOrange\nBlue\nYellow\nPink\n89 (41.8)\n52 (24.4)\n39 (18.3)\n33 (15.5)\n179 (34.7)\n119 (23.1)\n67 (13.0)\n151 (29.3)\n\n\nBrain Freeze, n (%)\n100 (46.9)\n212 (41.1)\n\n\nTime Standing (minutes), mean (SD)\n31.0 (16.8)\n30.1 (17.9)\n\n\nHeadache\n116 (37.8)\n102 (24.2)\n\n\n\n\nNow, if our causal question is “Does jumping on a trampoline cause a headache?” we need to look at the two groups that we are comparing. If we review Table 1, do the two groups look similar? Well for starters, the mean age is different. People in the trampoline group are an average age of 54 compared to 34 in the chair sitting group. The other characteristics seem quite different as well. If these two are so different how can we expect to even compare the two?! That’s like comparing apples to coconuts!\nWell, luckily we can use a statistical method known as inverse probability weighting to make those coconuts look more like apples. Think of it like we are painting the coconuts, and focusing more on the ones that are a similar size and shape."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#pseudo-population",
    "href": "posts/ip-weighting/ip_weighting.html#pseudo-population",
    "title": "Inverse Probability Weighting",
    "section": "Pseudo-Population",
    "text": "Pseudo-Population\nThe goal of reweighing is to make a pseudo-population where exchangeability holds. Essentially, in our “make believe” sample the two groups would be comparable. Figure 1 shows the two different groups and the proportion of male to females. Now, we can make these more similar by reweighting them to align with the overall sample (n = 729).\n\n\n\nFigure 1: Before and After Reweighting\n\n\nOne way to do this, is to fit a logistic regression (since the outcome would be a yes/no) to trampoline jumpers. Using this model, we can predict the probability of someone being a trampoline jumper, or conversely the probability that they are not a trampoline jumper. Once we do that, we can compare the two groups. First, we need to go over some basics of IP weighting.\n\n\n\n\n\n\nImportant\n\n\n\nReweighting here is done to estimate the average treatment effect (ATE). Depending upon what the estimand of interest is, the weighting may be different. For example, if you want to estimate the average treatment effect in the treated (ATT), the people in the treated group would receive a weight of 1.0 while those in the control group are reweighted. For more information on choosing the estimand of interest, I recommend reading Greifer and Stuart (2021)."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#ip-weighting",
    "href": "posts/ip-weighting/ip_weighting.html#ip-weighting",
    "title": "Inverse Probability Weighting",
    "section": "IP Weighting",
    "text": "IP Weighting\nThe pseudo-population is created by weighting each individual by the inverse of the conditional probability of receiving the treatment they did actually receive (Hernan and Robins 2021, 151). The formula for this is:\n\\[\nW^a = \\frac{1}{f[A|L]}\n\\]\nFor our example, \\(f[A|L]\\) is the probability of being a trampoline jumper conditional on the measured confounders. In mathematical notation:\n\\[\nPr[A = \\text{trampoline jumpers} | \\text{L = age, sex, time standing}]\n\\]\nNow, from probability we know that the total probability has to equal 1. So:\n\\[\nPr[A = \\text{not trampoline jumpers}|L] = 1 - Pr[A = \\text{trampoline jumpers} | L]\n\\]\nGreat! So basically all we need to do is calculate the probability of being a trampoline jumper given measured confounders then we can calculate the conditional probability of not being a trampoline jumper . Now how do we calculate this conditional probability?\n\nLogistic Regression\nSince our two groups can be thought of as a binary variable, trampoline jumpers or not trampoline jumpers, we get a parametric estimate using logistic regression. Assuming that our model is correct, we can then predict/estimate \\(Pr[A=\\text{trampoline jumper}|L]\\). If no confounding for the effect of A in the pseudo-population and the model is correctly specified, then association is causation and an unbiased estimator of the associational difference in the pseudo-population (Hernan and Robins 2021, 151) :\n\\[\nE[Headache | A = \\text{trampoline jumper}] - E[Headache | A = \\text{not a trampoline jumper}]\n\\]\nis also an unbiased estimator of the causal difference:\n\\[\nE[Headache^{a = \\text{trampoline jumper}}] - E[Headache^{a = \\text{not a trampoline jumper}}]\n\\]"
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#estimating-weights",
    "href": "posts/ip-weighting/ip_weighting.html#estimating-weights",
    "title": "Inverse Probability Weighting",
    "section": "Estimating Weights",
    "text": "Estimating Weights\nNow we are ready to estimate some weights! We will estimate the weights using a logistic regression with the following confounders: age, sex, and time standing.\n\n\n\n\n\n\nNote\n\n\n\nFor this example we have selected age, sex and time standing to be confounders. We know this because it is a simulated dataset however there are approaches for selecting potential confounders. Methods for selecting confounders will be discussed in an upcoming post.\n\n\nUsing that model we will calculate the weights, for trampoline jumpers as:\n\n\\[\n\\hat{W} = \\frac{1}{\\hat{Pr}[A = \\text{trampoline jumper} | L]}\n\\]\nand for not trampoline jumpers as:\n\\[\n\\hat{W} = \\frac{1}{1 - \\hat{Pr}[A = \\text{trampoline jumper} | L]}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\hat{W}\\) and \\(\\hat{Pr}\\) are the estimated, or predicted, values. A logistic regression is used in this case because our outcome is binary (trampoline jumper: yes/no). Using this model we can predict the conditional probability which then is used to calculate the weights.\n\n\n\n\nCode\nlibrary(tidyverse)\n\nmod.fit &lt;- stats::glm(formula = tramp_group ~ age + as.factor(sex) + time_standing + I(age ^ 2) + I(time_standing ^ 2), \n                      family = binomial(link = \"logit\"),\n                      data = df)\n\ndf.weights &lt;- df %&gt;% \n  dplyr::mutate(\n    ps = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ip_weights = 1/ps,\n    half_ipw = 0.5/ps\n  )\n\n\nNow we have the weights, let’s check the summary statistics of them.\n\n\nCode\nsummary(df.weights$ip_weights)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.074   1.163   1.430   1.997   2.039  12.130 \n\n\nThe IP weights simulate a pseudo-population where all members of the sample are replaced by two copies of themselves. (Hernan and Robins 2021, 153) One copy receives the treatment value A = 1 and the other copy receives the value A = 0. (Hernan and Robins 2021, 153). The expected mean of the weights should be 2 because all individuals are included both under treatment and under no treatment.\nIf we look back to our example, we can examine the summary statistics of these weights. The mean is sufficiently close to 2, 1.997, which is what we’d expect, however the maximum weight is 12!! For one individual to be weighted as 12, that is rather large. Now there are a few options.\nOne would be to create a pseudo-population similar to what we have done, except using 0.5 for the numerator. That is, the unconditional probability of being a trampoline jumper is 0.5 and the unconditional probability of not being a trampoline jumper is 0.5. In this scenario the pseudo-population would be the same size as the study population, and would be equal to if we used \\(\\frac{1}{f(A|L}\\) but divided all the weights by 2 (Hernan and Robins 2021, 153). We can write this more generally\n\nGeneral Formula\nA general formula for \\(W^a\\) is: (Hernan and Robins 2021, 153).\n\\[\nW^a = \\frac{p}{f[A|L]}\n\\]\nwhere \\(p\\) is the unconditional probability of treatment. Note: \\(0&lt; p \\leq 1\\), whereas \\(f[A|L]\\) is the probability of treatment based on covariates L. An alternative is for different people to have different probabilities (Hernan and Robins 2021, 153). A common choice is to use \\(Pr[A = 1]\\) for \\(p\\) in the treated and \\(Pr[A=0]\\) for \\(p\\) in the untreated. \\(Pr\\) in this case would just be the proportion. Using our example again, \\(Pr[A = \\text{trampoline jumpers}] = \\frac{213}{729} = 0.292\\) and for the not trampoline jumpers, \\(Pr[A = \\text{not trampoline jumper}] = \\frac{516}{729} = 0.708\\). If we use these values for the numerator in calculating the weights for our example:\n\n\nCode\nlibrary(tidyverse)\n\ndf.weights &lt;- df %&gt;% \n  dplyr::mutate(\n    ps = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ip_weights = dplyr::case_when(\n      tramp_group == 1 ~ 0.292/ps,\n      tramp_group == 0 ~ 0.708/ps\n  )\n  )\n  \nsummary(df.weights$ip_weights)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4122  0.7843  0.8460  0.9997  1.0805  3.5420 \n\n\nThis is notably different from when we used \\(\\frac{1}{f[A|L]}\\). Now the weights range from 0.412 to 3.54 whereas before they ranged from 1.07 to 12.1. Not only that, but now in the pseudo-population, the ratio of trampoline jumpers to not trampoline jumpers is kept. The stabilizing factor, \\(f[A]\\), is responsible for the narrower range (Hernan and Robins 2021, 153). Weights that use the stabilizng factor are referred to as stabilized weights."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#stabilized-weights",
    "href": "posts/ip-weighting/ip_weighting.html#stabilized-weights",
    "title": "Inverse Probability Weighting",
    "section": "Stabilized Weights",
    "text": "Stabilized Weights\n\\[\nSW^a = \\frac{f(A)}{f[A|L]}\n\\]\nFrom the above section, we saw that the stabilizing factor made our weights have a narrower range. The mean of the stabilized weights is also expected to be 1 because the size of the pseudo-population is equal to the study population (Hernan and Robins 2021, 153). This is important to check when conducting an analysis using stabilized weights. An alternative to using the nonparametric estimator (i.e., the proportion of individuals), is to estimate \\(f[A]\\) using the same model but with an intercept and no covariates. If we do that using our example:\n\n\nCode\nlibrary(tidyverse)\n\nmod.intercept &lt;- stats::glm(formula = tramp_group ~ 1, \n                      family = binomial(link = \"logit\"),\n                      data = df)\n\ndf.weights &lt;- df %&gt;% \n  dplyr::mutate(\n    ps.denominator = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                    type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ps.numerator = predict(mod.intercept, type = \"response\"),\n    ip_weights = dplyr::case_when(\n      tramp_group == 1 ~ ps.numerator/ps.denominator,\n      tramp_group == 0 ~ (1-ps.numerator)/(ps.denominator)\n  )\n  )\n  \nsummary(df.weights$ip_weights)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4125  0.7841  0.8458  0.9997  1.0803  3.5442 \n\n\nNow we have weights with a mean that is close to 1 with a maximum of 3.54. There is still one more thing that has to be checked whenever using these weights: the positivity assumption! We have to ensure that all participants have a greater than 0 probability of being a trampoline jumper! After checking this, we are confident that twe can use these weights.\n\n\n\n\n\n\nNote\n\n\n\nWhile checking the summary statistics and positivity assumption are important, it is always a good idea to additional check:\n\nBalance has been achieved for the variables that were included in the model\nThe distribution of weights, typically done graphically\nCheck the higher order moments. For example, don’t just check to see if mean is similar for a continuous variable, but that standard deviation is similar as well.\n\n\n\nNow which method would we prefer for estimating the weights? Well this comes down to opinion. For this example, both methods give very similar results for the weights. My personal preference is to use the parametric estimated weights, using logistic regression, for both the numerator and denominator rather than a parametric estimator for the denominator and nonparametric for the numerator."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#stabilized-or-nonstabilized",
    "href": "posts/ip-weighting/ip_weighting.html#stabilized-or-nonstabilized",
    "title": "Inverse Probability Weighting",
    "section": "Stabilized or Nonstabilized?",
    "text": "Stabilized or Nonstabilized?\nAt this point, you may be wondering to yourself if we should be using stabilized or nonstabilized weights. One reason is stabilized weights result in narrower 95% CIs (Hernan and Robins 2021, 154). However, this only occurs when the model is not saturated. A model is saturated when the number of parameters equals the number of quantities to be estimated. For example, \\(E[Y|A] = \\beta_0 + \\beta_1*A\\) is a saturated model because it has two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), and two quantities to estimate \\(E[Y|A = 1]\\) and \\(E[Y|A = 0]\\) (Hernan and Robins 2021, 151). Keep in mind this example is for a binary variable, however this becomes nearly impossible to meet for continuous variables since you would need to have a parameter for every value of that variable."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#did-it-work",
    "href": "posts/ip-weighting/ip_weighting.html#did-it-work",
    "title": "Inverse Probability Weighting",
    "section": "Did it work?",
    "text": "Did it work?\nFor any adjustment technique that aims to achieve balance, such as IPTW, entropy balancing, weights used as part of matching adjusted indirect comparisons (MAICs), we need to check to see if balance has actually been met. In this case, we were trying to balance on the confounders age, sex and time standing. We can now check to see if this is achieved.\n\n\nCode\nlibrary(tidyverse)\nlibrary(Hmisc)\n\ntramp.wt &lt;- df.weights %&gt;% dplyr::filter(tramp_group == 1)\n\n# Age \nx &lt;- tramp.wt$age\nwt &lt;- tramp.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Sex\n\n# df.weights %&gt;% \n  # dplyr::filter(tramp_group == 1) %&gt;% \n  # count(sex, \n        #wt = ip_weights)\n\n# Time Standing\n\nx &lt;- tramp.wt$time_standing\nwt &lt;- tramp.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Chair Sitters\n\nchair.wt &lt;- df.weights %&gt;% dplyr::filter(tramp_group == 0)\n\n\n# Age \nx &lt;- chair.wt$age\nwt &lt;- chair.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Sex\n\n# df.weights %&gt;% \n#   dplyr::filter(tramp_group == 0) %&gt;% \n#   count(sex, \n#         wt = ip_weights)\n\n# Time Standing\n\nx &lt;- chair.wt$time_standing\nwt &lt;- chair.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n\n\n\nTable 2: Characteristics After Reweighting\n\n\n\n\n\n\n\n\n\nTrampoline Jumpers\n(n = 213)\nChair Sitters\n(n = 516)\nOverall (unweighted)\n(n = 729)\n\n\n\n\nAge, mean (SD)\n40.1 (23.5)\n40.2 (23.4)\n40.1 (23.3)\n\n\nFemale, n (%)\n92 (43.9)\n231 (44.8)\n329 (45.1)\n\n\nTime Standing, mean (SD)\n32.0 (17.7)\n30.5 (17.6)\n30.4 (17.6)\n\n\n\n\nTable 2 shows the characteristics that we included in the regression model after reweighting using stabilized weights. The mean of both groups is now similar, which consequently is also close to the overall unweighted sample. If these values don’t look any different to you, then compare them to those show in Table 1."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#drumroll-please",
    "href": "posts/ip-weighting/ip_weighting.html#drumroll-please",
    "title": "Inverse Probability Weighting",
    "section": "Drumroll Please!",
    "text": "Drumroll Please!\nNow we are finally ready to answer our question! We will use a logistic regression since our outcome is binary, headache: yes/no, with our new fancy weights!\n\n\n\n\n\n\nNote\n\n\n\nTo determine the 95% CI we need to use a method that takes the IP weighting into account (Hernan and Robins 2021, 152). One approach is to use nonparametric bootstrapping. Another approach is to use the robust variance estimator. Here we will use the robust variance estimator, however it is important to note that the robust variance estimator is conservative since it covers the super-population parameter more than 95% of the time. (Hernan and Robins 2021, 152)\n(Thanks to Giusi Moffa for pointing out I should be more clear about the CIs!)\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(sandwich)\n\nmod.outcome &lt;- stats::glm(\n  formula = headache ~ tramp_group,\n  family = binomial(link = \"logit\"),\n  data = df.weights,\n  weights = ip_weights\n)\n\nsandwich_se &lt;- diag(sandwich::vcovHC(mod.outcome, type = \"HC\"))^0.5\n\noutput &lt;- broom::tidy(mod.outcome) %&gt;% \n  dplyr::filter(\n    term == \"tramp_group\"\n  ) %&gt;% \n  dplyr::mutate(\n    log.upper.ci = estimate + qnorm(0.975)*0.221,\n    log.lower.ci = estimate - qnorm(0.975)*0.221,\n    OR = exp(estimate),\n    upper.ci = exp(log.upper.ci),\n    lower.ci = exp(log.lower.ci)\n  )\n\npaste0(\"OR, \", round(output$OR,3), \";\",\n       \" (95% CI, \", round(output$lower.ci, 2), \"-\", \n       round(output$upper.ci,2), \")\")\n\n\n[1] \"OR, 1.001; (95% CI, 0.65-1.54)\"\n\n\nUsing our stabilized weights that we calculated earlier, we can now answer our causal question: “Does jumping on a trampoline cause headaches?”. Based upon our model, the answer is no since our 95% CI includes unity, aka 1. We are now free to jump on trampolines without the worry of headaches!!"
  },
  {
    "objectID": "posts/causal-estimands/causal_estimands.html",
    "href": "posts/causal-estimands/causal_estimands.html",
    "title": "Who are we talking about?",
    "section": "",
    "text": "A key part of any research question is to figure out who our target population is. These are the people we want to study. For example, is it 85 year olds who play american football? Is it children between 5-11 years of age using the swings at the park? Determining this allows us to figure out who to sample but also who our results will be applicable to. The best way show this is through an example, so without further ado let’s get started!"
  },
  {
    "objectID": "posts/causal-estimands/causal_estimands.html#who-does-this-apply-too",
    "href": "posts/causal-estimands/causal_estimands.html#who-does-this-apply-too",
    "title": "Who are we talking about?",
    "section": "",
    "text": "A key part of any research question is to figure out who our target population is. These are the people we want to study. For example, is it 85 year olds who play american football? Is it children between 5-11 years of age using the swings at the park? Determining this allows us to figure out who to sample but also who our results will be applicable to. The best way show this is through an example, so without further ado let’s get started!"
  },
  {
    "objectID": "posts/causal-estimands/causal_estimands.html#magical-water-gun-for-height",
    "href": "posts/causal-estimands/causal_estimands.html#magical-water-gun-for-height",
    "title": "Who are we talking about?",
    "section": "Magical Water Gun for Height?",
    "text": "Magical Water Gun for Height?\nImagine that monsters came out of your closet, Monsters Inc. style, and claimed that they have a magical water gun that makes you grow taller. We want to know if they are telling the truth or if they are fibbing, but first we need to figure out who exactly they are claiming this will work on.\n\nIn clinical epidemiology, we determine this by following the PICO acronym: Population, Intervention, Comparator and Outcome(s). For this situation, our research question would look something like this: “Does being sprayed with a water gun increased your height, compared to not being sprayed?”.\n\nNow this is a good start but who exactly are we referring too? Who do we want to know if it works for? Everyone in the world? Just children? Short adults? The causal estimand helps us with this part.\n\n\n\nFigure 1: Monsters\n\n\nThese monsters that have shown up all of a sudden say they’ve sprayed some of themselves with the water gun and it’s worked! Some kids who want to grow have eagerly signed up and been sprayed as well. The effect has varied individual by individual so we need to know: who’s this actually working for, if at all? Why? Well both monsters and kids are pretty short, their friends want to know if it will work on them! (cue the monstars from Space Jam). Figure 1 shows what our sample looks like. As you can see, there’s a mixture of monsters and kids in each group which will make it harder to determine if the magical water gun is actually working, or if monsters just grow taller than kids.\n\nThe figure is just illustrative. Let’s look at the actual characteristics from the individuals we will be studying outlined in Table 1.\n\n\nTable 1: Characteristics\n\n\n\n\n\n\n\n\nSprayed by Water Gun\n(n = 458)\nNot Sprayed\n(n = 129)\n\n\n\n\nNumber of eyes, mean (SD)\n2.59 (1.31)\n2.38 (1.29)\n\n\nAge, mean (SD)\n27.75 (40.4)\n23.27 (37.83)\n\n\nMonsters, n (%)\n110 (24.0%)\n25 (19.4%)\n\n\nHeight (in feet), mean (SD)\n1.71 (0.65)\n1.60 (0.66)\n\n\n\n\nThere were 458 individuals sprayed by the water gun, while 129 were not sprayed. There’s differences in age and percentage of monsters. Looking at just height, you could think that this magical water gun works! However….and a big however…does it? Or is it just that it works better on monsters? Or that monsters are taller?\n\nNow, confounding aside, what do we want to know? Who to spray with this gun! It’s not quite as simple and straightforward as that. Let’s briefly go over the four estimands we’ll be talking about in this case."
  },
  {
    "objectID": "posts/causal-estimands/causal_estimands.html#types-of-estimands",
    "href": "posts/causal-estimands/causal_estimands.html#types-of-estimands",
    "title": "Who are we talking about?",
    "section": "Types of Estimands",
    "text": "Types of Estimands\nWe’ll focus on four causal estimands, in no particular order:\n\nAverage treatment effect in the population (ATE)\nAverage treatment effect in the treated (ATT)\nAverage treatment effect in the untreated (ATU)\nAverage treatment effect in the overlap (ATO)\n\nOne important thing to note here is that all of these estimands would be the same in a randomized trial because the characteristics will be the same (at least in theory). Therefore, the ATT, ATU, ATE and ATO will be the same. However, not so in observational data.\nLike many, almost all, areas of science, there are several assumptions that must be met. For causal inference, these assumptions are exchangeability, positivity, consistency, no versions of treatment and no interference. I suggest checking out the appendix in Greifer and Stuart (2021) since these can vary by the estimand selected.\n\n\n\n\n\n\nPotential Outcomes\n\n\n\nAn important component here is the concept of potential outcomes. These can be written as \\(Y_0\\) and \\(Y_1\\) for each person (Greifer and Stuart 2021). For a binary treatment, like in our case, we can think of it as each person has two potential outcomes prior to them getting the treatment or not. After they receive the treatment, only one of them is revealed (Greifer and Stuart 2021).\n\nNow, the individual causal effect is \\(Y_1\\) - \\(Y_0\\) for each person. In a perfect world, we’d know this however the world isn’t perfect. Instead, we take an average (if we assume certain things, described in Greifer and Stuart (2021) ). In math terms, we show this as \\(E[Y_{1} - Y_{0}]\\).\n\n\n\n\n\n\n\n\nPropensity Score\n\n\n\nWe will be using propensity scores to estimate each of these estimands. A brief overview of the propensity score is that it’s the conditional probability of treatment. More simply put, it’s the probability of being sprayed by the water gun based on your number of eyes and if you’re a monster or not.\n\n\n\nATE\nAsk yourself: would it be feasible to spray all eligible people in this study with the magical water gun? If we assume yes, then we could look at the ATE (Desai and Franklin 2019).\nThe ATE is the effect of the treatment for the entire study population (Greifer and Stuart 2021). Another way of thinking of this is to ask how would height change if everyone was sprayed with the water gun versus if no one was sprayed with the water gun. If we write it in math terms, it’s:\n\\[E[Y_{1}-Y_{0}]\\]\nNow, how do we estimate this? One way is to use inverse probability weights. To calculate these, we need the propensity score (PS). Once we know the PS, we can calculate these weights as \\(1/{PS}\\) for the treated group and \\(1/(1-{PS})\\) for the control group. Then we can plug these weights into the outcome model and away we go!\n\n\nCode\n# First, fit a model for the treatment (in this case our water gun)\n\nps.mod &lt;- glm(sprayed ~ eyes + group, \n              family = binomial(link = \"logit\"), \n              data = df)\n\n# Use \n\ndf.ps &lt;- df %&gt;% \n  dplyr::mutate(\n    ps = predict(ps.mod, type = \"response\"), \n    weights_ate = dplyr::case_when(\n      \n      # Note: for simplicity using unstabilized weights \n      \n      sprayed == 1 ~ 1/ps, \n      sprayed == 0 ~ 1/(1-ps)\n    )\n  )\n\noutcome.mod &lt;- glm(height ~ as.character(sprayed),\n                   family = gaussian(link = \"identity\"),\n                   weights = weights_ate, \n                   data = df.ps)\n\nsandwich_se &lt;- diag(sandwich::vcovHC(outcome.mod, type = \"HC\"))^0.5\n\noutcome &lt;- broom::tidy(outcome.mod) |&gt; \n  dplyr::filter(\n    term == \"as.character(sprayed)1\"\n  ) |&gt; \n  dplyr::mutate(\n   upper.ci = estimate + qnorm(0.975)*sandwich_se[2],\n   lower.ci = estimate - qnorm(0.975)*sandwich_se[2], \n  )\n\npaste0(\"Average Height Difference: \", round(outcome$estimate, 3), \" 95% CI (\", round(outcome$lower.ci,3), \" to \", round(outcome$upper.ci, 3), \")\")\n\n\n[1] \"Average Height Difference: 0.022 95% CI (-0.109 to 0.154)\"\n\n\nTa da! Now we know that being sprayed with a water gun doesn’t affect the height in our ATE population. But what about if not everyone is eligible for the treatment?\n\n\nATT\nNow we ask, would the treatment only be given to patients with certain characteristics? If yes, then the ATT might be of interest (Desai and Franklin 2019). This is also useful when considering the harmful effects of a treatment (Greifer and Stuart 2021). For our example, we want to know what the effects of the water gun are in those receiving it. If it doesn’t work in the people getting sprayed, then why are we spraying them? Would we be better off not soaking them? In mathematical notation:\n\\[E[Y_{1}-Y_{0}|T = 1]\\]\nIn term of weights, all of the patients in the treated group get a value of \\(1\\) while those in the control get a value of \\(PS / (1-PS)\\). These are known as standardized mortality ratio weights.\n\n\nCode\ndf.ps &lt;- df %&gt;% \n  dplyr::mutate(\n    ps = predict(ps.mod, type = \"response\"), \n    weights_att = dplyr::case_when(\n      \n      # Note: for simplicity using unstabilized weights \n      \n      sprayed == 1 ~ 1, \n      sprayed == 0 ~ ps/(1-ps)\n    )\n  )\n\noutcome.mod &lt;- glm(height ~ as.character(sprayed),\n                   family = gaussian(link = \"identity\"),\n                   weights = weights_att, \n                   data = df.ps)\n\nsandwich_se &lt;- diag(sandwich::vcovHC(outcome.mod, type = \"HC\"))^0.5\n\noutcome &lt;- broom::tidy(outcome.mod) |&gt; \n  dplyr::filter(\n    term == \"as.character(sprayed)1\"\n  ) |&gt; \n  dplyr::mutate(\n   upper.ci = estimate + qnorm(0.975)*sandwich_se[2],\n   lower.ci = estimate - qnorm(0.975)*sandwich_se[2], \n  )\n\npaste0(\"Average Height Difference: \", round(outcome$estimate, 3), \" 95% CI (\", round(outcome$lower.ci,3), \" to \", round(outcome$upper.ci, 3), \")\")\n\n\n[1] \"Average Height Difference: 0.022 95% CI (-0.11 to 0.154)\"\n\n\nOur results are similar but not the same as those for the ATE! In this case they are close but often that isn’t true.\n\n\nATU\nThe ATU is similar to the ATT except opposite (okay admittedly that sentence doesn’t make a lot of sense). The ATU is asking how would the height be different if the monsters and kids who didn’t get sprayed, got sprayed? In mathematical notation:\n\\[E[Y_{1}-Y_{0}|T = 0]\\]\nHow do we calculate the ATU? You guessed it! The opposite of the ATT…well kind of. It’s \\(1\\) for those in the control group and \\((1-PS)/PS\\) for the treated group.\n\n\nCode\ndf.ps &lt;- df |&gt; \n  dplyr::mutate(\n    ps = predict(ps.mod, type = \"response\"), \n    weights_atu = dplyr::case_when(\n      \n      # Note: for simplicity using unstabilized weights \n      \n      sprayed == 1 ~ (1-ps)/ps, \n      sprayed == 0 ~ 1\n    )\n  )\n\noutcome.mod &lt;- glm(height ~ as.character(sprayed),\n                   family = gaussian(link = \"identity\"),\n                   weights = weights_atu, \n                   data = df.ps)\n\nsandwich_se &lt;- diag(sandwich::vcovHC(outcome.mod, type = \"HC\"))^0.5\n\noutcome &lt;- broom::tidy(outcome.mod) |&gt; \n  dplyr::filter(\n    term == \"as.character(sprayed)1\"\n  ) |&gt; \n  dplyr::mutate(\n   upper.ci = estimate + qnorm(0.975)*sandwich_se[2],\n   lower.ci = estimate - qnorm(0.975)*sandwich_se[2], \n  )\n\npaste0(\"Average Height Difference: \", round(outcome$estimate, 3), \" 95% CI (\", round(outcome$lower.ci,3), \" to \", round(outcome$upper.ci, 3), \")\")\n\n\n[1] \"Average Height Difference: 0.023 95% CI (-0.104 to 0.15)\"\n\n\n\n\nATO\nATO is the average effect of the treatment in the overlap. Those patients who it’s unclear if the benefits would outweigh the costs (Greifer and Stuart 2021). These patients are very similar to each other but less similar for patients who the treatment decisions are more “clear cut” so to say (Greifer and Stuart 2021). In mathematical notation:\n\\[E[Y_{1}-Y_{0}]\\]\nNow something you might notice here is that the equation is the same as above for the ATE. Why is that? Well it’s because we are still reweighing both the treated and control groups but it’s a different target population. Instead of it being “everyone” like above, it’s the people who are in the uncertain group. We weigh them as \\(PS\\) for the sprayed group and \\(1-PS\\) for the not sprayed group.\n\n\nCode\ndf.ps &lt;- df |&gt; \n  dplyr::mutate(\n    ps = predict(ps.mod, type = \"response\"), \n    weights_ato = dplyr::case_when(\n      \n      # Note: for simplicity using unstabilized weights \n      \n      sprayed == 1 ~ (1-ps), \n      sprayed == 0 ~ ps\n    )\n  )\n\noutcome.mod &lt;- glm(height ~ as.character(sprayed),\n                   family = gaussian(link = \"identity\"),\n                   weights = weights_ato, \n                   data = df.ps)\n\nsandwich_se &lt;- diag(sandwich::vcovHC(outcome.mod, type = \"HC\"))^0.5\n\noutcome &lt;- broom::tidy(outcome.mod) |&gt; \n  dplyr::filter(\n    term == \"as.character(sprayed)1\"\n  ) |&gt; \n  dplyr::mutate(\n   upper.ci = estimate + qnorm(0.975)*sandwich_se[2],\n   lower.ci = estimate - qnorm(0.975)*sandwich_se[2], \n  )\n\npaste0(\"Average Height Difference: \", round(outcome$estimate, 3), \" 95% CI (\", round(outcome$lower.ci,3), \" to \", round(outcome$upper.ci, 3), \")\")\n\n\n[1] \"Average Height Difference: 0.022 95% CI (-0.106 to 0.151)\""
  },
  {
    "objectID": "posts/causal-estimands/causal_estimands.html#choose-your-adjustment-technique-wisely",
    "href": "posts/causal-estimands/causal_estimands.html#choose-your-adjustment-technique-wisely",
    "title": "Who are we talking about?",
    "section": "Choose Your Adjustment Technique Wisely!",
    "text": "Choose Your Adjustment Technique Wisely!\nPicking the estimand of interest isn’t just important for the target population but also for the adjustment technique selected. Certain adjustment techniques can only target certain estimands (Greifer and Stuart 2021). This becomes even more important when you are comparing methods.\nFor example, IPW and PSM do not target the same estimand (Greifer and Stuart 2021). There is a great flowchart for the steps involved by Desai and Franklin (2019) when trying to decide on the technique. Additionally, Greifer and Stuart (2021) have a good table for selecting the technique based on the causal estimand."
  },
  {
    "objectID": "posts/causal-estimands/causal_estimands.html#next-steps",
    "href": "posts/causal-estimands/causal_estimands.html#next-steps",
    "title": "Who are we talking about?",
    "section": "Next Steps",
    "text": "Next Steps\nWhat’s next? Well know you can go out into the world and answer all your burning questions like “Does this magical water gun actually work?” and follow-up with “Who exactly does it work on?”"
  },
  {
    "objectID": "posts/simulating-data/sim_data.html",
    "href": "posts/simulating-data/sim_data.html",
    "title": "Mastering Statistics Through Make-Believe",
    "section": "",
    "text": "Simulating data wasn’t something that I was taught in school. I’ve learned it since graduating/during my PhD, in my pursuit of improving my stats knowledge (the more I learn the more I feel I don’t know, weird feeling). It’s been unbelievably useful.\nI wanted to write this post to help anyone else who isn’t familiar with simulating data. Before, I just want to give a few use cases where I’ve found simulating data helpful:\n\nShowing bias (confounder, collider, etc)\nUnderstanding how methods work\nComparing different methods (1:1 matching vs IPTW, etc)\nUnderstand data generating mechanisms\n\nEnough about how it’s helpful, how do we do it!\n\n\n\n\n\n\nR Code\n\n\n\nI primarily use R for my coding, so this post focuses on using R. However, the methodology apply to whatever program you use for analysis. If you prefer to use MS Excel, you can do this using Excel as well, although I’d suggest a different software.\nI’m more of a journeyman tradesman biostatistician. What I mean by that is that I’ve done a few statistics courses but don’t have a degree in it (my MSc and PhD are in clinical epidemiology).\n\nIf you do have a background in statistics, hopefully this is a good refresher. If you don’t, don’t worry! I don’t either, so hopefulyl"
  },
  {
    "objectID": "posts/simulating-data/sim_data.html#why-simulate-data",
    "href": "posts/simulating-data/sim_data.html#why-simulate-data",
    "title": "Mastering Statistics Through Make-Believe",
    "section": "",
    "text": "Simulating data wasn’t something that I was taught in school. I’ve learned it since graduating/during my PhD, in my pursuit of improving my stats knowledge (the more I learn the more I feel I don’t know, weird feeling). It’s been unbelievably useful.\nI wanted to write this post to help anyone else who isn’t familiar with simulating data. Before, I just want to give a few use cases where I’ve found simulating data helpful:\n\nShowing bias (confounder, collider, etc)\nUnderstanding how methods work\nComparing different methods (1:1 matching vs IPTW, etc)\nUnderstand data generating mechanisms\n\nEnough about how it’s helpful, how do we do it!\n\n\n\n\n\n\nR Code\n\n\n\nI primarily use R for my coding, so this post focuses on using R. However, the methodology apply to whatever program you use for analysis. If you prefer to use MS Excel, you can do this using Excel as well, although I’d suggest a different software.\nI’m more of a journeyman tradesman biostatistician. What I mean by that is that I’ve done a few statistics courses but don’t have a degree in it (my MSc and PhD are in clinical epidemiology).\n\nIf you do have a background in statistics, hopefully this is a good refresher. If you don’t, don’t worry! I don’t either, so hopefulyl"
  },
  {
    "objectID": "posts/simulating-data/sim_data.html#how-to-simulate-data-in-r",
    "href": "posts/simulating-data/sim_data.html#how-to-simulate-data-in-r",
    "title": "Mastering Statistics Through Make-Believe",
    "section": "How to Simulate Data in R",
    "text": "How to Simulate Data in R\nBefore we dive into some R code, which I do love, it’s useful to first understand at a high level how this works. Before you scramble to close this, wait! I promise it isn’t going to be technical or boring….or at least I’ll try my best to not make it boring.\n\nFor simplicity, we’ll categorize outcomes as continuous, binary or time-to-event (TTE). This post won’t tackle TTE, but a future blog post will! Alright, let’s get started.\n\nContinuous Outcomes\nTo simulate data in R, we can use a family of functions that start with r. For example, if we want to simulate data from a normal distribution we can use the rnorm function. Let’s simulate an outcome and exposure that we will assess with a generalized linear model.\nLet’s use hours that a four month old slept as the outcome and number of pacifiers in their bed.\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(123) # we need to set a seed prior to simulating data. This allows us to replicate the data. For more details check out this blog post: [insert blog post about using different seeds for simulations]\n\nn.babies = 128 # note to get this value I used runif(1, min = 100, max = 500) then picked closest number divisible by 2\n\nnum_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)\nage_months = rnorm(n.babies, mean = 9, sd = 2)\nhours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months \n\ndf = data.frame(\n  num_pacifiers, \n  age_months,\n  hours_slept\n)\n\n\nOkay, so we have our data. Now let’s do something with it! Let’s try fitting a GLM\n\n\nCode\nmod = glm(hours_slept ~ num_pacifiers + age_months, \n          family = gaussian(), # we know this because we simulated the data. In reality, you have to use a combination of visualizing the data, understanding the data generating mechanism (aka what distribution it came from and the best model fit (i.e., Poisson vs negative binomial))\n          data = df \n          )\n\nsummary(mod)\n\n\n\nCall:\nglm(formula = hours_slept ~ num_pacifiers + age_months, family = gaussian(), \n    data = df)\n\nCoefficients:\n               Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)   4.000e+00  9.149e-16 4.372e+15   &lt;2e-16 ***\nnum_pacifiers 5.000e-01  6.223e-17 8.034e+15   &lt;2e-16 ***\nage_months    2.500e-01  8.405e-17 2.974e+15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 3.496232e-30)\n\n    Null deviance: 2.5793e+02  on 127  degrees of freedom\nResidual deviance: 4.3703e-28  on 125  degrees of freedom\nAIC: -8313.5\n\nNumber of Fisher Scoring iterations: 1\n\n\nNow, as you can see our estimates were pretty accurate. Let’s try this again, but this time we’ll assume that pacifiers doesn’t matter, it’s only the age. We can still adjust for both. Let’s try two models: one where we adjust for both variables and one where we adjust for only age.\n\n\nCode\nnum_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)\nage_months = rnorm(n.babies, mean = 9, sd = 2)\nhours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months \n\ndf = data.frame(\n  num_pacifiers, \n  age_months,\n  hours_slept\n)\n\n# Could alternatively try adjusting for the wrong variable (i.e., pacifiers but not age)\n\nmod1 &lt;- glm(hours_slept ~ age_months, \n            family = gaussian(link = \"identity\"), \n            data = df)\n\nmod2 &lt;- glm(hours_slept ~ age_months + num_pacifiers, \n            family = gaussian(link = \"identity\"), \n            data = df)\n\nsummary(mod1)\n\n\n\nCall:\nglm(formula = hours_slept ~ age_months, family = gaussian(link = \"identity\"), \n    data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.85497    0.64176  12.240   &lt;2e-16 ***\nage_months   0.27997    0.06918   4.047    9e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.382045)\n\n    Null deviance: 339.15  on 127  degrees of freedom\nResidual deviance: 300.14  on 126  degrees of freedom\nAIC: 478.33\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\nsummary(mod2)\n\n\n\nCall:\nglm(formula = hours_slept ~ age_months + num_pacifiers, family = gaussian(link = \"identity\"), \n    data = df)\n\nCoefficients:\n               Estimate Std. Error   t value Pr(&gt;|t|)    \n(Intercept)   4.000e+00  4.115e-15 9.719e+14   &lt;2e-16 ***\nage_months    2.500e-01  3.915e-16 6.386e+14   &lt;2e-16 ***\nnum_pacifiers 5.000e-01  2.519e-16 1.985e+15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 7.615348e-29)\n\n    Null deviance: 3.3915e+02  on 127  degrees of freedom\nResidual deviance: 9.5192e-27  on 125  degrees of freedom\nAIC: -7919.1\n\nNumber of Fisher Scoring iterations: 1\n\n\nNow you see how the impact of adjusting for a variable that doesn’t affect the outcome, at least in this case. For our model where we adjust for only age, the result is . When we adjust for the “correct” variables , we end up with a coefficient of 0.25.\nThis is how you simulate a continuous outcome, but what about a binary outcome?\n\n\n\n\n\n\nContinuous Distributions distributions\n\n\n\nFor the above example, we used a normal distribution. However, this doesn’t need to be the case. If we are dealing with age we may want to use a uniform distribution (using runif) and specifying the minimum and maximum. For example, if we are thinking of a variable where it may not make sense to have a value below a certain value (i.e., age where people are between 18 and 65).\nFor simplicity here, and because a number of statistical methods assume normality (cough also the central limit theorem cough), we will use rnorm.\n\n\n\n\nBinary Outcomes\nSimulating a binary outcome is similar to simulating a continuous outcome except we need to put these variables in the probability argument. We can convert linear predictors to probabilities for the logistic distribution (since we’ll be fitting a logistic regression) using the plogis function.\n\n\nCode\n# Double check this example (using ChatGPT and another way/blog post/expert)\n\nn.parents = n.babies*1.5\n\ncoffee_consumption = rnorm(n = n.parents, mean = 3, sd = 0.5)\nhours_baby_slept = rnorm(n = n.parents, mean = 4, sd = 0.25)\ncold_room = rbinom(n = n.parents, size = 1, prob = 0.5)\n\nlinpred = 0.25*hours_baby_slept \n\nprob_tired = plogis(linpred)\n\n# linpred = 3 + 0.25*hours_baby_slept + 0.005*cold_room + 0.05*coffee_consumption\n# \n# prob_tired = plogis(linpred)\n  \ntired_parents = rbinom(n = n.parents, size = 1, prob = prob_tired)\n\ndf = data.frame(\n  cold_room,\n  coffee_consumption,\n  hours_baby_slept,\n  tired_parents\n)\n\n\nNow we can fit a logistic regression model!\n\n\nCode\nmod = glm(tired_parents ~ hours_baby_slept,\n          # tired_parents ~ coffee_consumption + hours_baby_slept + cold_room,\n          family = binomial(link = \"logit\"), \n          data = df)\n\nsummary(mod)\n\n\n\nCall:\nglm(formula = tired_parents ~ hours_baby_slept, family = binomial(link = \"logit\"), \n    data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)        0.1552     2.5939   0.060    0.952\nhours_baby_slept   0.2344     0.6437   0.364    0.716\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 215.94  on 191  degrees of freedom\nResidual deviance: 215.80  on 190  degrees of freedom\nAIC: 219.8\n\nNumber of Fisher Scoring iterations: 4\n\n\nBased on this, we can see that our model doesn’t give a great example but if we increase the sample size, what happens?\n\n\nCode\n# Double check this example (using ChatGPT and another way/blog post/expert)\n\nn.parents.large = n.parents*10\n\ncoffee_consumption = rnorm(n = n.parents.large, mean = 3, sd = 0.5)\nhours_baby_slept = rnorm(n = n.parents.large, mean = 4, sd = 0.25)\ncold_room = rbinom(n = n.parents.large, size = 1, prob = 0.5)\n\nlinpred = 0.25*hours_baby_slept \n\nprob_tired = plogis(linpred)\n\n# linpred = 3 + 0.25*hours_baby_slept + 0.005*cold_room + 0.05*coffee_consumption\n# \n# prob_tired = plogis(linpred)\n  \ntired_parents = rbinom(n = n.parents.large, size = 1, prob = prob_tired)\n\ndf = data.frame(\n  cold_room,\n  coffee_consumption,\n  hours_baby_slept,\n  tired_parents\n)\n\n\nmod = glm(tired_parents ~ hours_baby_slept,\n          # tired_parents ~ coffee_consumption + hours_baby_slept + cold_room,\n          family = binomial(link = \"logit\"), \n          data = df)\n\nsummary(mod)\n\n\n\nCall:\nglm(formula = tired_parents ~ hours_baby_slept, family = binomial(link = \"logit\"), \n    data = df)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)       -0.3543     0.8335  -0.425    0.671\nhours_baby_slept   0.3389     0.2087   1.624    0.104\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2236.9  on 1919  degrees of freedom\nResidual deviance: 2234.3  on 1918  degrees of freedom\nAIC: 2238.3\n\nNumber of Fisher Scoring iterations: 4\n\n\nSo if we include 10 times the people, our result gets closer! If we include 100 times the people, it gets even closer.\n\n\n\n\n\n\nTTE Outcome\n\n\n\nAt this point, you may be expecting a section on simulating TTE outcome. This will be the topics of a future blog post, since it’s not quite as straight forward as continuous and binary outcomes.\n\n\nYou might be thinking “this is great and all, but how does this apply to causal inference?”"
  },
  {
    "objectID": "posts/simulating-data/sim_data.html#simulating-causal-concepts",
    "href": "posts/simulating-data/sim_data.html#simulating-causal-concepts",
    "title": "Mastering Statistics Through Make-Believe",
    "section": "Simulating Causal Concepts",
    "text": "Simulating Causal Concepts"
  },
  {
    "objectID": "posts/simulating-data/sim_data.html#causal-concepts",
    "href": "posts/simulating-data/sim_data.html#causal-concepts",
    "title": "Mastering Statistics Through Make-Believe",
    "section": "Causal Concepts",
    "text": "Causal Concepts\nNow, this is great and all but this blog is about causal inference! So let’s incorporate some into this post shall we? Instead of looking at GLMs, let’s demonstrate how confounding and colliders can introduce bias. We’ll calculate bias as (Morris, White, and Crowther 2019):\n\\[\nE[\\hat{\\theta}] - \\theta\n\\]\nwhere \\(E[\\hat{\\theta]}\\) is the expected, or average, estimated value and \\(\\theta\\) is the “actual” value. I’ll explain why we use the average later on in the Over and Over and Over Again section [TRY TO ADD CROSS REFERENCE].\n\n\n\n\n\n\nCausal Estimand\n\n\n\nWe need to make sure we are looking at the same causal estimand when comparing methods. For example, if we want to compare PSM to IPTW that would result in different answer…because they target different estimands!"
  },
  {
    "objectID": "posts/simulating-data/sim_data.html#showing-confounding",
    "href": "posts/simulating-data/sim_data.html#showing-confounding",
    "title": "Mastering Statistics Through Make-Believe",
    "section": "Showing Confounding",
    "text": "Showing Confounding\nLet’s assume that we want to demonstrate how confounding works. I’m an avid coffee lover, so let’s use an example with coffee! Suppose that our research question is “Does consuming coffee cause you to be happy?” We can start by drawing a DAG with our three variables: happy, coffee and sleep.\n\n\nCode\nlibrary(ggdag)\n\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nCode\ntheme_set(theme_dag())\n\ncoffee_dag &lt;- ggdag::dagify(\n  happy ~ coffee + sleep,\n  coffee ~ sleep,\n  exposure = \"coffee\",\n  outcome = \"happy\",\n  labels = c(\n    coffee = \"Coffee\",\n    happy = \"Happiness\",\n    sleep = \"Sleep\"\n  )\n)\n\nggdag::ggdag(coffee_dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\nNow we have our DAG, let’s get to simulating some data!\n\n\nCode\n# Look into Matthew Fox article for simulating data \n\n# Simulating Data, from ChatGPT\n\nset.seed(123) # Setting a seed for reproducibility\n\n# Number of observations\nn &lt;- 1000\n\n# Simulating sleep hours (normal distribution with mean=7 and sd=1.5)\nsleep_hours &lt;- rnorm(n, mean = 7, sd = 1.5)\n\n# Simulating coffee consumption based on sleep (negative correlation: less sleep -&gt; more coffee)\ncoffee_consumption &lt;- 5 - 0.5 * sleep_hours + rnorm(n, mean = 0, sd = 1)\n\n# Simulating happiness based on both sleep (positive correlation: more sleep -&gt; more happiness)\n# and coffee consumption (positive correlation: more coffee -&gt; more happiness)\nhappiness &lt;- rbinom(n = n, size = 1, prob = plogis(0.3 * sleep_hours + 0.2 * coffee_consumption + rnorm(n, mean = 0, sd = 1)))\n\n# Creating a data frame to hold the variables\ndf &lt;- data.frame(sleep_hours, coffee_consumption, happiness)\n\n# Displaying the first few rows of the data\nhead(df)\n\n\n  sleep_hours coffee_consumption happiness\n1    6.159287          0.9245580         1\n2    6.654734          0.6326781         1\n3    9.338062          0.3129885         1\n4    7.105763          1.3149436         1\n5    7.193932         -1.1463086         1\n6    9.572597          1.2542747         1\n\n\nNow we have our data, let’s show some confounding. How? We’ll fit two models: 1) not adjusting for the confounder, 2) adjusting for the confounder.\n\n\nCode\nmod1 &lt;- glm(formula = happiness ~ coffee_consumption,\n            family = binomial(link = \"logit\"),\n            data = df)\n\nmod2 &lt;- glm(formula = happiness ~ coffee_consumption + sleep_hours,\n            family = binomial(link = \"logit\"),\n            data = df)\n\nbias1 = coef(mod1)[2] - 0.2\nbias2 = coef(mod2)[2] - 0.2\n\n\nWe can now calculate bias for each of these models. As we can see, the bias from model one (-0.1397736) is more than the bias from model two (0.0277288). But how can we trust this? We only did it once. What if a sample of the same size (N = 250) gave a different answer? To account for this, we need to repeat this multiple times. So, let’s do that! Let’s repeat it a thousand times\n\n\nCode\nset.seed(123) # Setting a seed for reproducibility\n\nsimulation &lt;- function() {\n  # Number of observations\n  n &lt;- 250\n  \n  # Simulating sleep hours (normal distribution with mean=7 and sd=1.5)\n  sleep_hours &lt;- rnorm(n, mean = 7, sd = 1.5)\n  \n  # Simulating coffee consumption based on sleep (negative correlation: less sleep -&gt; more coffee)\n  coffee_consumption &lt;- 5 - 0.5 * sleep_hours + rnorm(n, mean = 0, sd = 1)\n  \n  # Simulating happiness based on both sleep (positive correlation: more sleep -&gt; more happiness)\n  # and coffee consumption (positive correlation: more coffee -&gt; more happiness)\n  happiness &lt;- rbinom(n = n, size = 1, prob = plogis(0.3 * sleep_hours + 0.2 * coffee_consumption + rnorm(n, mean = 0, sd = 1)))\n  \n  # Creating a data frame to hold the variables\n  df &lt;- data.frame(sleep_hours, coffee_consumption, happiness)\n  \n  # Building the models\n  mod1 &lt;- glm(formula = happiness ~ coffee_consumption,\n              family = binomial(link = \"logit\"),\n              data = df)\n  \n  mod2 &lt;- glm(formula = happiness ~ coffee_consumption + sleep_hours,\n              family = binomial(link = \"logit\"),\n              data = df)\n  \n  # Calculating the biases\n  coef1 &lt;- coef(mod1)[2] \n  coef2 &lt;- coef(mod2)[2] \n  \n  coef_results &lt;- data.frame(\n    coef1, coef2\n  )\n  \n  # Returning the biases as a named vector\n  return(\n    coef_results\n  )\n}\n\n# Replicating the simulation 1000 times\noutput_list &lt;- replicate(n = 1000, expr = simulation(), simplify = FALSE) \n\n# Bind all data frames in the list into a single data frame\ndf.out &lt;- do.call(rbind, output_list)\n\nbias1 = mean(df.out$coef1) - 0.2\nbias2 = mean(df.out$coef2) - 0.2\n\n\nNow we can look at\n\n\n\n\n\n\nPicking a number of simulations\n\n\n\nFor our example we picked 1000 repetitions but where did this number come from? Truthfully, it was completely arbitrary. In practice, we need to carefully choose how many we need. I highly suggest Morris, White, and Crowther (2019) as a reference for more information if you need to pick the number of simulations."
  },
  {
    "objectID": "posts/standardization/standardization.html",
    "href": "posts/standardization/standardization.html",
    "title": "Standardize Your Way to Causal Inference",
    "section": "",
    "text": "Imagine that a teacher wants to know if giving a lesson about growing plants causes plants to grow longer. They decide to use two groups of peoples: kids (group A) and their parents (group O). The kids (Group A) received a lesson about growing plants, while the parents (group O) did not. After receiving the lesson, each person got a plant and grew it. After 12 weeks, the plants length was measured.\n\nWhen the teacher looks at the plants grown, they conclude that group O are better at growing plants. That’s when a kid runs up huffing and puffing “You can’t compare those! That’s like comparing apples and oranges” when another kid chimes in “If we didn’t water them as much then we’d have done way better than them!”\n\nNow they have a point, watering a plant will certainly have an impact on how the plants grow, as will the sunlight, soil and other important factors. “So what do we do? Throw out this experiment?” the teacher asks the parent. “Not so fast! We can standardize these and then compare the groups!” you exclaim. So how exactly do we standardize to compare Groups A and O?\n\n\nAlright so we have our question, “Does getting a lesson in plant growing cause your plants to grow longer?”, but how exactly do we apply that in this case? Well first we need to have a look at the data and see what we have.\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(1513) # picked based on runif(1, min = 0, max = 2023)\n\ngrp_a &lt;- data.frame(\n  age = rnorm(n = 116, mean = 10, sd = 3),\n  indoors = rbinom(n = 116, size = 1, prob = 0.3), \n  hrs_sunlight = runif(n = 116, min = 2, max = 6), \n  watered = rbinom(n = 116, size = 1, prob = 0.4),\n  group = 1\n) \n\ngrp_o &lt;- data.frame(\n  age = rnorm(n = 116, mean = 30, sd = 7),\n  indoors = rbinom(n = 116, size = 1, prob = 0.8), \n  hrs_sunlight = runif(n = 116, min = 8, max = 12), \n  watered = rbinom(n = 116, size = 1, prob = 0.6),\n  group = 0\n) \n\ndf = rbind(grp_a, grp_o) |&gt; \n  dplyr::mutate(\n    plant_length = rnorm(n = 232, mean = 1.5*group + 0.2*age + 0.3*indoors + 0.8*hrs_sunlight - 0.5*watered, sd = 1)\n  ) \n\n\n\n\n\n\n\n\n\n\nCharacteristic\nGroup A\n(n = 116)\nGroup O\n(n = 116)\n\n\n\n\nAge, Mean (SD)\n10 (2.74)\n30.04 (6.12)\n\n\nIndoors, n (%)\n34, (30.2%)\n99 (85.3%)\n\n\nHours in Sunlight, Mean (SD)\n3.77 (1.23)\n10.02 (1.18)\n\n\nAdequately watered, n (%)\n46, (39.7%)\n59, (50.9%)\n\n\nLength of plant, Mean (SD)\n6.52 (1.58)\n14.2 (1.89)\n\n\n\nAlright so now we have our data and know the age of each person, how many plants were grown indoors in each group, the hours in the sunlight of each plant, whether the plant was adequately watered and the length of each plant. So what? Well we need to standardize these groups.\n\nYou may be thinking of standardization in terms of other methods, that are not necessarily statistical per se. For example, say you weighed three rocks: one is 110 kilograms, one is 167 pounds and one is 25 stones. How could you possibly compare these three? They are in different units! You may be thinking to yourself “well, we just convert them all to the same units duh”…exactly! You are standardizing the units. In this case we are doing something similar."
  },
  {
    "objectID": "posts/standardization/standardization.html#comparing-apples-and-oranges",
    "href": "posts/standardization/standardization.html#comparing-apples-and-oranges",
    "title": "Standardize Your Way to Causal Inference",
    "section": "",
    "text": "Imagine that a teacher wants to know if giving a lesson about growing plants causes plants to grow longer. They decide to use two groups of peoples: kids (group A) and their parents (group O). The kids (Group A) received a lesson about growing plants, while the parents (group O) did not. After receiving the lesson, each person got a plant and grew it. After 12 weeks, the plants length was measured.\n\nWhen the teacher looks at the plants grown, they conclude that group O are better at growing plants. That’s when a kid runs up huffing and puffing “You can’t compare those! That’s like comparing apples and oranges” when another kid chimes in “If we didn’t water them as much then we’d have done way better than them!”\n\nNow they have a point, watering a plant will certainly have an impact on how the plants grow, as will the sunlight, soil and other important factors. “So what do we do? Throw out this experiment?” the teacher asks the parent. “Not so fast! We can standardize these and then compare the groups!” you exclaim. So how exactly do we standardize to compare Groups A and O?\n\n\nAlright so we have our question, “Does getting a lesson in plant growing cause your plants to grow longer?”, but how exactly do we apply that in this case? Well first we need to have a look at the data and see what we have.\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(1513) # picked based on runif(1, min = 0, max = 2023)\n\ngrp_a &lt;- data.frame(\n  age = rnorm(n = 116, mean = 10, sd = 3),\n  indoors = rbinom(n = 116, size = 1, prob = 0.3), \n  hrs_sunlight = runif(n = 116, min = 2, max = 6), \n  watered = rbinom(n = 116, size = 1, prob = 0.4),\n  group = 1\n) \n\ngrp_o &lt;- data.frame(\n  age = rnorm(n = 116, mean = 30, sd = 7),\n  indoors = rbinom(n = 116, size = 1, prob = 0.8), \n  hrs_sunlight = runif(n = 116, min = 8, max = 12), \n  watered = rbinom(n = 116, size = 1, prob = 0.6),\n  group = 0\n) \n\ndf = rbind(grp_a, grp_o) |&gt; \n  dplyr::mutate(\n    plant_length = rnorm(n = 232, mean = 1.5*group + 0.2*age + 0.3*indoors + 0.8*hrs_sunlight - 0.5*watered, sd = 1)\n  ) \n\n\n\n\n\n\n\n\n\n\nCharacteristic\nGroup A\n(n = 116)\nGroup O\n(n = 116)\n\n\n\n\nAge, Mean (SD)\n10 (2.74)\n30.04 (6.12)\n\n\nIndoors, n (%)\n34, (30.2%)\n99 (85.3%)\n\n\nHours in Sunlight, Mean (SD)\n3.77 (1.23)\n10.02 (1.18)\n\n\nAdequately watered, n (%)\n46, (39.7%)\n59, (50.9%)\n\n\nLength of plant, Mean (SD)\n6.52 (1.58)\n14.2 (1.89)\n\n\n\nAlright so now we have our data and know the age of each person, how many plants were grown indoors in each group, the hours in the sunlight of each plant, whether the plant was adequately watered and the length of each plant. So what? Well we need to standardize these groups.\n\nYou may be thinking of standardization in terms of other methods, that are not necessarily statistical per se. For example, say you weighed three rocks: one is 110 kilograms, one is 167 pounds and one is 25 stones. How could you possibly compare these three? They are in different units! You may be thinking to yourself “well, we just convert them all to the same units duh”…exactly! You are standardizing the units. In this case we are doing something similar."
  },
  {
    "objectID": "posts/standardization/standardization.html#standardization-for-causal-inference",
    "href": "posts/standardization/standardization.html#standardization-for-causal-inference",
    "title": "Standardize Your Way to Causal Inference",
    "section": "Standardization for Causal Inference",
    "text": "Standardization for Causal Inference\nStandardization models the outcome, whereas inverse probability weighting (IPW), models the treatment (Hernan and Robins 2021). More formally, if we were to write it as an equation, assuming that no individuals are censored (C=0) (Hernan and Robins 2021, 162) :\n\\[\n{\\sum_{l}E[Y|A = a, C=0, L=l]}  \\times Pr[L=l]\n\\]\nNow, in an ideal world we’d be able to calculate this nonparametrically. To do that, we could calculate the mean outcome in each stratum \\(l\\) of the confounders \\(L\\). So in our case we could look at one strata as the individuals who are 5 years old, grew their plant indoor, had their plant in the sunlight for 2 hours and adequately watered their plant. We could do this for each strata, then take the weighted mean sum using the above formula.\n\n\n\n\n\n\nWhat if L is continuous?\n\n\n\nIf \\(L\\) is continuous in the above formula, then we need to replace \\(Pr[L=l]\\) with the probability density function \\(f_{L}[l]\\) instead (Hernan and Robins 2021, 162).\n\n\nNow, as you can imagine that is a lot of work especially when the variable is continuous. You can probably imagine that this isn’t always possible, especially when dealing with real-world data or many covariates. As the number of covariates increases, so does the number of strata. Luckily for us, we have handy dandy modelling in our statistical toolbox!"
  },
  {
    "objectID": "posts/standardization/standardization.html#standardizing-the-mean-outcome-to-the-confounder-distribution",
    "href": "posts/standardization/standardization.html#standardizing-the-mean-outcome-to-the-confounder-distribution",
    "title": "Standardize Your Way to Causal Inference",
    "section": "Standardizing the Mean Outcome to the Confounder Distribution",
    "text": "Standardizing the Mean Outcome to the Confounder Distribution\nBefore we get started, we need to go over the four steps. I’ll keep it brief here, but the steps are as follows (Hernan and Robins 2021, 164):\n\nExpansion of data set\nModelling\nPrediction\nStandardization by averaging\n\n\nExpansion of data set\nBefore starting any analysis, we first need data. Here we are going to create three data sets. The first will be the observed data, aka the original data set. We will then created another data set that is very similar to the first, except the group will be set to 0 (no lesson) and we will consider the outcome as missing. You can probably guess what the third data set will be….bingo! Same as the second data set but with group as 1 (lesson).\n\n\nCode\ndf_og &lt;- df\n\ndf_grp0 &lt;- df |&gt; \n  dplyr::select(\n    -plant_length, -group\n  ) |&gt; \n  dplyr::mutate(\n    group = 0\n  )\n\ndf_grp1 &lt;- df |&gt; \n  dplyr::select(\n    -plant_length, -group\n  ) |&gt; \n  dplyr::mutate(\n    group = 1\n  )\n\n\n\n\nModelling\nNow we can get to the fun stuff! Since our outcome is continuous, we decide to use a generalized linear model with the Gaussian distribution. We’ll only be able to use the original data set, since that is the one with the measured outcomes.\n\n\n\n\n\n\nModel Assumptions\n\n\n\nAs a side note, it is important to check the assumptions for the model you are using. For the sake of this post, I won’t since I don’t want to distract from the goal of explaining standardization but in practice should always check the assumptions for the model you are using (i.e., in this case a GLM).\n\n\n\n\nCode\nfit &lt;- glm(\n  plant_length ~ group + age + indoors + hrs_sunlight + watered, \n  family = gaussian(link = \"identity\"), \n  data = df\n)\n\n\n\n\nPrediction\nUsing our handy dandy model, we will now predict the outcome for both data sets with missing outcome data. First we predict the outcome for the data set if everyone were in group O. Next, we predict the outcome for the data set if everyone were in group A.\n\n\nCode\npred0 &lt;- predict(fit, df_grp0) |&gt; as.data.frame() |&gt; \n  rename(plant_length = `predict(fit, df_grp0)`)\n\npred1 &lt;- predict(fit, df_grp1) |&gt; as.data.frame() |&gt; \n  rename(plant_length = `predict(fit, df_grp1)`)\n\n\n\n\nStandardization by averaging\nNow that we have predicted the outcomes, we can calculate the average outcome for each data set. Before we calculate this, you might be asking about the uncertainty for this measurement, which I’m glad you asked! To get 95% confidence intervals for this, we can use bootstrapping.\n\n\n\nCode\n# A special thanks to Joy Shi, Sean McGrath and Tom Palmer for providing this code for free. Link here: https://remlapmot.github.io/cibookex-r/standardization-and-the-parametric-g-formula.html#program-13.4\n\nlibrary(boot)\n\n# Function, altered from the above link for our use \n\nstandardization &lt;- function(data, indices) {\n  \n  # We need to first make the datasets that we need\n  \n  # 1st copy: our original\n  \n  d &lt;- data[indices, ] \n  \n  # 2nd copy: Same as original but with group = 0 and outcome as missing\n  \n  d0 &lt;- d \n  d0$group &lt;- 0 # setting group to 0\n  d0$plant_length &lt;- NA # setting plant length (our outcome) to missing \n  \n  # 3rd copy: Same as original but with group = 1 and outcome as missing\n  \n  d1 &lt;- d \n  d1$group &lt;- 1 # setting group to 1\n  d1$plant_length &lt;- NA # setting plant length (our outcome) to missing \n  \n  # Making one sample\n  \n  d.onesample &lt;- rbind(d, d0, d1) # combining datasets\n  \n  # Fitting a model for each iteration\n  \n  fit &lt;- glm(\n    plant_length ~ group + age + indoors + hrs_sunlight + watered,\n    data = d.onesample\n  )\n  \n  # Using model to predict the outcome\n  \n  d.onesample$predicted_meanY &lt;- predict(fit, d.onesample)\n  \n  # Calculate the mean for each of the groups. The third calculation is for the difference in group O (0) vs group A (1)\n  \n  return(c(\n    mean(d.onesample$predicted_meanY[d.onesample$group == 0]),\n    mean(d.onesample$predicted_meanY[d.onesample$group == 1]),\n    # Treatment - No Treatment\n    \n    mean(d.onesample$predicted_meanY[d.onesample$group == 1]) -\n    mean(d.onesample$predicted_meanY[d.onesample$group == 0])\n  ))\n}\n\n\n# Now we can bootstrap this statistic using our dataset\n\nresults &lt;- boot(data = df,\n                statistic = standardization,\n                R = 5)\n\n# Using the bootstrapped sample (titled result), we can calculate the confidence intervals. We take the standard deviation of the sampling distribution. This will give us our standard error\n\nse &lt;- c(sd(results$t[, 1]),\n        sd(results$t[, 2]),\n        sd(results$t[,3]))\n\n\nmean &lt;- results$t0 # calculate mean \nll &lt;- mean - qnorm(0.975) * se # lower limits\nul &lt;- mean + qnorm(0.975) * se # upper limits\n\nfinalresults &lt;- data.frame(\n  result_title = c(\"Group O\", \"Group A\", \"Difference\"),\n  mean = round(mean,3),\n  ci = paste0(\"95% CI: \", round(ll, 3), \" - \", round(ul, 3))\n)\n\n\nFinally we have our results! We end up with a mean difference of -1.43 (95% CI: -0.96 to -1.90). Finally, we can put to rest that the people who received the lesson (the kids) grew shorter plants than those who didn’t have the lesson. Feeling smug, the parents start to gloat before one of the kids walks up and casually asks “Are these really valid? What kind of assumptions are you making?”"
  },
  {
    "objectID": "posts/standardization/standardization.html#assumptions-for-standardization",
    "href": "posts/standardization/standardization.html#assumptions-for-standardization",
    "title": "Standardize Your Way to Causal Inference",
    "section": "Assumptions for Standardization",
    "text": "Assumptions for Standardization\nThe kid brings up an excellent point, one that we should always keep in mind when trying to make a causal inference. So when are these estimates valid? We can group the assumptions into three groups: identifability conditions, measurement of variables and specification of the model (Hernan and Robins 2021, 168). The identifability conditions are exchangeability, positivity and consistency. Positivity can be checked empirically but the other two are opinion based. For a more detailed version of these, check out my other blog post “Intro to Causal Inference”.\nThe second condition is the variables used in the analysis need to be correctly measured. Measurement error in the treatment, outcome or the confounders will generally result in bias (see chapter 9 of Hernan and Robins (2021) for more specifics).\n\nFinally, all models need to be correctly specified (see chapter 11 of Hernan and Robins (2021) for more specifics). Of course, all of these will never hold perfectly but some remain a matter of judgement which means it can be open to criticism. It’s important to keep these assumptions in mind and the validity of them. For example, a critique in our example could be that our intervention is not well-defined (which I’d agree with and we could make our intervention definition more defined).\nWe need to make sure all of these conditions hold, at least in approximately since in the real-world it is difficult (i.e., in practice there is most likely some form of model misspecification). Some of these assumptions are based on judgement, which is important to note.\n\n\n\n\n\n\nPositivity Assumption\n\n\n\nThe positivity assumption is a key assumption for causal inference. Simply put, no individual can have a probability of 0 for the treatment. Now, for standardization, we can still use this method if this assumption isn’t met. However, we need to be willing to reply on extrapolation (parametric extrapolation to be exact) that will smooth over the strata for those with a probability of 0 of receiving treatment. If we do this, we’ll introduce bias, specifically the 95% CIs will cover the true effect less than 95% of the time. For more details, please see (Hernan and Robins 2021, 162)."
  },
  {
    "objectID": "posts/standardization/standardization.html#what-about-other-measures",
    "href": "posts/standardization/standardization.html#what-about-other-measures",
    "title": "Standardize Your Way to Causal Inference",
    "section": "What about other measures?",
    "text": "What about other measures?\nOur example used a continuous outcome, so we used risk difference as our causal estimand. Of course, there are other causal estimands of interest as well. I won’t bore you with another example here (maybe in the future). I’d highly recommend checking out Lee and Lee (2022) if you are interested in other estimands including relative risk and odds ratios. The process is very similar."
  },
  {
    "objectID": "posts/standardization/standardization.html#ip-weighting-or-standardization",
    "href": "posts/standardization/standardization.html#ip-weighting-or-standardization",
    "title": "Standardize Your Way to Causal Inference",
    "section": "IP Weighting or Standardization?",
    "text": "IP Weighting or Standardization?\nIf we were to do both without using any models (i.e., nonparametrically), then we would expect both methods to give the exact same result. This is because they are modelling different things (treatment for IPW, outcome for standardizaiton) and we can always expect some level of misspecification of a model in practice. So if they don’t give the same answer then how do we pick? The short answer is we don’t.\n\nLarge differences between them will let us know that there is some serious misspecificaiton in at least one of the estimates. Small differences may still indicate there’s a problem but not as serious misspecification.\n\nBasically what I’m trying to say is when both methods can be used, just use both."
  },
  {
    "objectID": "posts/standardization/standardization.html#the-g-formula",
    "href": "posts/standardization/standardization.html#the-g-formula",
    "title": "Standardize Your Way to Causal Inference",
    "section": "The G-Formula",
    "text": "The G-Formula\nEnough teasing already! What does the parametric g-formula have to do with this!! You may be screaming at your screen right now. If you are, I don’t blame you. Alright, so first the equation for the g-formula is formally expressed as (Naimi, Cole, and Kennedy 2017):\n\\[\nE(Y^{a_0, a_1}) = \\sum_{z_1}E(Y|A_1 = a_1, Z_1 = z1, A_0 = a_0)P(Z_1=z_1|A_0=a_0)\n\\]\nAt this point you may be wondering “What in the world does this formula have to do with standardization?”. If you are thinking that, props to you! (side note: I personally always like to ask these types of questions). The answer is this is how we calculate our expected values (E). The g-formula has a broad use, specifically for time-varying applications. In our case, we can just ignore \\(A_1\\) since that refers to the point in time (we only have one time point for our example).\n\nSince we already have our values, from standardization, we are going to plug-in those values to this formula (technically we already did this, but you get the point). We say that we are using a plug-in g-formula estimator since we just…you guessed it….plug in the values! Standardization is a plug-in g-formula estimator however since, in our case, the estimate came from a parametric model we call it the parametric g-formula."
  },
  {
    "objectID": "posts/standardization/standardization.html#what-next",
    "href": "posts/standardization/standardization.html#what-next",
    "title": "Standardize Your Way to Causal Inference",
    "section": "What Next?",
    "text": "What Next?\nNow you are free to go standardize away out there in the real world!"
  },
  {
    "objectID": "posts/welcome/welcome.html",
    "href": "posts/welcome/welcome.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Overview\nThis is my first post in this blog! The aim of this blog is to describe methods related to real-world evidence, causal inference and statistical fun. My goal for this is to elaborate on methods I’ve come across while reading and hopefully be able to teach one person something new, or in a more intuitive way. A huge credit to the Count Bayesie blog (@willkurt on twitter). Reading his blog posts encouraged me to do the same.\nPlease note: we are all human. Sometimes due to a lack of sleep, caffeine, or being distracted everyone makes mistakes (myself included). If you notice a mistake on this blog, please reach out to me so I can correct it."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html",
    "href": "posts/or_rr_hr/or_rr_hr.html",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "",
    "text": "Let’s start with probability because that’s probably a good place to start. Probability is basically how likely something is to occur. For example, what are the chances that my favorite show will come on TV today? Or how likely is it that it will rain today? Based upon these probabilities you can make informed decisions. The neat thing about probability, is that you can incorporate it into different measures when conducting scientific research. An example of this is the odds that something will happen, similar to how a betting line works when gambling.\n\nTo convert from probability to odds, the formula is the probability of something happening divided by the probability of that something not happening. In mathematical terms:\n\\[\nodds = p/(1-p)\n\\]\nAn example always helps to contextualize the principles. What are the odds that my favorite candy will be available at the movie theatre tonight? Well, if the probability (p) is 0.70 that it will be there tonight, then the odds are:\n\\[\nodds = (0.70) / (1-0.70) = 0.70/0.30 = 2.33\n\\]\nThis means that the odds are 2.33:1 that my candy will be there tonight. Hooray!\nOdds are useful, however often times in research we want to know how the ratio of these two odds can be compared. For example, are women more likely to have a heart attack than men? To answer this, we turn to odds ratios (OR)."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html#probability",
    "href": "posts/or_rr_hr/or_rr_hr.html#probability",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "",
    "text": "Let’s start with probability because that’s probably a good place to start. Probability is basically how likely something is to occur. For example, what are the chances that my favorite show will come on TV today? Or how likely is it that it will rain today? Based upon these probabilities you can make informed decisions. The neat thing about probability, is that you can incorporate it into different measures when conducting scientific research. An example of this is the odds that something will happen, similar to how a betting line works when gambling.\n\nTo convert from probability to odds, the formula is the probability of something happening divided by the probability of that something not happening. In mathematical terms:\n\\[\nodds = p/(1-p)\n\\]\nAn example always helps to contextualize the principles. What are the odds that my favorite candy will be available at the movie theatre tonight? Well, if the probability (p) is 0.70 that it will be there tonight, then the odds are:\n\\[\nodds = (0.70) / (1-0.70) = 0.70/0.30 = 2.33\n\\]\nThis means that the odds are 2.33:1 that my candy will be there tonight. Hooray!\nOdds are useful, however often times in research we want to know how the ratio of these two odds can be compared. For example, are women more likely to have a heart attack than men? To answer this, we turn to odds ratios (OR)."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html#odds-ratios",
    "href": "posts/or_rr_hr/or_rr_hr.html#odds-ratios",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "Odds Ratios",
    "text": "Odds Ratios\nOdds ratios essentially take two different odds, as explained above, and compared them to each other. For example, we have the below table (note: these numbers were simulated, not from an actual study). So the odds of men with a heart condition is 87/176 = 0.49, compared to women with a heart condition. If we do the same for the subgroup of patients with no heart condition: 101/147 = 0.69.\n\nNow, these odds are great but really we want to know, how do men and women compare? Well, we take the ratio of these two values. 0.49/0.69 = 0.71 and…dramatic pause… now you have an odds ratio!\n\n\nTable 1: Contingency Table for Heart Attack by Sex\n\n\n\nMen\nWomen\nTotal\n\n\nHeart Attack\n87\n176\n263\n\n\nNo Heart Attack\n101\n147\n248\n\n\nTotal\n188\n323\n511\n\n\n\n\nThe interpretation for this would be that men have a decreased odds of having a heart attack than women. If this is hard to interpret, personally I find it easier to contextualize ORs &gt; 1 to other people, you can take the reciprocal of the OR and flip what is called the reference group. So women have increased odds, 1.41, of having a heart attack (1 / 0.71 = 1.41)."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html#relative-risks",
    "href": "posts/or_rr_hr/or_rr_hr.html#relative-risks",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "Relative Risks",
    "text": "Relative Risks\nRelative risks (RR) are similar to odds ratios, however the way they are calculated are different. Using the above example, Table 1, we can calculate the relative risk. The formula for relative risk is\n\\[\nRR = (87/263) / (101/248) = 0.33/0.407 = 0.81\n\\]\nAgain, this means that men with a heart condition have a decreased risk compared to women. It is important to note here that the relative risk is specific to the study that was conducted. In our example, RR = 0.81 for the 511 people included in our study.\n\nThis is a very important distinction to make because people have varying degrees of baseline risk. As such, sometimes ORs and RRs are used interchangeably however they are distinctly different. In an upcoming post, I will elaborate on transportability."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html#hazard-ratios",
    "href": "posts/or_rr_hr/or_rr_hr.html#hazard-ratios",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "Hazard Ratios",
    "text": "Hazard Ratios\nHazard ratios are typically reported from time-to-event analyses, whereas odds ratios and relative risks are calculated from binary events: yes/no. Due to this, hazards cannot be explained quite as simple using a table, however that isn’t a problem.\nA hazard is the probability that an individual has an event at that time t Clark et al. (2003). Alternatively, it represents the instantaneous event rate for an individual. This is commonly used in survival analyses, which we will touch on in a later post, however it doesn’t have to be for survival (sometimes a common misconception). The hazard is for the event. Since it is a time-to-event measure, it could be used to determine the probability that my socks will instantaneously fall off. The typical notation for a hazard is \\(h(t)\\) or \\(\\lambda(t)\\)\nAs with the previous two ratios, it is a ratio. So if we use the same heart attack example, Table 1, except this time we are analyzing the time-to-heart attack then we can calculate a hazard for each group. Unfortunately, the math is not quite as straightforward so I’ll save you the pain. Using an appropriate model, a subsequent post will touch on models for time-to-event analysis but a common methods you may have heard of is Cox Proportional Hazards Model (note: if proportional hazards assumption is held, then hazard is cumulative Clark et al. (2003).\nUsing this imaginary model, we calculated the hazard for men \\(h(men)\\) as 0.20 and the hazard for women \\(h(women)\\) as 0.369. Now, we take the ratio of these two hazards and….*I’ll pause while you roll your eyes*….bingo! We have a hazard ratio (HR):\n\\[\nHR = h(men)/h(women) = 0.20/0.369 = 0.542.\n\\]\nThe interpretation for this is that men, yet again, have a lower probability of having an instantaneous heart attack than women (HR = 0.542)."
  },
  {
    "objectID": "posts/collapsible-effect-measure/collapsible_effects.html",
    "href": "posts/collapsible-effect-measure/collapsible_effects.html",
    "title": "Beyond the Odds",
    "section": "",
    "text": "Epidemiologists and clinical researchers use different metrics to quantify the relationship between the exposure and outcome. Commonly used measures include the risk ratio (RR), risk difference (RD), odds ratio (OR), mean difference (MD) and hazard ratio (HR). Each one of these has benefits and drawbacks, including the type of outcome (binary, continuous, time-to-event), how these can apply to other samples/populations (often referred to as transportability or portability) (Doi et al. 2022) and collapsibility. For this post, we’re going to focus on the last one."
  },
  {
    "objectID": "posts/collapsible-effect-measure/collapsible_effects.html#measures-of-effect",
    "href": "posts/collapsible-effect-measure/collapsible_effects.html#measures-of-effect",
    "title": "Beyond the Odds",
    "section": "",
    "text": "Epidemiologists and clinical researchers use different metrics to quantify the relationship between the exposure and outcome. Commonly used measures include the risk ratio (RR), risk difference (RD), odds ratio (OR), mean difference (MD) and hazard ratio (HR). Each one of these has benefits and drawbacks, including the type of outcome (binary, continuous, time-to-event), how these can apply to other samples/populations (often referred to as transportability or portability) (Doi et al. 2022) and collapsibility. For this post, we’re going to focus on the last one."
  },
  {
    "objectID": "posts/collapsible-effect-measure/collapsible_effects.html#example-time",
    "href": "posts/collapsible-effect-measure/collapsible_effects.html#example-time",
    "title": "Beyond the Odds",
    "section": "Example time!",
    "text": "Example time!\nRather than bore you with a bunch of jargon up front, it’s always better to work through an example first and then bore you with jargon (just kidding!….maybe…maybe not).\nLet’s say that we have two animal types (turtles and lions) and whether they caught a ball (yes/no). There is also sometimes a zookeeper that is around. Now this zookeeper can throw the animals a ball, which makes a difference on if they catch it or not. The first thing to do is to look at our fancy pants data we have. Let’s separate it by whether a zookeeper was present or not.\n\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(123)\n\nn = 500\n\nanimal_type &lt;- rbinom(n, size = 1, prob = 0.5) # 0 for turtle, 1 for lion\nzookeeper_present &lt;- rbinom(n, size = 1, prob = 0.5) # 0 for absent, 1 for present\nball_catch &lt;- rbinom(n, 1, plogis(-1 + 2*animal_type + zookeeper_present))\n\ndf &lt;- data.frame(animal_type, zookeeper_present, ball_catch)\n\n\n\nZookeeper Present\n\n\n\nBall Not Caught\nBall Caught\nTotal\n\n\n\n\nTurtle\n65\n70\n135\n\n\nLion\n13\n110\n123\n\n\nTotal\n78\n180\n258\n\n\n\n\nZookeeper Absent\n\n\n\nBall Not Caught\nBall Caught\nTotal\n\n\n\n\nTurtle\n95\n35\n130\n\n\nLion\n30\n82\n112\n\n\nTotal\n125\n117\n242\n\n\n\n\nOverall\n\n\n\nBall Not Caught\nBall Caught\nTotal\n\n\n\n\nTurtle\n160\n105\n265\n\n\nLion\n43\n192\n235\n\n\nTotal\n203\n197\n500\n\n\n\nNow let’s get down to some analysis. If we calculate the odds ratio for each of these tables, we’d find ORs of: 7.86 for when the zookeeper is present, 7.42 when the zookeeper is absent and 6.80 overall. You may be wondering where this difference comes from. Perhaps these are just different numbers and we should expect this? At first, you’d be tempted to think that it’s just a difference and perhaps if we average them it’ll fix it. However that’s not the case, the weighted average of 7.86 and 7.42 won’t give us 6.80. So what do we do? Maybe we’ll try a GLM."
  },
  {
    "objectID": "posts/collapsible-effect-measure/collapsible_effects.html#what-if-we-use-a-glm",
    "href": "posts/collapsible-effect-measure/collapsible_effects.html#what-if-we-use-a-glm",
    "title": "Beyond the Odds",
    "section": "What if we use a GLM?",
    "text": "What if we use a GLM?\nLet’s fit two different models. One will be unadjusted, to estimate the marginal effect while the other we will adjust for zookeeper type (aka include it in the model).\n\n\nCode\nfit1 &lt;- glm(ball_catch ~ animal_type, \n            family = binomial(link = \"logit\"), \n            data = df)\n\nfit2 &lt;- glm(ball_catch ~ animal_type + zookeeper_present, \n            family = binomial(link = \"logit\"),\n            data = df)\n\nround(exp(coef(fit1)['animal_type']), 2)\n\n\nanimal_type \n        6.8 \n\n\nCode\nround(exp(coef(fit2)['animal_type']), 2)\n\n\nanimal_type \n        7.6 \n\n\nAs you can see, the marginal effect is still 6.8 while the conditional effect is 7.6. “Gotta be confounding…or…some weird magical bias or…”. If you’re thinking that, relax. It’s a known “issue” (more on if it’s an issue later), that happens with odds ratios as a measure. We have simulated the data so we know it’s not due to confounding (zookeeper being present isn’t a confounder but is predictive of the outcome). What is happening here is something called non-collapsibility."
  },
  {
    "objectID": "posts/collapsible-effect-measure/collapsible_effects.html#what-exactly-do-we-mean-by-collapsible",
    "href": "posts/collapsible-effect-measure/collapsible_effects.html#what-exactly-do-we-mean-by-collapsible",
    "title": "Beyond the Odds",
    "section": "What exactly do we mean by collapsible?",
    "text": "What exactly do we mean by collapsible?\nGreenland and Pearl define collapsibility as “when an adjustment does not alter a measure, the measure is said to be collapsible over C or invariant with respect to the adjustment. Conversely, if an adjustment alters a measure the measure is said to be non-collapsible over C” (Greenland and Pearl 2011). So what exactly does this mean?\nWell it means that even when there is no confounding whether we include a covariate in our model matters for the magnitude of our treatment (if that covariate impacts the outcome) (Daniel, Zhang, and Farewell 2021). The aforementioned paper, illustrates a good way to tell if we should expect a measure to be collapsible or not. I won’t go into the mathematics behind this. Based on these magical mathematics, we know that odds ratios and hazard ratios are non-collapsible.\n\n\n\n\n\n\nMagical Mathematics\n\n\n\nIf you are actually interested in what I’m referring to, here’s the nitty gritty (keep in mind it’s a very, very brief summary). For generalized linear models, an important component is the link function. (Daniel, Zhang, and Farewell 2021) highlight five link functions: identity, log, logit, complementary log-log and probit. They use a function called the characteristic collapsibility function which can be used to explain the difference in collapsible vs noncollapsible measures.\n\n\\[\ng_\\nu(.) = f^{-1}{f(.) + \\nu}\n\\]\n\nUsing this, they demonstrate that by not allowing models to predict probabilities outside of [0,1] causes these bendy bits in the graph. As a result of this, there is noncollapsibility for ORs and HRs."
  },
  {
    "objectID": "posts/collapsible-effect-measure/collapsible_effects.html#so-whats-the-problem",
    "href": "posts/collapsible-effect-measure/collapsible_effects.html#so-whats-the-problem",
    "title": "Beyond the Odds",
    "section": "So what’s the problem?",
    "text": "So what’s the problem?\nSelecting the appropriate effect measure and the inclusion of variables in a model is an important part of any study. This noncollapsibility issue isn’t a be all end all. There’s a great blog post by Frank Harrell that goes into more detail if you’re interested (“Unadjusted Odds Ratios Are Conditional” 2020).\nBasically, as long as we know that it’s a fault of the OR then we can be aware of that. Everything in life has faults, but knowing this is an issue allows us to be careful of it. I work mostly with observational data, where not adjusting for covariates isn’t really an option whether it be through weighting, matching or outcome regression.\nWe just need to be mindful of comparing ORs across studies, especially if pooling them together. There are great resources of how to tackle this, or to convert from marginal to conditional ORs. Basically, to sum up it’s important to ask “what is being estimated?” including whether the measure is marginal or conditional."
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html",
    "href": "posts/p-value-power/p-value-power.html",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "P-values are a term commonly heard in scientific literature, however often they are misconstrued, misunderstood or misinterpreted. The p in p-value stands for probability, as a lot of statistics begins with. What exactly does it tell us? Let’s find out\n\n\n\n\nHypotheses are the best place to start for any science experiment or project. A hypothesis is what you are wanting to know. First you need to start with your research question. For example, are people who love lollipops older than people who do not? To do this, formally in statistics, we need a null hypothesis (\\(H_0\\)) and an alternative hypothesis (\\(H_A\\)). We will use two abbreviations here: LL for lollipop lickers and NLL for not lollipop lickers. We want to know “Do lollipop lickers have more sore tongues than those who do not lick lollipops?”\nRe-phrasing this in statistically terms, \\(H_0\\) is that people do not have more sore tongues if they lick lollipops compared to if they do. The question now is what is our alternative hypothesis? Well there are three options to choose from:\n\nWe think LL have more sore tongues than NLL\nWe think LL have less sore tongues than NLL\nWe don’t know, but we don’t think they are the same!\n\n1 and 2 are what are called one-sided hypothesis. A one-sided hypothesis, means you are assuming the difference is bigger or smaller. A two-sided hypothesis means we simply don’t know! We will assume we don’t know and use \\(p\\) to denote the proportion of lollipop sore tongues.\n\\[\nH_0: p_{LL-ST} = p_{NLL-ST}\n\\]\n\\[\nH_A: p_{LL-ST} \\neq p_{NLL-ST}\n\\]\nNow, before we continue we need to review how okay we are with being wrong. Being wrong is okay, but there are two different types of being wrong.\n\n\n\nA type I error, sometimes referred to as \\(\\alpha\\) error, is the error of saying something happened when it did not. For example, the error of me assuming you bought a balloon when you did not. A type II error, sometimes referred to as \\(\\beta\\) error, is the error of saying something did not happen when it did. Using our balloon example, me assuming you didn’t buy a balloon when you did.\n\n\n\nThe power of a statistical test is the probability that it correctly rejects the null hypothesis (Gelman and Carlin 2014). It is tied to the type II error, through the below formula.\n\\[\nPower = 1 - \\beta\n\\]\nRemember our lollipop example? For that it would be the probability that we can say lollipop lickers do not have the same amount of sore tongues as non lollipop lickers.\n\n\n\nAlright, I know at this point I’m boring you. One last definition and then we can get to the good stuff. A test statistic is exactly what it sounds like. A statistic used for tests! What kind of tests you may ask? Hypothesis tests like we have above. Now, the test statistic in itself may follow a variety of distributions depending on the test. I’ll leave distributions to another post.\n\n\n\nThe probability of such an extreme value of the test statistic occurring if the null hypothesis were true is often called the P-Value (Bland 2015).\n\n\n\nRemember \\(\\alpha\\) from before? The type I error is sometimes referred to as the significance level. The number that is picked for the type I error is usually 0.05, however it is completely arbitrary to pick 0.05. It could be 0.20 or 0.02.\n\n\n\n\n\n\nImportant\n\n\n\nThere is no such thing as more significant. It is either significant at the level that was pre-specified before the analysis or not. For example: a p-value of 0.01 is not more significant than a p-value of 0.05\n\n\n\n\n\n\nFinally, after all those boring definitions we are back to where we started. How are we going to determine whether people who are lollipop lickers have more sore tongues? This requires us to do a bit more thinking to determine the right statistical tool for the toolbox.\nLet’s think about this question a bit more first. Lollipop lickers tend to be younger right? Do boys like lollipops more than girls? Let’s assume that both of those things are true. We will want to control for both of those variables. Control in this sense, means adding the variable in the equation like below.\n\\[\nST = LL + age + sex\n\\]\nNow, the type of regression we will use is a generalized linear model (GLM). Generalized here just means that you can apply to almost any situation, hence general. Another post will cover the differences between GLMs and regular old regression (OLS).\n\n\n\nNow we know our question, have our tool and want to find out the answer. What are we missing…..the most important thing of all! Data! Luckily, there is a database that collects such data. It has data on 422 people. Below are the summary statistics for our groups\n\n\n\n\n\n\n\n\n\nSore Tongues\n(n = 181)\nNot Sore Tongues\n(n = 241)\n\n\n\n\nLollipop Lickers, n (%)\n146 (80.7%)\n60 (24.9%)\n\n\nAge, mean (SD)\n17.9 (7.30)\n48.0 (17.8)\n\n\nFemale, n (%)\n107 (59.1%)\n52 (21.6%)\n\n\n\n\n\n\nUsing our handy dandy formula from above, and our tool from our toolbox (GLM). Below are the results.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nSE\nStatistic\n(Z-Value)\nPr(&gt;|z|)\n(P-Value)\n\n\n\n\n(Intercept)\n6.31\n1.20\n5.25\n1.53e-7\n\n\nLL\n2.77\n0.484\n5.73\n1.00e-8\n\n\nAge\n-0.317\n0.0499\n-6.36\n2.04e-10\n\n\nSex\n2.15\n0.493\n4.36\n1.32e-5\n\n\n\n\n\n\nThe test statistic for regression is defined by \\(\\beta / SE(\\beta)\\). Beta here is the coefficient. For example, the test statistic for age is (note, close to -6.36. We will chalk up the difference to a rounding error):\n\\[\n-0.317/0.0499 = -6.35\n\\]\nNow, this number on it’s own is pretty useless. We want to know, what is our p-value?! Well, to do that we turn to the normal distribution.\n\n\n\nUsing the normal distribution, since our sample size is bigger than 30 (a wonderfully random number. Honestly, I have no idea why 30 is the cutoff), we can determine our almighty p-value.\n\n\nCode\n2*pnorm(-6.358)\n\n\n[1] 2.043975e-10\n\n\nTa-da! We have our p-value of 2.04e-10. This is less than our pre-specified value of 0.05. What does it mean though? From earlier, it is the probability of such an extreme value of the test statistic (-6.35) occurring if the null hypothesis were true. Finally, we can use significant in our interpretation right? Technically yes, however there are some strong caveats we need to go over first\n\n\n\nP-values are typically over emphasized. They are a piece of the puzzle, however they are not the whole puzzle. Arguably, a more important piece of the puzzle is the effect size. For our example, lollipop lickers have increased odds of a sore tongue compared to non-lollipop lickers (OR = 5.87, 95% CI: 2.53 to 13.87). The effect, in this case OR, should be another piece of the puzzle that is considered. Finally, there are two other errors we should go over that highlight how not confident we can be in the p-value.\n\n\nOften type I and type II errors are highlighted however type M and type S errors are important to note as well. A type S error is the probability of an estimate being in the wrong direction (Gelman and Carlin 2014). A type M error is the factor by which the magnitude of an effect might be overestimated (Gelman and Carlin 2014). Definitions are always great but an example helps to better understand.\nUsing our example from before, and the function outlined in Gelman and Carlin (2014):\n\n\nCode\nretrodesign &lt;- function(A, s, alpha=.05, df=Inf, n.sims=10000){\n  z &lt;- qt(1-alpha/2, df)\n  p.hi &lt;- 1 - pt(z-A/s, df)\n  p.lo &lt;- pt(-z-A/s, df)\n  power &lt;- p.hi + p.lo\n  typeS &lt;- p.lo/power\n  estimate &lt;- A + s*rt(n.sims,df)\n  significant &lt;- abs(estimate) &gt; s*z\n  exaggeration &lt;- mean(abs(estimate)[significant])/A\n  return(list(power=power, typeS=typeS, exaggeration=exaggeration))\n}\n\nretrodesign(-0.31703, 0.04986)\n\n\nThe type S error is 0.99, while the type M error is -0.997. This means that there is a 99% probability that the estimate is the wrong sign, and that the value is slightly underestimated (0.997 of what it should be). Doesn’t sound good right?\n\n\n\n\nP-values have a place in science, however they should be interpreted with caution and as a piece to the puzzle. Other pieces include the effect size, study design, sample that was studied and a myriad of other components."
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html#formal-definitions",
    "href": "posts/p-value-power/p-value-power.html#formal-definitions",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "Hypotheses are the best place to start for any science experiment or project. A hypothesis is what you are wanting to know. First you need to start with your research question. For example, are people who love lollipops older than people who do not? To do this, formally in statistics, we need a null hypothesis (\\(H_0\\)) and an alternative hypothesis (\\(H_A\\)). We will use two abbreviations here: LL for lollipop lickers and NLL for not lollipop lickers. We want to know “Do lollipop lickers have more sore tongues than those who do not lick lollipops?”\nRe-phrasing this in statistically terms, \\(H_0\\) is that people do not have more sore tongues if they lick lollipops compared to if they do. The question now is what is our alternative hypothesis? Well there are three options to choose from:\n\nWe think LL have more sore tongues than NLL\nWe think LL have less sore tongues than NLL\nWe don’t know, but we don’t think they are the same!\n\n1 and 2 are what are called one-sided hypothesis. A one-sided hypothesis, means you are assuming the difference is bigger or smaller. A two-sided hypothesis means we simply don’t know! We will assume we don’t know and use \\(p\\) to denote the proportion of lollipop sore tongues.\n\\[\nH_0: p_{LL-ST} = p_{NLL-ST}\n\\]\n\\[\nH_A: p_{LL-ST} \\neq p_{NLL-ST}\n\\]\nNow, before we continue we need to review how okay we are with being wrong. Being wrong is okay, but there are two different types of being wrong.\n\n\n\nA type I error, sometimes referred to as \\(\\alpha\\) error, is the error of saying something happened when it did not. For example, the error of me assuming you bought a balloon when you did not. A type II error, sometimes referred to as \\(\\beta\\) error, is the error of saying something did not happen when it did. Using our balloon example, me assuming you didn’t buy a balloon when you did.\n\n\n\nThe power of a statistical test is the probability that it correctly rejects the null hypothesis (Gelman and Carlin 2014). It is tied to the type II error, through the below formula.\n\\[\nPower = 1 - \\beta\n\\]\nRemember our lollipop example? For that it would be the probability that we can say lollipop lickers do not have the same amount of sore tongues as non lollipop lickers.\n\n\n\nAlright, I know at this point I’m boring you. One last definition and then we can get to the good stuff. A test statistic is exactly what it sounds like. A statistic used for tests! What kind of tests you may ask? Hypothesis tests like we have above. Now, the test statistic in itself may follow a variety of distributions depending on the test. I’ll leave distributions to another post.\n\n\n\nThe probability of such an extreme value of the test statistic occurring if the null hypothesis were true is often called the P-Value (Bland 2015).\n\n\n\nRemember \\(\\alpha\\) from before? The type I error is sometimes referred to as the significance level. The number that is picked for the type I error is usually 0.05, however it is completely arbitrary to pick 0.05. It could be 0.20 or 0.02.\n\n\n\n\n\n\nImportant\n\n\n\nThere is no such thing as more significant. It is either significant at the level that was pre-specified before the analysis or not. For example: a p-value of 0.01 is not more significant than a p-value of 0.05"
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html#sec-problem",
    "href": "posts/p-value-power/p-value-power.html#sec-problem",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "Finally, after all those boring definitions we are back to where we started. How are we going to determine whether people who are lollipop lickers have more sore tongues? This requires us to do a bit more thinking to determine the right statistical tool for the toolbox.\nLet’s think about this question a bit more first. Lollipop lickers tend to be younger right? Do boys like lollipops more than girls? Let’s assume that both of those things are true. We will want to control for both of those variables. Control in this sense, means adding the variable in the equation like below.\n\\[\nST = LL + age + sex\n\\]\nNow, the type of regression we will use is a generalized linear model (GLM). Generalized here just means that you can apply to almost any situation, hence general. Another post will cover the differences between GLMs and regular old regression (OLS)."
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html#data-on-lollipop-lickers-and-sore-tongues",
    "href": "posts/p-value-power/p-value-power.html#data-on-lollipop-lickers-and-sore-tongues",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "Now we know our question, have our tool and want to find out the answer. What are we missing…..the most important thing of all! Data! Luckily, there is a database that collects such data. It has data on 422 people. Below are the summary statistics for our groups\n\n\n\n\n\n\n\n\n\nSore Tongues\n(n = 181)\nNot Sore Tongues\n(n = 241)\n\n\n\n\nLollipop Lickers, n (%)\n146 (80.7%)\n60 (24.9%)\n\n\nAge, mean (SD)\n17.9 (7.30)\n48.0 (17.8)\n\n\nFemale, n (%)\n107 (59.1%)\n52 (21.6%)"
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html#analysis-time",
    "href": "posts/p-value-power/p-value-power.html#analysis-time",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "Using our handy dandy formula from above, and our tool from our toolbox (GLM). Below are the results.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nSE\nStatistic\n(Z-Value)\nPr(&gt;|z|)\n(P-Value)\n\n\n\n\n(Intercept)\n6.31\n1.20\n5.25\n1.53e-7\n\n\nLL\n2.77\n0.484\n5.73\n1.00e-8\n\n\nAge\n-0.317\n0.0499\n-6.36\n2.04e-10\n\n\nSex\n2.15\n0.493\n4.36\n1.32e-5"
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html#now-for-the-interpretation-of-the-p-value",
    "href": "posts/p-value-power/p-value-power.html#now-for-the-interpretation-of-the-p-value",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "The test statistic for regression is defined by \\(\\beta / SE(\\beta)\\). Beta here is the coefficient. For example, the test statistic for age is (note, close to -6.36. We will chalk up the difference to a rounding error):\n\\[\n-0.317/0.0499 = -6.35\n\\]\nNow, this number on it’s own is pretty useless. We want to know, what is our p-value?! Well, to do that we turn to the normal distribution."
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html#normal-distribution",
    "href": "posts/p-value-power/p-value-power.html#normal-distribution",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "Using the normal distribution, since our sample size is bigger than 30 (a wonderfully random number. Honestly, I have no idea why 30 is the cutoff), we can determine our almighty p-value.\n\n\nCode\n2*pnorm(-6.358)\n\n\n[1] 2.043975e-10\n\n\nTa-da! We have our p-value of 2.04e-10. This is less than our pre-specified value of 0.05. What does it mean though? From earlier, it is the probability of such an extreme value of the test statistic (-6.35) occurring if the null hypothesis were true. Finally, we can use significant in our interpretation right? Technically yes, however there are some strong caveats we need to go over first"
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html#issues-with-the-p-value",
    "href": "posts/p-value-power/p-value-power.html#issues-with-the-p-value",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "P-values are typically over emphasized. They are a piece of the puzzle, however they are not the whole puzzle. Arguably, a more important piece of the puzzle is the effect size. For our example, lollipop lickers have increased odds of a sore tongue compared to non-lollipop lickers (OR = 5.87, 95% CI: 2.53 to 13.87). The effect, in this case OR, should be another piece of the puzzle that is considered. Finally, there are two other errors we should go over that highlight how not confident we can be in the p-value.\n\n\nOften type I and type II errors are highlighted however type M and type S errors are important to note as well. A type S error is the probability of an estimate being in the wrong direction (Gelman and Carlin 2014). A type M error is the factor by which the magnitude of an effect might be overestimated (Gelman and Carlin 2014). Definitions are always great but an example helps to better understand.\nUsing our example from before, and the function outlined in Gelman and Carlin (2014):\n\n\nCode\nretrodesign &lt;- function(A, s, alpha=.05, df=Inf, n.sims=10000){\n  z &lt;- qt(1-alpha/2, df)\n  p.hi &lt;- 1 - pt(z-A/s, df)\n  p.lo &lt;- pt(-z-A/s, df)\n  power &lt;- p.hi + p.lo\n  typeS &lt;- p.lo/power\n  estimate &lt;- A + s*rt(n.sims,df)\n  significant &lt;- abs(estimate) &gt; s*z\n  exaggeration &lt;- mean(abs(estimate)[significant])/A\n  return(list(power=power, typeS=typeS, exaggeration=exaggeration))\n}\n\nretrodesign(-0.31703, 0.04986)\n\n\nThe type S error is 0.99, while the type M error is -0.997. This means that there is a 99% probability that the estimate is the wrong sign, and that the value is slightly underestimated (0.997 of what it should be). Doesn’t sound good right?"
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html#so-are-p-values-garbage",
    "href": "posts/p-value-power/p-value-power.html#so-are-p-values-garbage",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "P-values have a place in science, however they should be interpreted with caution and as a piece to the puzzle. Other pieces include the effect size, study design, sample that was studied and a myriad of other components."
  },
  {
    "objectID": "posts/glm/glm.html",
    "href": "posts/glm/glm.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Regression is a commonly used tool in inferential statistics to build a model to make inferences about a super-population. Before we start however, I’d like to highlight that regression is not always necessary or even the best approach. For example, sometimes descriptive statistics are a more appropriate tool. With that being said, regression is a key tool in a statistician’s toolbox. Before we get started, I’d like to revisit one of my favorite quotes by George Box, a British statistician: “All models are wrong, some are useful”.\nWith this in mind, let’s get started! The real question is where should we start? Typically, most courses start with ordinary least squares (OLS) regression. Seems like a reasonable place to me to begin as well! However, we will back up a little bit and start with the equation of a line."
  },
  {
    "objectID": "posts/glm/glm.html#regression-swiss-army-knife-of-statistics",
    "href": "posts/glm/glm.html#regression-swiss-army-knife-of-statistics",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Regression is a commonly used tool in inferential statistics to build a model to make inferences about a super-population. Before we start however, I’d like to highlight that regression is not always necessary or even the best approach. For example, sometimes descriptive statistics are a more appropriate tool. With that being said, regression is a key tool in a statistician’s toolbox. Before we get started, I’d like to revisit one of my favorite quotes by George Box, a British statistician: “All models are wrong, some are useful”.\nWith this in mind, let’s get started! The real question is where should we start? Typically, most courses start with ordinary least squares (OLS) regression. Seems like a reasonable place to me to begin as well! However, we will back up a little bit and start with the equation of a line."
  },
  {
    "objectID": "posts/glm/glm.html#equation-of-a-line",
    "href": "posts/glm/glm.html#equation-of-a-line",
    "title": "Generalized Linear Models",
    "section": "Equation of a Line",
    "text": "Equation of a Line\nFrom high school math, you probably remember the equation of a line as\n\\[\ny = mx +b\n\\]\nwhere \\(y\\) is the dependent variable, \\(m\\) is the slope of the line, \\(x\\) is the independent variable and \\(b\\) is the y-intercept. A key exercise that is often used in math, using graph paper, is to first plot data points based on their coordinates (x, y), then draw a line of best fit through them. Once you draw the line, you can calculate the slope and extend the line through the y-intercept. This gives you the equation of your line which you can use to predict values of y! When drawing the line of best first, the goal is to capture as many data points on the line as possible, or closest to the line. OLS regression is similar to this."
  },
  {
    "objectID": "posts/glm/glm.html#plain-old-ordinary-regression",
    "href": "posts/glm/glm.html#plain-old-ordinary-regression",
    "title": "Generalized Linear Models",
    "section": "Plain Old Ordinary Regression",
    "text": "Plain Old Ordinary Regression\nOLS regression tries to minimize the sum of squares of the differences between the observed variable and those predicted by the model (hence the least squares). Put another way: by minimizing the residuals. A way to conceptualize this is by drawing the line of best fit.\n\nFor example, imagine we have a database that we can use to ask a burning question we have: Does a person’s age effect the number of balloons they own? Now, if we draw a line of best fit through the data, using number of balloons as the outcome/y/dependent variable, and age as the predictor/x/independent variable… we get the figure below.\n\n\nCode\nset.seed(2228) # August 28, 2022\n\nlibrary(tidyverse)\nlibrary(ggxmean)\n\nn.id = 250\n\ndf &lt;- data.frame(\n  age = runif(n = n.id, min = c(5, 18), max = c(30, 85)),\n  sex = rbinom(n = n.id, size = 1, prob = c(0.60,0.20)),\n  candy_lover = rbinom(n = n.id, size = 1, prob = c(0.64, 0.45)),\n  \n  # Beta Coefficients\n  \n  beta_age = runif(n = n.id, min = 0, max = 0.10),\n  beta_sex = runif(n = n.id, min = 0, max = 0.30),\n  beta_candy = runif(n = n.id, min = 0, max = 0.40)\n) %&gt;% \n  dplyr::mutate(\n    num_ballons = round((beta_age*age + beta_sex*sex + beta_candy*candy_lover)*3,0) # number of ballons that you own\n  )\n\nggplot2::ggplot(data = df, mapping = aes(x = age, y = num_ballons)) +\n  ggplot2::geom_point(color = \"red\") +\n  geom_lm() +\n  ggxmean::geom_lm_residuals(linetype = \"dashed\") +\n  labs(x = \"Age\", y = \"Number of Balloons Owned\") +\n  theme_minimal() + \n  ggtitle(\"Number of Balloons by Age\") +\n  scale_x_continuous(limits = c(0, 100)) + \n  scale_y_continuous(limits = c(0, 30)) + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nLet’s break down this figure into digestible chunks. First, let’s focus on the distance from the red dot to the blue line for each data point. This measurement is from the observed value (red dot) to the predicted value (point on the blue line). This is also known as the residual; in the figure, the dashed black line. By drawing the line of best fit, we are aiming to minimize the residuals. Once we have that, we can determine the equation of the line and predict some values!\nOf course, this is great in theory to draw the line of best fit, however it can get more complicated when there are additional variables we’d like to adjust for. There are two extra variables in our database that we think could be important: sex and candy lover (yes/no). We may want to adjust for whether a person loves candy or not since candy buyers tend to buy more balloons. Furthermore, we may also want to adjust for sex. How do we draw a line of best fit for all of these? The simple answer is we don’t. A more useful approach is to use a mathematical equation.\n\nUsing Equations and Matrices\nFirst, we need to expand our equation to align with our research question\n\\[\n\\text{number of balloons} = \\beta_0 + \\beta_1*age + \\beta_2*sex + \\beta_3*\\text{candy lover}\n\\]\nNow we need to estimate the parameters (i.e., \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)). Instead of drawing a line, we can find the estimated values of the parameters using the below equation. Note that these variables are referring to the matrix, rather than a specific variable.\n\\[\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n\\]\nwhere \\(\\hat{\\beta}\\) is the ordinary least squares estimator, \\(X\\) is the matrix containing the predictor variables and \\(y\\) is the vector of the response variable. For our example, \\(X\\) is the matrix containing the values of an intercept and values for the following variables: age, sex and candy lover.\n\n\n\n\n\n\nNote\n\n\n\nFor those not familiar with linear algebra, the superscript T is called the transpose. Similarly, the -1 superscript is referred to as the inverse of a matrix.\n\n\nAbstract concepts can be helpful however an example using data is always better. Using our data from before on balloons and age, we can create our matrix of predictors, \\(X\\). In our matrix, we will also include a column of all 1s to reflect the intercept. Looking at the first six individuals, we get a sense of what the matrix looks like.\n\n\nCode\nX = as.matrix(cbind(1, df$age, df$sex, df$candy_lover)) \n\nX.df = X %&gt;% as.data.frame() \ncolnames(X.df) = c(\"intercept\", \"age\", \"sex\", \"candy lover\")\n\nhead(X.df)\n\n\n  intercept      age sex candy lover\n1         1 29.23506   1           0\n2         1 54.38400   0           1\n3         1 27.16523   0           0\n4         1 44.35563   0           0\n5         1 29.51780   1           1\n6         1 75.95165   0           1\n\n\nAs you can see, the first column for the intercept is all 1s. The second is the value for age, third is sex (1 = female, 0 = male) and fourth is the value for candy lover (1 = yes, 0 = no). Similarly, for the \\(y\\) matrix\n\n\nCode\ny = as.matrix(df$num_ballons)\ny.df = y %&gt;% as.data.frame()\ncolnames(y.df) &lt;- c(\"number of ballons owned\")\n\nhead(y.df)\n\n\n  number of ballons owned\n1                       2\n2                      10\n3                       2\n4                      12\n5                       6\n6                       2\n\n\nWe can see that the \\(y\\) matrix is just the values for the outcome of interest. Now, how do we estimate the parameters? For that, we turn back to our handy dandy formula.\n\nEstimating Parameters\nRemember our formula from before? If you don’t, no problem I will add it below again just for good measure. Using our handy dandy formula and plugging in our matrices\n\\[\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n\\]\n\n\nCode\n# Doing it step by step:\n\nstep.1 &lt;- t(X)%*%X\nstep.2 &lt;- solve(step.1) # solve will return the inverse of a\nstep.3 &lt;- step.2%*%t(X)\nstep.4 &lt;- step.3%*%y\nbeta &lt;- step.4\n\n# Alternatively, can do in one messy looking code: \n\nbeta &lt;- solve(t(X)%*%X)%*%t(X)%*%y\n\n# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code all in one\n\nbeta %&gt;% \n  as.data.frame() %&gt;% \n  dplyr::mutate(\n    variable = c(\"intercept\", \"age\", \"sex\", \"candy lover\")\n  ) %&gt;% \n  dplyr::rename(\n    estimate = V1\n  ) %&gt;% \n  dplyr::select(\n    variable, estimate\n  )\n\n\n     variable    estimate\n1   intercept  0.09488954\n2         age  0.14316794\n3         sex -0.09891130\n4 candy lover  0.96749456\n\n\nGreat Scott! We have some results! The estimates are great but you may be wondering, “what about that standard error tho?” If you were thinking that, great point! First, we’ll need to get the variance from the variance-covariance matrix. How can we determine this variance-covariance matrix? Another formula!\n\n\nEstimating Variance\nIn order to calculate the variance-covariance matrix, we first the residuals. We can do that using the below formula (how many times have I said formula so far in the post?)\n\\[\nResiduals = y - \\beta_1 - \\beta_2*age - \\beta_3*sex - \\beta_4*\\text{candy lover}\n\\]\nOnce we know the residuals, we can calculate the variance-covariance matrix as\n\\[\nVCov = \\frac{1}{n-k}(RES^TRES)*(X^TX)^{-1}\n\\]\nwhere \\(n, k, RES, X\\) are the number of observations, number of parameters estimated, matrix of residuals and matrix of values for the predictor variables. Back to our example\n\n\nCode\n# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code \n\n# Calculating residuals as y-intercept-beta_1*X1-beta_2*X2\n\nres &lt;- as.matrix(y-beta[1]-beta[2]*X[,2]-beta[3]*X[,3] - beta[4]*X[,4])\n\n# Note the above is really:\n# res &lt;- as.matrix(y-beta[1]-beta[n]*X[,n]) for as many n's as there are \n\n# Variance-Covariance Matrix (VCV) \n\n# Formula for VCV: (1/df)*t(res)*res*[t(X)(X)]^-1\n\nn = nrow(df)\nk = ncol(X)\n\nVCV &lt;- (1/(n-k))*as.numeric(t(res)%*%res)*solve(t(X)%*%X)\n\nVCV.df &lt;- VCV %&gt;% as.data.frame() \ncolnames(VCV.df) &lt;- c(\"intercept\", \"age\", \"sex\", \"candy lover\")\nrownames(VCV.df) &lt;- c(\"intercept\", \"age\", \"sex\", \"candy lover\")\nVCV.df\n\n\n               intercept           age          sex   candy lover\nintercept    0.376249319 -0.0054645875 -0.156856159 -0.1350433698\nage         -0.005464587  0.0001331024  0.002296164  0.0002074815\nsex         -0.156856159  0.0022961636  0.294962047 -0.0373553074\ncandy lover -0.135043370  0.0002074815 -0.037355307  0.2375580016\n\n\nTa-da! We now have our variance-covariance matrix! You may be asking yourself “so what?”. Well, besides that being rude, this matrix is how we can determine the standard error. The diagonals of the matrix are the variance for each of the variables. If we take the square root of the variance, we will get the standard deviation. In this case the standard deviation of the sampling distribution, aka the standard error.\n\n\nCode\nse &lt;- sqrt(diag(VCV))\n\nse\n\n\n[1] 0.6133917 0.0115370 0.5431041 0.4873992\n\n\nThe last piece to our puzzle is to add some p-values. While we’re at it, let’s make it a little easier to read by adding in some formatting and text to help the reader make sense of our results.\n\n\nCode\np_value &lt;- rbind(\n  2*pnorm(abs(beta[1]/se[1]), lower.tail = FALSE),\n  2*pnorm(abs(beta[2]/se[2]), lower.tail = FALSE),\n  2*pnorm(abs(beta[3]/se[3]), lower.tail = FALSE),\n  2*pnorm(abs(beta[4]/se[4]), lower.tail = FALSE)\n)\n\n# Output \n\noutput &lt;- as.data.frame(\n  cbind(\n    c(\"Intercept\", \"Age\", \"Sex\", \"Candy Lover\"),\n    round(beta,5),\n    round(se, 5),\n    round(p_value, 4)\n)\n)\n\nnames(output) &lt;- c(\n  \"Coefficients\",\n  \"Estimate\",\n  \"Std. Error\",\n  \"Pr(&gt;{Z}\"\n)\n\noutput\n\n\n  Coefficients Estimate Std. Error Pr(&gt;{Z}\n1    Intercept  0.09489    0.61339  0.8771\n2          Age  0.14317    0.01154       0\n3          Sex -0.09891     0.5431  0.8555\n4  Candy Lover  0.96749     0.4874  0.0471\n\n\n\n\nComparing to lm()\nGreat so we have worked through calculating OLS regression by hand, now what? Glad you asked! How do we know it worked? One way to check is to compare the values we got with those if we use the lm() function in R.\n\n\nCode\nmod.fit &lt;- lm(num_ballons ~ age + sex + candy_lover, \n              data = df)\n\nsummary(mod.fit)\n\n\n\nCall:\nlm(formula = num_ballons ~ age + sex + candy_lover, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9362 -2.0570  0.2046  1.6230 11.9744 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.09489    0.61339   0.155   0.8772    \nage          0.14317    0.01154  12.409   &lt;2e-16 ***\nsex         -0.09891    0.54310  -0.182   0.8556    \ncandy_lover  0.96749    0.48740   1.985   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.732 on 246 degrees of freedom\nMultiple R-squared:  0.423, Adjusted R-squared:  0.4159 \nF-statistic:  60.1 on 3 and 246 DF,  p-value: &lt; 2.2e-16\n\n\nReviewing the results above, these look very similar to those obtained from working through step by step (except for some rounding differences). Now that we have covered linear regression, we are ready to move onto generalized linear models (GLM).\n\n\n\n\n\n\nNote\n\n\n\nThis post focuses on how the estimates are derived for linear regression, not the assumptions or diagnostics that can be used to check these assumptions and model fit. It’s always important to check the assumptions to see if any of them are violated, for example the assumption of homoscedasticity for the residuals. A good resource for reviewing the assumptions of OLS is by Jim Frost (OLS Linear Regression Assumptions)"
  },
  {
    "objectID": "posts/glm/glm.html#generalized-linear-model",
    "href": "posts/glm/glm.html#generalized-linear-model",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nFor any statistical method there are certain assumptions that have to be met. For example, with OLS regression one of the key assumptions is homoscedasticity for the residuals. Sometimes these assumptions are not met. Using homoscedasticity as an example, the variance of the errors is not consistent across observations and is rarely met. Due to this violation, using OLS regression may provide inaccurate results. One solution is to use a generalized linear model (GLM), however GLMs have their own set of assumptions. But first, we need to review what a GLM is!\n\n\n\n\n\n\nNote\n\n\n\nWhy GLMs over OLS regression? A few of the reasons [STAT 504, Section 6.1]:\n\nSince the model uses MLE instead of OLS, parameter estimates and likelihood functions benefit from asymptotic normal and chi-square distributions\nHomoscedasticity is not required for GLMs\nNo need to transform the outcome variable to have a normal distribution\nChoosing the link has nothing to do with what you choose for the random component. For example, you can use a binomial distribution but choose to use a logit link or probit.\nThere is no separate error term\n\n\n\n\nDo Candy Lovers Own More Balloons?\nFor our next question, we want to know if candy lovers own more balloons than those who do not. Luckily, we can use the same database that we did for our regression analysis! First, we need to go through what a GLM is before we jump into getting answers.\n\n\nMain Parts of GLM\nThere are three components to any GLM [STAT 504, Section 6.1]:\n\nRandom Component. This refers to the response variable. We need to make an assumption about the probability distribution of the response variable (i.e., normal, binomial, Poisson, etc.). Note: this is the only random component in the model (i.e., there is not a separate error term like there is in OLS regression). For our example, we assume that candy lovers is from a binomial distribution.\nSystematic Component. This outlines the explanatory variables and how they are related. Again, using the example above, the systematic component is linear since it will be \\(\\beta_0 + \\beta_1*age + \\beta_2*sex + \\beta_3*\\text{number of balloons}\\)\nLink Function. The link function is a crucial component of GLMs which differentiates it from OLS regression. This specifies how the random and systematic components are connected. For our example, the link function is logit. If our outcome variable were continuous it would be the identity link function (why it’s called the identity link function has never made sense to me. Think of it as the one you are used to: y = mx + b).\n\nLike any statistical technique, there are assumptions as well [STAT 504, Section 6.1]:\n\nThe data are independently distributed\nThe dependent variable typically assumes a distribution from an exponential family (i.e., normal, binomial, Poisson, etc.)\nA linear relationship between the transformed expected response in terms of the link function and explanatory variables (however not in terms of the response and explanatory variables).\nErrors need to be independent but not normally distributed (this is a key difference between OLS regression and GLMs)\n\n\n\nMaximum Likelihood Estimation\nOne of the key differences between OLS and GLM is the way that parameters (aka coefficients) are estimated. While OLS uses ordinary least squares, GLMs use something called maximum likelihood estimation (MLE). MLE is like what it sounds like: it’s about maximizing the likelihood function so that the model uses values that make the observed data most probable (aka most likely).\nThere are different methods to determine this value using MLE including solving the derivative of the likelihood funciton then setting to 0 (where the maxima occurs, from calculus), or more iterative procedures such as the Gradient descent method or the Newton-Raphson method. Here we will focus on iteratively reweighted least squares (IWLS) since that is what R uses by default in the glm() function.\n\nIteratively Reweighted Least Squares\nIWLS, is an algorithm that is used to determine the parameters and standard errors of the parameters. We’ll use logistic regression to walk through the steps, although for other link functions it is a similar process. The steps for IWLS are outlined below (Fox 2014)\n\nSet the regression coefficients to some initial value. For our example, we will start with 0\nFor each iteration, t, calculate the fitted probabilities, \\(\\mu\\), variance-function values, \\(v\\), working-response values, \\(z\\), and weights, \\(w\\).\n\\(\\mu_i^{(t)} = [1 + exp(-\\eta_i^{(t)})]^{-1}\\)\n\\(v_i^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})\\)\n\\(z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)})/v_i^{(t)}\\)\n\\(w_i^{(t)} = n_i*v_i\\)\nNote: \\(n_i\\) represents the binomial denominator for the ith observation. For binary data, all of the \\(n_i\\) are 1.\nRegress the working response on the predictors using weighted least squares, minimizing the weighted residual sum of squares. To do that, you can use the following formula:\n\\(\\sum\\limits_{i = 1}^{n}w_i^{(t)}(z_i^{(t)} - x_i^{'}\\beta)^2\\)\nwhere \\(x_i^{'}\\) is the ith row of the model matrix.\nRepeat steps 2 and 3 until the regression coefficients stabilize at the maximum-likelihood estimator \\(\\hat\\beta\\)\nCalculate the estimated asymptotic covariance matrix of the coefficients using the below formula\n\\(\\hat{V}(\\hat{\\beta}) = (X^{'}WX)^{-1}\\)\nwhere \\(W = \\text{diag}\\text{(}w_i\\text{})\\) is the diagonal matrix of weights from the last iteration and \\(X\\) is the model matrix.\n\nReading through steps can be helpful but an example is always better. Let’s work through this in R. First we’ll want to make a function to calculate IWLS implementing these steps (credit to Michael Clark for code for implementing IWLS, link here)\n\n\nCode\niwls &lt;- function(X, y, tol = 1e-7, iter = 500){\n  \n  # Note: tol = 1e-7 is used by the lsfit function\n  \n  # First we need to start with some inital values\n  \n  int = log(mean(y)) / (1-mean(y)) # intercept\n  beta = c(int, rep(0, ncol(X) -1))\n  currtol = 1\n  it = 0\n  ll = 0 # log likelihood\n  \n  # As long as the tolerance calculate is greater than what we will allow we want the code to repeat\n  \n  while(currtol &gt; tol && it &lt; iter){\n    it = it + 1\n    ll_old = ll\n    \n    eta = X %*% beta\n    mu = plogis(eta)[,1]\n    s = mu*(1-mu)\n    S = diag(s)\n    z = eta + (y-mu)/s\n    beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z))\n    var = solve((t(X) %*% S %*% X))\n    \n    ll = sum(\n      dbinom(\n        y, \n        prob = plogis(X%*% beta), \n        size = 1, \n        log = TRUE)\n      )\n    \n    currtol = abs(ll - ll_old)\n  }\n  \n  list(\n    beta = beta, \n    var = var, \n    se = diag(sqrt(var)), # SE = sqrt(var) but we want diagnoals of the variance-covariance matrix \n    iter = it, \n    tol = currtol, \n    loglik = ll, \n    weights = plogis(X %*% beta) * (1 - plogis(X %*% beta))\n  )\n  \n}\n\n\n\n\n\nComparing our method to glm()\nNow that we’ve written a function that calculates estimates of the parameters and standard errors, we need to see if it works! What’s a better way to check than comparing with a well-established method? We’ll compare our function to that from using glm(), although admittedly our output is not as clean.\n\n\nCode\nlibrary(tidyverse)\n\nX &lt;- cbind(1, df$age, df$sex, df$num_ballons) %&gt;% as.matrix()\ny &lt;- df$candy_lover %&gt;% as.matrix()\n\nour.way &lt;- iwls(X, y)\n\nour.way$beta\n\n\n            [,1]\n[1,]  0.23378250\n[2,] -0.01323819\n[3,]  0.68071575\n[4,]  0.06809751\n\n\nCode\ndiag(our.way$se)\n\n\n         [,1]        [,2]      [,3]      [,4]\n[1,] 0.300512 0.000000000 0.0000000 0.0000000\n[2,] 0.000000 0.008062909 0.0000000 0.0000000\n[3,] 0.000000 0.000000000 0.3057791 0.0000000\n[4,] 0.000000 0.000000000 0.0000000 0.0354048\n\n\nCode\nglm.way &lt;- glm(candy_lover ~ age + sex + num_ballons, \n               family = binomial(link = \"logit\"), \n               data = df)\n\n\n\nOutput from our function compared to glm()\n\n\n\n\n\n\n\nParameter\nEstimate (SE)\nglm()\nEstimate (SE)\nour method\n\n\n\n\nIntercept\n0.233782 (0.300512)\n0.233783 (0.300512)\n\n\nAge\n-0.013238 (0.008063)\n-0.013238 (0.008063)\n\n\nSex\n0.680716 (0.305779)\n0.680716 (0.305779)\n\n\nNumber of Balloons\n0.068098 (0.35405)\n0.068098 (0.035405)\n\n\n\nLooking at the estimates from the glm() function to our function…nearly identical results! Yippee! This is also what we’d expect. We can also look at the weights from the last iteration for both the glm() method and using our function. The code is in the below snippet, however rather than boring you with weights for 250 observations, I will leave that up to you to review if you are interested (tldr: they are quite similar).\n\n\nCode\nresult.weights &lt;- cbind(glm.way$weights, our.way$weights) %&gt;% as.data.frame()"
  },
  {
    "objectID": "posts/glm/glm.html#whats-next",
    "href": "posts/glm/glm.html#whats-next",
    "title": "Generalized Linear Models",
    "section": "What’s Next?",
    "text": "What’s Next?\nSince every good story must come to an end, so too does our GLM by hand exercise…but fear not! You can now use this to foray into the world of GLM with a better understanding of how these parameters are calculated!"
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html",
    "href": "posts/causal_inference_intro/causal_inference.html",
    "title": "Intro to Causal Inference",
    "section": "",
    "text": "I was first introduced to causal inference by Hernan and Robins (2021), in the summer of 2021 prior to starting my PhD. This book was tremendous and changed my thinking about “correlation doesn’t equal causation”.\n\nFrom introductory science, as scientists, we are taught that correlation doesn’t equal causation. I had never questioned this until I was in my MSc program and started thinking about the association between lung cancer and smokers. Stellman et al. (2001) reported an odds ratio of 40.4 (95% CI: 21.8-79.6), which to me seemed high.\nNow there are two ways to interpret this. On one hand, if we were a tobacco company we could argue that the 95% CI is pretty wide, indicating that the estimate is not that precise and the statistical method used must be wrong, etc (by etc, I mean whatever excuses you want to use). On the other hand, even at the lower limit an odds ratio of 21.8 is pretty damn high. In my opinion, this would require at the very least more investigation to determine “does smoking cause lung cancer?”"
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html#background",
    "href": "posts/causal_inference_intro/causal_inference.html#background",
    "title": "Intro to Causal Inference",
    "section": "",
    "text": "I was first introduced to causal inference by Hernan and Robins (2021), in the summer of 2021 prior to starting my PhD. This book was tremendous and changed my thinking about “correlation doesn’t equal causation”.\n\nFrom introductory science, as scientists, we are taught that correlation doesn’t equal causation. I had never questioned this until I was in my MSc program and started thinking about the association between lung cancer and smokers. Stellman et al. (2001) reported an odds ratio of 40.4 (95% CI: 21.8-79.6), which to me seemed high.\nNow there are two ways to interpret this. On one hand, if we were a tobacco company we could argue that the 95% CI is pretty wide, indicating that the estimate is not that precise and the statistical method used must be wrong, etc (by etc, I mean whatever excuses you want to use). On the other hand, even at the lower limit an odds ratio of 21.8 is pretty damn high. In my opinion, this would require at the very least more investigation to determine “does smoking cause lung cancer?”"
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html#common-ground",
    "href": "posts/causal_inference_intro/causal_inference.html#common-ground",
    "title": "Intro to Causal Inference",
    "section": "Common Ground",
    "text": "Common Ground\nPrior to diving straight into the world of causal inference, we first need to set some common ground rules.\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind causal inference is a broad field and will be the subject of many subsequent posts, this is just an introductory post or wetting your beak if you will.\n\n\n\nConfound It!\nIn order to get to the bottom of “does X cause Y?” we need to outlined some key methodological concepts. First, we need to outline what confounding is and clearly define what we mean.\nDifferent disciplines used different terminology, however confounding in clinical epidemiology refers to a variable that impacts both the exposure (i.e., smoking) and outcome (i.e., lung cancer). A useful way to visualize this is using directed acyclic graphs (DAGs). I’ll defer to Hernan and Robins (2021) for more detail about DAGs. However, the below illustrates what confounding is.\n\n\n\n\n\nFigure 1: Coffee Drinkers, Smoking and Lung Cancer.\n\n\n\n\nFigure 1 shows that coffee drinkers are associated with smoking. Logically, this makes sense because many smokers enjoy a cup of coffee with their cigarette. In fact, a lot of people enjoy coffee. A group of these people who are coffee lovers may also develop lung cancer. Based on this, you could say that people who drink coffee develop lung cancer. The question is, is this true? Or is it just that a lot of people like drinking coffee? Confounding bias, makes it difficult to tease out what coffee actually causes.\n\n\nRandomized Controlled Trials (RCTs)\nRandomized controlled trials are typically considered the gold standard in research. However, sometimes people forget why they are considered the gold standard. There are of course numerous reasons but I will touch on a few here. Firstly, an RCT is randomized meaning that patient’s are randomly assigned to one of the treatment arms. In theory, this is to ensure that characteristics are balanced between the treatment arms. By balancing characteristics, in theory, both observed and unobserved confounders are equal between the groups, making them exchangeable (aka similar).\n\n\n\n\n\n\nNote\n\n\n\nRandom is important to note here. There are different methods which are truly random, such as a random number generator, while there are others that are not.\nFor example, if you were picking players for a baseball team and wanted to assign people to two teams, team A & team B, how would you do it? Based on the color shirt they are wearing, hair color, eye color? None of those methods would be random.\n\n\nAnother key strength of RCTs is the nature of the intervention. For example, a well designed RCT will be very clear about what treatment is received by the participants and that all participants will receive the same treatment. Furthermore, one participant receiving a treatment won’t affect the other participant receiving treatment. It is also worth noting here that all participants in a RCT will receive some form of exposure (typically either treatment or placebo).\n\n\nObservational Data\nObservational data in research often gets a bad wrap. Since the gold standard is a RCT, people often view observational data as second-tier compared to an RCT. However, it all depends on how well the study is conducted. If the study is conducted rigorously, which is subjective but we will elaborate more below, then a study using observational data can be almost as good as an RCT."
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html#conditions-required-for-causal-inference",
    "href": "posts/causal_inference_intro/causal_inference.html#conditions-required-for-causal-inference",
    "title": "Intro to Causal Inference",
    "section": "Conditions Required for Causal Inference",
    "text": "Conditions Required for Causal Inference\nReal-life is very complicated. For any method or analysis, such as designing a house, assumptions are required. This is is no different for causal inference. Key assumptions for causal inference include (Hernán 2012):\n\nExchangeability\nPositivity\nConsistency\nNo versions of treatment\nNo interference\n\n\n\n\n\n\n\nNote\n\n\n\nCollectively, consistency, no versions of treatment and no interference have been referred to as the stable-unit-treatment-value assumption (SUTVA) (Hernán 2012).\n\n\nLet’s break down these assumptions down one by one.\n\nExchangeability\nRemember confounding from earlier? Well that pesky bias is back again to haunt us.\n\n\n\n\n\n\nNote\n\n\n\nRecommend to pause here and re-read Section 2.1 if needed\n\n\nThe goal, although not always, of a lot of research is to compare two different exposures. An exposure could be a treatment, for example drug A to placebo, or to compare purple Popsicle eaters with water drinkers. An example always helps.\nLet’s pretend we want to compare purple Popsicle eaters with water drinkers, to see if there is a difference in who is hungrier. Using an existing database of hungry people, we decide we want to compare these two groups of people. In the database, there are 591 people who have reported hunger.\n\n\nTable 1: Purple Popsicle Eaters vs Water Drinkers: Demographic Characteristics\n\n\n\n\n\n\n\nDemographic Variable\nPurple Popsicle Eaters\n(n = 250)\nWater Drinkers\n(n = 341)\n\n\n\n\nAge, mean (SD)\n9.18 (3.39)\n42.58 (22.03)\n\n\nFemale, n (%)\n190 (76.0%)\n145 (42.5%)\n\n\n\n\nTable 1 shows the average age and proportion of females in each of the two groups. Now, after looking at these two groups, you may think to yourself “Wait a minute…water drinks on average are 42 while purple Popsicle eaters are an average age of 9? These can’t possibly be compared!”.\nYour inclination was right. These two groups are quite different and may vary in more ways than what is shown. This lack of similarity could be reworded as a lack of exchangeability. Luckily, there are fancy stats methods to fix this, which will be the topic of other posts.\n\n\n\n\n\n\nImportant\n\n\n\nWhile we can adjust for observed confounding, it is not possible to adjust for unobserved confounding. We simply don’t know about it, for example how many people in each group enjoy the taste of grape crush.\n\n\n\n\nPositivity\nPositivity refers to the condition that every individual has a greater than 0 probability of being assigned to each of the treatment levels (Hernan and Robins (2021), pp. 30). Logically, this makes sense because if we think about our purple Popsicle eaters and water drinks, if someone in that group had a less than 0 probability they couldn’t possibly be in that group!\n\n\nConsistency\nConsistency means that the observed outcome for every treated individual equals her outcome if she received treatment and that the observed outcome for every untreated individual equals her outcome if she had remained untreated, that is \\(Y^a = Y\\) for every individual with \\(A = a\\) (Hernan and Robins (2021), pp. 31).\nOn the surface, this seems intuitive but lets dive a little deeper. There are two components to consistency:\n\nA precise definition of the counterfactual outcome \\(Y^a\\) via a detailed specification of the superscript \\(a\\) ( Hernan and Robins (2021)\nLinkage of the counterfactual outcomes to the observed outcomes ( Hernan and Robins (2021))\n\nThe first bullet point is we essentially want to have a well defined intervention. If we think back to our purple Popsicles, we want to make sure that it is the same size, shape and brand that each participant receives. Doing this allows us to make sure that each participant is getting the same treatment.\nFor the second bullet point, remember we are using observational data for our purple Popsicle eaters. We have to make sure that for the analysis, we only treat people receiving our intervention (purple Popsicle eaters) as treated, or as Popsicle eaters, and the others as not Popsicle eaters.\n\n\nNo Versions of Treatment\nAs discussed in Section 3.3, we cannot have multiple versions of the same treatment. This would muddy our findings if we were giving different people different Popsicle. Plus, this would make it hard to determine the effect of purple Popsicles on hunger. Imagine, you gave some people a much bigger Popsicle than others. Of course they’d be less hungry!\n\n\nNo Interference\nInterference here refers to the interference of the exposure. Exposure could be treatment, infectious disease or Popsicles. For our example, and most examples in this blog, the condition of no interference will be assumed to be met. Examples where this is not met is infectious disease studies (Hernan and Robins (2021))."
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html#when-to-use-cause",
    "href": "posts/causal_inference_intro/causal_inference.html#when-to-use-cause",
    "title": "Intro to Causal Inference",
    "section": "When to use Cause",
    "text": "When to use Cause\nFinally, all we wanted to do was get to use the word cause! Why? Because it’s important to use. If the above conditions hold, three main ones: exchangeability, positivity and consistency, then you can use the term cause. Besides, wouldn’t you want to know if Popsicles caused hunger?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causally Curious in the Real World",
    "section": "",
    "text": "Mastering Statistics Through Make-Believe\n\n\nSimulated Data in R\n\n\n\n\nSimulating Data\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nHot Diggity DAG\n\n\nAn Introduction to DAGs\n\n\n\n\nDAGs\n\n\nCausal Diagrams\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nWho are we talking about?\n\n\nThe Role of Causal Estimands\n\n\n\n\nEstimands\n\n\nATE\n\n\nATT\n\n\nATU\n\n\nATO\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2023\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nBeyond the Odds\n\n\nUnravelling the Enigma of Non-Collapsibility\n\n\n\n\nOR\n\n\nCollapsibility\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nStandardize Your Way to Causal Inference\n\n\nStandardization and the Parametric G-Formula\n\n\n\n\nStandardization\n\n\nParametric G-Formula\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nGeneralized Linear Models\n\n\n\n\n\n\n\nGLM\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\n\n\nIPW\n\n\nIPTW\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nMarginal vs Conditional Effects\n\n\n\n\n\n\n\nMarginal\n\n\nConditional\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nReal-World Evidence: What’s all the hype?\n\n\n\n\n\n\n\nRWE\n\n\nObservational Data\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nP-Values and Power: Please Explain\n\n\n\n\n\n\n\nP-Values\n\n\nHypothesis Tests\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nIntro to Causal Inference\n\n\n\n\n\n\n\nCausal Inference\n\n\nRCT\n\n\nObservational Data\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nOR/RR/HR: What’s the Difference?\n\n\n\n\n\n\n\nOR\n\n\nRR\n\n\nHR\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2022\n\n\nRyan Batten\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My undergraduate degree is in civil engineering, with a focus on structural engineering. After graduating, I decided that while I loved math, which is why I went into engineering in the first place, perhaps the career wasn’t for me. I began looking for other opportunities/career paths when I had a meeting with my now supervisor. He introduced me to clinical epidemiology, which led me to start my MSc in 2016.\nA few different work experiences later, I’ve been able to apply my clinical epidemiology training + LOVE of statistics + passion for math/big data to everyday life. As a result, I started my PhD in January 2022 in clinical epidemiology focusing on causal inference and real-world evidence.\n\nExperience\nI’ve had a lot of different jobs relevant to research and not so relevant to research. There has been a wide range of jobs, varying from working as an engineering student, to as an estimator, to at a Canadian hardware store chain. Research relevant experience? I’ve worked as a student, consultant, on grants, for the Public Health Agency of Canada, EVERSANA and most recently at PHASTAR. In my recent role, as a Biostatistician III I’ve been able to do what I love. Diving into various statistical methodologies, while working with talented people. More importantly, working with caring/nice people.\n\n\nPersonal\nOkay so you made it this far, I’m impressed! I probably wouldn’t have read it this far to be honest so props to you. I LOVE statistics, figured I’d mention it again in case you didn’t know that yet, but have outside work interests as well. I’m an avid sports fan. A Buffalo Sabres fan, which has been rough as of late and a Kansas City Chiefs fan (2023 Super Bowl Champs!!).\nI am currently living in Halifax, Nova Scotia with my wife and daughter, and appreciate a good cider. While I live in Nova Scotia, I’m originally from Newfoundland (highly recommend people visit when the weather is nice, key is when)."
  }
]