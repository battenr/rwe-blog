[
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "",
    "text": "You may have seen real-world evidence (RWE) being used increasingly in research settings. It has tremendous potential for answering important research questions, however what exactly is RWE? RWE is evidence generated from real-world data (RWD). RWD is defined by the FDA as data relating to patient health status and/or the delivery of health care routinely collected from a variety of sources (FDA December 2018). This data could be from electronic health records, medical claims and billing, electronic medical records, data from product registries, wearable devices, mobile devices and other sources. Sounds pretty good right? Well the quality of the evidence is only as a good as the quality of the study conducted."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#hierarchy-of-research",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#hierarchy-of-research",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "Hierarchy of Research",
    "text": "Hierarchy of Research\nHistorically, the hierarchy of research has been that systematic reviews with a meta-analysis are at the top of the evidence pyramid, followed by randomized controlled trials (RCTs), cohort studies, case-control studies and anecdotal research.\n\n\n\nFigure 1: Hierarchy\n\n\nRCTs are near the top of this pyramid for good reason. A well-conducted RCT can provide insight into efficacy of a treatment in a controlled environment so that no data is missing, both observed and unobserved confounders are balanced between groups and you can measure the variables you need for analysis. There are however, drawbacks/potential limitations: RCTs can be expensive to conduct, can require a long duration of follow-up, are not feasible to conduct ethically, limited sample size for assessing safety and limited generalizability. Luckily, observational study designs can help fill in the gaps in these areas."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#observational-studies",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#observational-studies",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "Observational Studies",
    "text": "Observational Studies\nWhen people discuss about the hierarchy of study design, such as in Figure 1, the quality of the study is not mentioned. For example, is a poorly conducted RCT better than a rigorously conducted case-control study using observational data? That is up for debate however, observational studies can be used to supplement findings from RCTs or where RCTs are infeasible.\n\nOne of the key issues with observational studies is the variability in methodological rigor. Due to this, results can vary widely from study to study and really depend upon the researchers conducting the study. Luckily, recent strides have been made in this field. To mention a few, the STaRT-RWE template (Wang et al. 2021) and the principles behind emulating a target trial proposed by Hernan & Robins (Hernán and Robins 2016)."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#examples-of-rwe",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#examples-of-rwe",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "Examples of RWE",
    "text": "Examples of RWE\nDiscussing about RWE in theory is useful but what are some use cases? Recently, regulatory bodies are accepting RWE as part of submissions. These can vary from accepting burden of illness studies to help contextualize the current treatment and cost of available treatments to external control arms (ECA). ECAs can be beneficial when a single arm clinical trial is conducted. In certain disease areas, such as cancer, single arm trials are common but this makes it difficult for regulatory reviewers to ascertain the efficacy of a treatment since there is no comparator arm. Demonstration of safety is another area that RWE is well-suited due to the larger sample size and the generalizability of findings to real-world settings.\nPurpura et al. (Purpura et al. 2022) reported 116 FDA-approvals of New Drug and Biologics License Applications. Of these 116, 83 used RWE to support therapeutic context and 88 used RWE to support safety and/or effectiveness. Over the coming years, these numbers are expected to rise as the methodological rigor of these studies increases."
  },
  {
    "objectID": "posts/rwe-whats-the-hype/rwe_intro.html#whats-next",
    "href": "posts/rwe-whats-the-hype/rwe_intro.html#whats-next",
    "title": "Real-World Evidence: What’s all the hype?",
    "section": "What’s Next?",
    "text": "What’s Next?\nRWE is starting to be adopted more by regulatory bodies, with the FDA accepting RWE as part of submissions (FDA December 2018), as well as the National Institute for Health and Care Excellence (NICE) issuing guidance for the development of RWE. It is a very exciting time for developing RWE given the vast data that is available! However, always keep in mind that these studies need to be conducted to similar standards that we would hope a clinical trial is."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html",
    "href": "posts/marginal-conditional/marginal_conditional.html",
    "title": "Marginal vs Conditional Effects",
    "section": "",
    "text": "This post is going to be about marginal compared to conditional effects. First, we need to understand what in the world these terms even mean. To do that, can you guess what we are going to start with? Yup! Probability again."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#conditional-vs-unconditional-probability",
    "href": "posts/marginal-conditional/marginal_conditional.html#conditional-vs-unconditional-probability",
    "title": "Marginal vs Conditional Effects",
    "section": "Conditional vs Unconditional Probability",
    "text": "Conditional vs Unconditional Probability\nFirst things first. Let’s review some basic probability terms. Unconditional probability, denoted as \\(P(A)\\), is the probability that something will happen. Unconditional probability sounds like a mouthful, so we will use marginal instead. For example, let’s assume in a made-up world that the probability of you getting a visit from the toothfairy is 0.30 on any given night. In mathematical terms:\n\\[\nP(\\text{toothfairy visit}) = 0.30\n\\]\nNow, the toothfairy randomly visiting you seems a bit bizarre doesn’t it? She’d just drop by any night for no reason at all? Maybe if you wore pink pyjamas to bed she’d visit. This can be worded as “the probability that the toothfairy will visit, given that you are wearing pink pyjamas to bed”. This probability is conditional, since it is the probability of something happening depending on another variable. Using mathematical notation again:\n\\[\nP(\\text{toothfairy visit} | \\text{wearing pink pyjamas})\n\\]\nThis same logic goes for when we are conducting analyses. At this point, it is important to highlight that exchangeability and positivity are two requirements for causal inference. If the effect is marginal, then exchangeability and positivity need to hold in all levels of the variable (Hernan and Robins 2021, 51). If the effect of interest is conditional, then exchangeability and positivity need to hold in a subset of the sample. Before going any further however, we need to review the conditional mean."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#conditional-mean",
    "href": "posts/marginal-conditional/marginal_conditional.html#conditional-mean",
    "title": "Marginal vs Conditional Effects",
    "section": "Conditional Mean",
    "text": "Conditional Mean\nA conditional mean is similar to the concept of conditional probability, except the measure is a mean instead of a probability. To determine the conditional mean, we can use either parametric estimators or nonparametric estimators. Parameters can be thought of as the coefficients of a variable. These are typically denoted by \\(\\theta\\) or by \\(\\beta\\). These parameters are estimated from the data, which can then be used to make predictions or to determine the expected value, denoted by \\(E\\).\n\nParametric Estimators of the Conditional Mean\nLet’s assume that we use the toothfairy visit example again. We want to know the average number of toothfairy visits among people wearing pink pyjamas. In math terms, that would be\n\\[\nE[\\text{Toothfairy visits} | \\text {Pyjama color}]\n\\]\nTo figure this out, we need a model. Using mathematical notation again:\n\\[\nE[\\text{Toothfairy visits} | \\text {Pyjama Color}] = \\theta_0 + \\theta_1*\\text{pyjama color}\n\\]\nThe equation above is called a parametric conditional mean model (Hernan and Robins 2021, 141), because it describes the conditional mean function in terms of a finite number of parameters (note: \\(\\theta_0\\) and \\(\\theta_1\\) are referred to as parameters of the model). Once we’ve fitted some data to the model, we can determine the predicted value, \\(\\hat{E}\\), for each value of pyjama color. In mathematical terms:\n\\[\n\\hat{E}[\\text{Toothfairy visits} | \\text{pyjama color = pink}] = \\hat{\\theta_0} + \\hat{\\theta_1}*pink\n\\]\nParametric estimators, those based on a parametric conditional mean model, allow us to estimate quantities that cannot be estimated otherwise. However, the inferences are only correct if the restrictions are correct (i.e., the model is correctly specified) (Hernan and Robins 2021, 152). In the above equation, we have to restrict the shape of the relation, also known as the functional form (Hernan and Robins 2021, 141). For the above example, we are assuming that it is a linear relationship between the color of pyjamas and toothfairy visits.\nImagine that in our database, we have no people that wear pink pyjamas BUT we have people that wear blue and green pyjamas! We could use the data on the people wearing blue and green pyjamas to estimate how many toothfairy visits people wearing pink pyjamas get.\n\n\nNonparametric Estimators of the Conditional Mean\nNonparametric estimators of the conditional mean are those that produce estimates from the data without any prior restrictions on the conditional mean function (Hernan and Robins 2021, 143). To use our example above, the only way to have a nonparametric estimator of the conditional mean would be to have that value measured. For our data on toothfairy visits and pink pyjamas, if there were no participants wearing pink pyjamas then there would be no nonparametric estimator for the conditional mean of the toothfairy visiting."
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#conditional-to-marginal",
    "href": "posts/marginal-conditional/marginal_conditional.html#conditional-to-marginal",
    "title": "Marginal vs Conditional Effects",
    "section": "Conditional to Marginal",
    "text": "Conditional to Marginal\nNow, we can calculate the unconditional expectation using the law of total expectation, however when we are talking about conditional versus marginal in clinical epidemiology, there is an additional layer we need to discuss: effect modification.\n\\[\nE[Y] = \\sum_x E[Y|X = x]Pr[X=x]\n\\]"
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#effect-modification",
    "href": "posts/marginal-conditional/marginal_conditional.html#effect-modification",
    "title": "Marginal vs Conditional Effects",
    "section": "Effect Modification",
    "text": "Effect Modification\nAn effect modifier is when the average causal effect varies across levels of that variable (Hernan and Robins 2021, 42). Since our causal effect of interest is pyjama color on toothfairy visits, if we were to stratify by age, we would notice that people who are older than 20 receive less toothfairy visits than people younger than 20. In this case, age would be\nNow, there are four different techniques that we’ll elaborate on subsequent posts that can be used to adjust for effect modification: standardization, IP weighting, stratification/restriction and matching. Standardization and IP weighting can be used to compute either marginal or conditional effects. Stratification/restriction and matching can only be used to compute conditional effects in certain subsets of the population (Hernan and Robins 2021, 51). Logically this makes sense because standardization and IP weighting can be done using the entire sample, while stratification/matching only use a subset of the population!"
  },
  {
    "objectID": "posts/marginal-conditional/marginal_conditional.html#why-do-we-care",
    "href": "posts/marginal-conditional/marginal_conditional.html#why-do-we-care",
    "title": "Marginal vs Conditional Effects",
    "section": "Why do we care?",
    "text": "Why do we care?\nFinally, why do we care about marginal vs conditional effects at all? Well it matters when we are defining our research question! In order to have a well-defined research question, you need to clearly define what you want to know. For us, it was “does wearing pink pyjamas cause the toothfairy to visit?”."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html",
    "href": "posts/ip-weighting/ip_weighting.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "Re-weighting in this context has nothing to do with weight. Instead it is a statistical method that is used to adjust for confounding to ensure exchangeability. This method can be helpful for answering questions about the marginal or conditional causal effect.\nNow, what is a better way to get started than with probability yet again? In this case, we are going to talk about the probability of receiving treatment.\n\n\n\n\n\n\nNote\n\n\n\nThis post will draw heavily from Chapter 12 of What If (Hernan and Robins 2021)"
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#propensity-score",
    "href": "posts/ip-weighting/ip_weighting.html#propensity-score",
    "title": "Inverse Probability Weighting",
    "section": "Propensity Score",
    "text": "Propensity Score\nA term that is commonly used in clinical epidemiology is the propensity score. The propensity score is the conditional probability of receiving treatment (Hernan and Robins 2021), or in mathematical notation:\n\\[\nPr[A = 1| L = l]\n\\]\nWhere A is treatment and L is the covariate(s) of interest. While equations are good, examples are better. Imagine we have a clinical trial with two groups, wanting to determine if bouncing up and down on a trampoline causes a headache. The treatment group get to bounce on a trampoline for 10 minutes, while the control group have to sit down on a chair for 10 minutes. In this scenario, the propensity score would be the conditional probability of getting to bounce on the trampoline based on your age, sex and color shirt you are wearing."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#does-jumping-on-a-trampoline-cause-a-headache",
    "href": "posts/ip-weighting/ip_weighting.html#does-jumping-on-a-trampoline-cause-a-headache",
    "title": "Inverse Probability Weighting",
    "section": "Does jumping on a trampoline cause a headache?",
    "text": "Does jumping on a trampoline cause a headache?\nWhile theoretical discussions are helpful, numbers always help to really drive a point home. While we can’t conduct a clinical trial, luckily there is a database that has already collected data on such a scenario!\n\n\nCode\nset.seed(2218) # August 18, 2022\n\nlibrary(tidyverse)\nlibrary(rwetasks)\n\ndf.tramp <- data.frame(\n  group = \"trampoline\",\n  age = runif(n = 260, min = 9, max = 25),\n  sex = rbinom(n = 260, size = 1, prob = 0.77),\n  t_shirt = sample(c(\"blue\", \"pink\", \"orange\", \"yellow\"),\n                   size = 260, \n                   replace = TRUE, \n                   prob = c(0.17, 0.46, 0.30, 0.07)),\n  brain_freeze = rbinom(260, size = 1, prob = 0.33), # brain freeze before group\n  time_standing = runif(n = 260, min = 0, max = 60), # time standing in minutes\n  headache = rbinom(260, size = 1, prob = 0.64)\n) \n\ndf.chair <- data.frame(\n  group = \"chair\",\n  age = runif(n = 469, min = 19, max = 88),\n  sex = rbinom(n = 469, size = 1, prob = 0.28),\n  t_shirt = sample(c(\"blue\", \"pink\", \"orange\", \"yellow\"),\n                   size = 469, \n                   replace = TRUE, \n                   prob = c(0.36, 0.16, 0.42, 0.23)),\n  brain_freeze = rbinom(469, size = 1, prob = 0.48), # brain freeze before group\n  time_standing = runif(n = 469, min = 0, max = 60), # time standing in minutes\n  headache = rbinom(469, size = 1, prob = 0.39)\n)\n\ndf <- rbind(df.tramp, df.chair) %>% \n  dplyr::mutate(\n    beta_age = runif(1, min = 0, max = 0.10), \n    beta_sex = runif(1, min = 0, max = 0.10),\n    beta_tshirt = runif(1, min = 0, max = 0.10),\n    beta_ts = runif(1, min = 0, max = 0.10), \n    prob_tramp = (beta_age*age + beta_sex*sex + beta_ts*time_standing)/10,\n    tramp_group = rbinom(729, size = 1, prob = prob_tramp),\n    prob_headache = (beta_age*age + 2*beta_sex*sex + 3*beta_ts*time_standing)/20,\n    headache = rbinom(729, size = 1, prob = prob_headache),\n  )\n\n# Trampoline Group Demographics\n\ndf.tramp.demo <- df %>% dplyr::filter(tramp_group == 1)\n\n# cbind(mean(df.tramp.demo$age), sd(df.tramp.demo$age)) # age\n# rwetasks::count_percent(df.tramp.demo, sex) # sex\n# rwetasks::count_percent(df.tramp.demo, t_shirt) # t-shirt\n# rwetasks::count_percent(df.tramp.demo, brain_freeze) # brain-freeze\n# cbind(mean(df.tramp.demo$time_standing), sd(df.tramp.demo$time_standing)) # time standing\n# rwetasks::count_percent(df.tramp.demo, headache) # headache\n\n# Chair Demographics\n\ndf.chair.demo <- df %>% dplyr::filter(tramp_group == 0)\n\n# cbind(mean(df.chair.demo$age), sd(df.chair.demo$age)) # age\n# rwetasks::count_percent(df.chair.demo, sex) # sex\n# rwetasks::count_percent(df.chair.demo, t_shirt) # t-shirt\n# rwetasks::count_percent(df.chair.demo, brain_freeze) # brain-freeze\n# cbind(mean(df.chair.demo$time_standing), sd(df.chair.demo$time_standing)) # time standing\n# rwetasks::count_percent(df.chair.demo, headache) # headache\n\n\n\n\nTable 1: Trampoline Jumpers vs Chair Sitters\n\n\n\n\n\n\n\n\nTrampoline\n(n = 213)\nChair\n(n = 516)\n\n\n\n\nAge, mean (SD)\n54.2 (22.7)\n34.2 (21.0)\n\n\nFemale, n (%)\n71 (33.3)\n258 (50.0)\n\n\nT-Shirt Color, n (%)\nOrange\nBlue\nYellow\nPink\n89 (41.8)\n52 (24.4)\n39 (18.3)\n33 (15.5)\n179 (34.7)\n119 (23.1)\n67 (13.0)\n151 (29.3)\n\n\nBrain Freeze, n (%)\n100 (46.9)\n212 (41.1)\n\n\nTime Standing (minutes), mean (SD)\n31.0 (16.8)\n30.1 (17.9)\n\n\nHeadache\n116 (37.8)\n102 (24.2)\n\n\n\n\nNow, if our causal question is “Does jumping on a trampoline cause a headache?” we need to look at the two groups that we are comparing. If we review Table 1, do the two groups look similar? Well for starters, the mean age is different. People in the trampoline group are an average age of 54 compared to 34 in the chair sitting group. The other characteristics seem quite different as well. If these two are so different how can we expect to even compare the two?! That’s like comparing apples to coconuts!\nWell, luckily we can use a statistical method known as inverse probability weighting to make those coconuts look more like apples. Think of it like we are painting the coconuts, and focusing more on the ones that are a similar size and shape."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#pseudo-population",
    "href": "posts/ip-weighting/ip_weighting.html#pseudo-population",
    "title": "Inverse Probability Weighting",
    "section": "Pseudo-Population",
    "text": "Pseudo-Population\nThe goal of reweighing is to make a pseudo-population where exchangeability holds. Essentially, in our “make believe” sample the two groups would be comparable. Figure 1 shows the two different groups and the proportion of male to females. Now, we can make these more similar by reweighting them to align with the overall sample (n = 729).\n\n\n\nFigure 1: Before and After Reweighting\n\n\nOne way to do this, is to fit a logistic regression (since the outcome would be a yes/no) to trampoline jumpers. Using this model, we can predict the probability of someone being a trampoline jumper, or conversely the probability that they are not a trampoline jumper. Once we do that, we can compare the two groups. First, we need to go over some basics of IP weighting.\n\n\n\n\n\n\nImportant\n\n\n\nReweighting here is done to estimate the average treatment effect (ATE). Depending upon what the estimand of interest is, the weighting may be different. For example, if you want to estimate the average treatment effect in the treated (ATT), the people in the treated group would receive a weight of 1.0 while those in the control group are reweighted. For more information on choosing the estimand of interest, I recommend reading Greifer and Stuart (2021)."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#ip-weighting",
    "href": "posts/ip-weighting/ip_weighting.html#ip-weighting",
    "title": "Inverse Probability Weighting",
    "section": "IP Weighting",
    "text": "IP Weighting\nThe pseudo-population is created by weighting each individual by the inverse of the conditional probability of receiving the treatment they did actually receive (Hernan and Robins 2021, 151). The formula for this is:\n\\[\nW^a = \\frac{1}{f[A|L]}\n\\]\nFor our example, \\(f[A|L]\\) is the probability of being a trampoline jumper conditional on the measured confounders. In mathematical notation:\n\\[\nPr[A = \\text{trampoline jumpers} | \\text{L = age, sex, time standing}]\n\\]\nNow, from probability we know that the total probability has to equal 1. So:\n\\[\nPr[A = \\text{not trampoline jumpers}|L] = 1 - Pr[A = \\text{trampoline jumpers} | L]\n\\]\nGreat! So basically all we need to do is calculate the probability of being a trampoline jumper given measured confounders then we can calculate the conditional probability of not being a trampoline jumper . Now how do we calculate this conditional probability?\n\nLogistic Regression\nSince our two groups can be thought of as a binary variable, trampoline jumpers or not trampoline jumpers, we get a parametric estimate using logistic regression. Assuming that our model is correct, we can then predict/estimate \\(Pr[A=\\text{trampoline jumper}|L]\\). If no confounding for the effect of A in the pseudo-population and the model is correctly specified, then association is causation and an unbiased estimator of the associational difference in the pseudo-population (Hernan and Robins 2021, 151) :\n\\[\nE[Headache | A = \\text{trampoline jumper}] - E[Headache | A = \\text{not a trampoline jumper}]\n\\]\nis also an unbiased estimator of the causal difference:\n\\[\nE[Headache^{a = \\text{trampoline jumper}}] - E[Headache^{a = \\text{not a trampoline jumper}}]\n\\]"
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#estimating-weights",
    "href": "posts/ip-weighting/ip_weighting.html#estimating-weights",
    "title": "Inverse Probability Weighting",
    "section": "Estimating Weights",
    "text": "Estimating Weights\nNow we are ready to estimate some weights! We will estimate the weights using a logistic regression with the following confounders: age, sex, and time standing.\n\n\n\n\n\n\nNote\n\n\n\nFor this example we have selected age, sex and time standing to be confounders. We know this because it is a simulated dataset however there are approaches for selecting potential confounders. Methods for selecting confounders will be discussed in an upcoming post.\n\n\nUsing that model we will calculate the weights, for trampoline jumpers as:\n\n\\[\n\\hat{W} = \\frac{1}{\\hat{Pr}[A = \\text{trampoline jumper} | L]}\n\\]\nand for not trampoline jumpers as:\n\\[\n\\hat{W} = \\frac{1}{1 - \\hat{Pr}[A = \\text{trampoline jumper} | L]}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(\\hat{W}\\) and \\(\\hat{Pr}\\) are the estimated, or predicted, values. A logistic regression is used in this case because our outcome is binary (trampoline jumper: yes/no). Using this model we can predict the conditional probability which then is used to calculate the weights.\n\n\n\n\nCode\nlibrary(tidyverse)\n\nmod.fit <- stats::glm(formula = tramp_group ~ age + as.factor(sex) + time_standing + I(age ^ 2) + I(time_standing ^ 2), \n                      family = binomial(link = \"logit\"),\n                      data = df)\n\ndf.weights <- df %>% \n  dplyr::mutate(\n    ps = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ip_weights = 1/ps,\n    half_ipw = 0.5/ps\n  )\n\n\nNow we have the weights, let’s check the summary statistics of them.\n\n\nCode\nsummary(df.weights$ip_weights)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.074   1.163   1.430   1.997   2.039  12.130 \n\n\nThe IP weights simulate a pseudo-population where all members of the sample are replaced by two copies of themselves. (Hernan and Robins 2021, 153) One copy receives the treatment value A = 1 and the other copy receives the value A = 0. (Hernan and Robins 2021, 153). The expected mean of the weights should be 2 because all individuals are included both under treatment and under no treatment.\nIf we look back to our example, we can examine the summary statistics of these weights. The mean is sufficiently close to 2, 1.997, which is what we’d expect, however the maximum weight is 12!! For one individual to be weighted as 12, that is rather large. Now there are a few options.\nOne would be to create a pseudo-population similar to what we have done, except using 0.5 for the numerator. That is, the unconditional probability of being a trampoline jumper is 0.5 and the unconditional probability of not being a trampoline jumper is 0.5. In this scenario the pseudo-population would be the same size as the study population, and would be equal to if we used \\(\\frac{1}{f(A|L}\\) but divided all the weights by 2 (Hernan and Robins 2021, 153). We can write this more generally\n\nGeneral Formula\nA general formula for \\(W^a\\) is: (Hernan and Robins 2021, 153).\n\\[\nW^a = \\frac{p}{f[A|L]}\n\\]\nwhere \\(p\\) is the unconditional probability of treatment. Note: \\(0< p \\leq 1\\), whereas \\(f[A|L]\\) is the probability of treatment based on covariates L. An alternative is for different people to have different probabilities (Hernan and Robins 2021, 153). A common choice is to use \\(Pr[A = 1]\\) for \\(p\\) in the treated and \\(Pr[A=0]\\) for \\(p\\) in the untreated. \\(Pr\\) in this case would just be the proportion. Using our example again, \\(Pr[A = \\text{trampoline jumpers}] = \\frac{213}{729} = 0.292\\) and for the not trampoline jumpers, \\(Pr[A = \\text{not trampoline jumper}] = \\frac{516}{729} = 0.708\\). If we use these values for the numerator in calculating the weights for our example:\n\n\nCode\nlibrary(tidyverse)\n\ndf.weights <- df %>% \n  dplyr::mutate(\n    ps = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ip_weights = dplyr::case_when(\n      tramp_group == 1 ~ 0.292/ps,\n      tramp_group == 0 ~ 0.708/ps\n  )\n  )\n  \nsummary(df.weights$ip_weights)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4122  0.7843  0.8460  0.9997  1.0805  3.5420 \n\n\nThis is notably different from when we used \\(\\frac{1}{f[A|L]}\\). Now the weights range from 0.412 to 3.54 whereas before they ranged from 1.07 to 12.1. Not only that, but now in the pseudo-population, the ratio of trampoline jumpers to not trampoline jumpers is kept. The stabilizing factor, \\(f[A]\\), is responsible for the narrower range (Hernan and Robins 2021, 153). Weights that use the stabilizng factor are referred to as stabilized weights."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#stabilized-weights",
    "href": "posts/ip-weighting/ip_weighting.html#stabilized-weights",
    "title": "Inverse Probability Weighting",
    "section": "Stabilized Weights",
    "text": "Stabilized Weights\n\\[\nSW^a = \\frac{f(A)}{f[A|L]}\n\\]\nFrom the above section, we saw that the stabilizing factor made our weights have a narrower range. The mean of the stabilized weights is also expected to be 1 because the size of the pseudo-population is equal to the study population (Hernan and Robins 2021, 153). This is important to check when conducting an analysis using stabilized weights. An alternative to using the nonparametric estimator (i.e., the proportion of individuals), is to estimate \\(f[A]\\) using the same model but with an intercept and no covariates. If we do that using our example:\n\n\nCode\nlibrary(tidyverse)\n\nmod.intercept <- stats::glm(formula = tramp_group ~ 1, \n                      family = binomial(link = \"logit\"),\n                      data = df)\n\ndf.weights <- df %>% \n  dplyr::mutate(\n    ps.denominator = dplyr::case_when(\n      tramp_group == 1 ~ predict(mod.fit, \n                                    type = \"response\"),\n      tramp_group == 0 ~ (1 - predict(mod.fit, \n                                    type = \"response\"))\n    ),\n    ps.numerator = predict(mod.intercept, type = \"response\"),\n    ip_weights = dplyr::case_when(\n      tramp_group == 1 ~ ps.numerator/ps.denominator,\n      tramp_group == 0 ~ (1-ps.numerator)/(ps.denominator)\n  )\n  )\n  \nsummary(df.weights$ip_weights)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4125  0.7841  0.8458  0.9997  1.0803  3.5442 \n\n\nNow we have weights with a mean that is close to 1 with a maximum of 3.54. There is still one more thing that has to be checked whenever using these weights: the positivity assumption! We have to ensure that all participants have a greater than 0 probability of being a trampoline jumper! After checking this, we are confident that twe can use these weights.\n\n\n\n\n\n\nNote\n\n\n\nWhile checking the summary statistics and positivity assumption are important, it is always a good idea to additional check:\n\nBalance has been achieved for the variables that were included in the model\nThe distribution of weights, typically done graphically\nCheck the higher order moments. For example, don’t just check to see if mean is similar for a continuous variable, but that standard deviation is similar as well.\n\n\n\nNow which method would we prefer for estimating the weights? Well this comes down to opinion. For this example, both methods give very similar results for the weights. My personal preference is to use the parametric estimated weights, using logistic regression, for both the numerator and denominator rather than a parametric estimator for the denominator and nonparametric for the numerator."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#stabilized-or-nonstabilized",
    "href": "posts/ip-weighting/ip_weighting.html#stabilized-or-nonstabilized",
    "title": "Inverse Probability Weighting",
    "section": "Stabilized or Nonstabilized?",
    "text": "Stabilized or Nonstabilized?\nAt this point, you may be wondering to yourself if we should be using stabilized or nonstabilized weights. One reason is stabilized weights result in narrower 95% CIs (Hernan and Robins 2021, 154). However, this only occurs when the model is not saturated. A model is saturated when the number of parameters equals the number of quantities to be estimated. For example, \\(E[Y|A] = \\beta_0 + \\beta_1*A\\) is a saturated model because it has two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), and two quantities to estimate \\(E[Y|A = 1]\\) and \\(E[Y|A = 0]\\) (Hernan and Robins 2021, 151). Keep in mind this example is for a binary variable, however this becomes nearly impossible to meet for continuous variables since you would need to have a parameter for every value of that variable."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#did-it-work",
    "href": "posts/ip-weighting/ip_weighting.html#did-it-work",
    "title": "Inverse Probability Weighting",
    "section": "Did it work?",
    "text": "Did it work?\nFor any adjustment technique that aims to achieve balance, such as IPTW, entropy balancing, weights used as part of matching adjusted indirect comparisons (MAICs), we need to check to see if balance has actually been met. In this case, we were trying to balance on the confounders age, sex and time standing. We can now check to see if this is achieved.\n\n\nCode\nlibrary(tidyverse)\nlibrary(Hmisc)\n\ntramp.wt <- df.weights %>% dplyr::filter(tramp_group == 1)\n\n# Age \nx <- tramp.wt$age\nwt <- tramp.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Sex\n\n# df.weights %>% \n  # dplyr::filter(tramp_group == 1) %>% \n  # count(sex, \n        #wt = ip_weights)\n\n# Time Standing\n\nx <- tramp.wt$time_standing\nwt <- tramp.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Chair Sitters\n\nchair.wt <- df.weights %>% dplyr::filter(tramp_group == 0)\n\n\n# Age \nx <- chair.wt$age\nwt <- chair.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n# Sex\n\n# df.weights %>% \n#   dplyr::filter(tramp_group == 0) %>% \n#   count(sex, \n#         wt = ip_weights)\n\n# Time Standing\n\nx <- chair.wt$time_standing\nwt <- chair.wt$ip_weights\n\n# wtd.mean(x, wt)\n# sqrt(wtd.var(x, wt)) # std dev = sqrt(var)\n\n\n\n\nTable 2: Characteristics After Reweighting\n\n\n\n\n\n\n\n\n\nTrampoline Jumpers\n(n = 213)\nChair Sitters\n(n = 516)\nOverall (unweighted)\n(n = 729)\n\n\n\n\nAge, mean (SD)\n40.1 (23.5)\n40.2 (23.4)\n40.1 (23.3)\n\n\nFemale, n (%)\n92 (43.9)\n231 (44.8)\n329 (45.1)\n\n\nTime Standing, mean (SD)\n32.0 (17.7)\n30.5 (17.6)\n30.4 (17.6)\n\n\n\n\nTable 2 shows the characteristics that we included in the regression model after reweighting using stabilized weights. The mean of both groups is now similar, which consequently is also close to the overall unweighted sample. If these values don’t look any different to you, then compare them to those show in Table 1."
  },
  {
    "objectID": "posts/ip-weighting/ip_weighting.html#drumroll-please",
    "href": "posts/ip-weighting/ip_weighting.html#drumroll-please",
    "title": "Inverse Probability Weighting",
    "section": "Drumroll Please!",
    "text": "Drumroll Please!\nNow we are finally ready to answer our question! We will use a logistic regression since our outcome is binary, headache: yes/no, with our new fancy weights!\n\n\n\n\n\n\nNote\n\n\n\nTo determine the 95% CI we need to use a method that takes the IP weighting into account (Hernan and Robins 2021, 152). One approach is to use nonparametric bootstrapping. Another approach is to use the robust variance estimator. Here we will use the robust variance estimator, however it is important to note that the robust variance estimator is conservative since it covers the super-population parameter more than 95% of the time. (Hernan and Robins 2021, 152)\n(Thanks to Giusi Moffa for pointing out I should be more clear about the CIs!)\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(sandwich)\n\nmod.outcome <- stats::glm(\n  formula = headache ~ tramp_group,\n  family = binomial(link = \"logit\"),\n  data = df.weights,\n  weights = ip_weights\n)\n\nsandwich_se <- diag(sandwich::vcovHC(mod.outcome, type = \"HC\"))^0.5\n\noutput <- broom::tidy(mod.outcome) %>% \n  dplyr::filter(\n    term == \"tramp_group\"\n  ) %>% \n  dplyr::mutate(\n    log.upper.ci = estimate + qnorm(0.975)*0.221,\n    log.lower.ci = estimate - qnorm(0.975)*0.221,\n    OR = exp(estimate),\n    upper.ci = exp(log.upper.ci),\n    lower.ci = exp(log.lower.ci)\n  )\n\npaste0(\"OR, \", round(output$OR,3), \";\",\n       \" (95% CI, \", round(output$lower.ci, 2), \"-\", \n       round(output$upper.ci,2), \")\")\n\n\n[1] \"OR, 1.001; (95% CI, 0.65-1.54)\"\n\n\nUsing our stabilized weights that we calculated earlier, we can now answer our causal question: “Does jumping on a trampoline cause headaches?”. Based upon our model, the answer is no since our 95% CI includes unity, aka 1. We are now free to jump on trampolines without the worry of headaches!!"
  },
  {
    "objectID": "posts/standardization/standardization.html",
    "href": "posts/standardization/standardization.html",
    "title": "Standardize Your Way to Causal Inference",
    "section": "",
    "text": "Imagine that a teacher wants to know if giving a lesson about growing plants causes plants to grow longer. They decide to use two groups of peoples: kids (group A) and their parents (group O). The kids (Group A) received a lesson about growing plants, while the parents (group O) did not. After receiving the lesson, each person got a plant and grew it. After 12 weeks, the plants length was measured.\n\nWhen the teacher looks at the plants grown, they conclude that group O are better at growing plants. That’s when a kid runs up huffing and puffing “You can’t compare those! That’s like comparing apples and oranges” when another kid chimes in “If we didn’t water them as much then we’d have done way better than them!”\n\nNow they have a point, watering a plant will certainly have an impact on how the plants grow, as will the sunlight, soil and other important factors. “So what do we do? Throw out this experiment?” the teacher asks the parent. “Not so fast! We can standardize these and then compare the groups!” you exclaim. So how exactly do we standardize to compare Groups A and O?\n\n\nAlright so we have our question, “Does getting a lesson in plant growing cause your plants to grow longer?”, but how exactly do we apply that in this case? Well first we need to have a look at the data and see what we have.\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(1513) # picked based on runif(1, min = 0, max = 2023)\n\ngrp_a <- data.frame(\n  age = rnorm(n = 116, mean = 10, sd = 3),\n  indoors = rbinom(n = 116, size = 1, prob = 0.3), \n  hrs_sunlight = runif(n = 116, min = 2, max = 6), \n  watered = rbinom(n = 116, size = 1, prob = 0.4),\n  group = 1\n) \n\ngrp_o <- data.frame(\n  age = rnorm(n = 116, mean = 30, sd = 7),\n  indoors = rbinom(n = 116, size = 1, prob = 0.8), \n  hrs_sunlight = runif(n = 116, min = 8, max = 12), \n  watered = rbinom(n = 116, size = 1, prob = 0.6),\n  group = 0\n) \n\ndf = rbind(grp_a, grp_o) |> \n  dplyr::mutate(\n    plant_length = rnorm(n = 232, mean = 1.5*group + 0.2*age + 0.3*indoors + 0.8*hrs_sunlight - 0.5*watered, sd = 1)\n  ) \n\n\n\n\n\n\n\n\n\n\nCharacteristic\nGroup A\n(n = 116)\nGroup O\n(n = 116)\n\n\n\n\nAge, Mean (SD)\n10 (2.74)\n30.04 (6.12)\n\n\nIndoors, n (%)\n34, (30.2%)\n99 (85.3%)\n\n\nHours in Sunlight, Mean (SD)\n3.77 (1.23)\n10.02 (1.18)\n\n\nAdequately watered, n (%)\n46, (39.7%)\n59, (50.9%)\n\n\nLength of plant, Mean (SD)\n6.52 (1.58)\n14.2 (1.89)\n\n\n\nAlright so now we have our data and know the age of each person, how many plants were grown indoors in each group, the hours in the sunlight of each plant, whether the plant was adequately watered and the length of each plant. So what? Well we need to standardize these groups.\n\nYou may be thinking of standardization in terms of other methods, that are not necessarily statistical per se. For example, say you weighed three rocks: one is 110 kilograms, one is 167 pounds and one is 25 stones. How could you possibly compare these three? They are in different units! You may be thinking to yourself “well, we just convert them all to the same units duh”…exactly! You are standardizing the units. In this case we are doing something similar."
  },
  {
    "objectID": "posts/standardization/standardization.html#standardization-for-causal-inference",
    "href": "posts/standardization/standardization.html#standardization-for-causal-inference",
    "title": "Standardize Your Way to Causal Inference",
    "section": "Standardization for Causal Inference",
    "text": "Standardization for Causal Inference\nStandardization models the outcome, whereas inverse probability weighting (IPW), models the treatment (Hernan and Robins 2021). More formally, if we were to write it as an equation, assuming that no individuals are censored (C=0) (Hernan and Robins 2021, 162) :\n\\[\n{\\sum_{l}E[Y|A = a, C=0, L=l]}  \\times Pr[L=l]\n\\]\nNow, in an ideal world we’d be able to calculate this nonparametrically. To do that, we could calculate the mean outcome in each stratum \\(l\\) of the confounders \\(L\\). So in our case we could look at one strata as the individuals who are 5 years old, grew their plant indoor, had their plant in the sunlight for 2 hours and adequately watered their plant. We could do this for each strata, then take the weighted mean sum using the above formula.\n\n\n\n\n\n\nWhat if L is continuous?\n\n\n\nIf \\(L\\) is continuous in the above formula, then we need to replace \\(Pr[L=l]\\) with the probability density function \\(f_{L}[l]\\) instead (Hernan and Robins 2021, 162).\n\n\nNow, as you can imagine that is a lot of work especially when the variable is continuous. You can probably imagine that this isn’t always possible, especially when dealing with real-world data or many covariates. As the number of covariates increases, so does the number of strata. Luckily for us, we have handy dandy modelling in our statistical toolbox!"
  },
  {
    "objectID": "posts/standardization/standardization.html#standardizing-the-mean-outcome-to-the-confounder-distribution",
    "href": "posts/standardization/standardization.html#standardizing-the-mean-outcome-to-the-confounder-distribution",
    "title": "Standardize Your Way to Causal Inference",
    "section": "Standardizing the Mean Outcome to the Confounder Distribution",
    "text": "Standardizing the Mean Outcome to the Confounder Distribution\nBefore we get started, we need to go over the four steps. I’ll keep it brief here, but the steps are as follows (Hernan and Robins 2021, 164):\n\nExpansion of data set\nModelling\nPrediction\nStandardization by averaging\n\n\nExpansion of data set\nBefore starting any analysis, we first need data. Here we are going to create three data sets. The first will be the observed data, aka the original data set. We will then created another data set that is very similar to the first, except the group will be set to 0 (no lesson) and we will consider the outcome as missing. You can probably guess what the third data set will be….bingo! Same as the second data set but with group as 1 (lesson).\n\n\nCode\ndf_og <- df\n\ndf_grp0 <- df |> \n  dplyr::select(\n    -plant_length, -group\n  ) |> \n  dplyr::mutate(\n    group = 0\n  )\n\ndf_grp1 <- df |> \n  dplyr::select(\n    -plant_length, -group\n  ) |> \n  dplyr::mutate(\n    group = 1\n  )\n\n\n\n\nModelling\nNow we can get to the fun stuff! Since our outcome is continuous, we decide to use a generalized linear model with the Gaussian distribution. We’ll only be able to use the original data set, since that is the one with the measured outcomes.\n\n\n\n\n\n\nModel Assumptions\n\n\n\nAs a side note, it is important to check the assumptions for the model you are using. For the sake of this post, I won’t since I don’t want to distract from the goal of explaining standardization but in practice should always check the assumptions for the model you are using (i.e., in this case a GLM).\n\n\n\n\nCode\nfit <- glm(\n  plant_length ~ group + age + indoors + hrs_sunlight + watered, \n  family = gaussian(link = \"identity\"), \n  data = df\n)\n\n\n\n\nPrediction\nUsing our handy dandy model, we will now predict the outcome for both data sets with missing outcome data. First we predict the outcome for the data set if everyone were in group O. Next, we predict the outcome for the data set if everyone were in group A.\n\n\nCode\npred0 <- predict(fit, df_grp0) |> as.data.frame() |> \n  rename(plant_length = `predict(fit, df_grp0)`)\n\npred1 <- predict(fit, df_grp1) |> as.data.frame() |> \n  rename(plant_length = `predict(fit, df_grp1)`)\n\n\n\n\nStandardization by averaging\nNow that we have predicted the outcomes, we can calculate the average outcome for each data set. Before we calculate this, you might be asking about the uncertainty for this measurement, which I’m glad you asked! To get 95% confidence intervals for this, we can use bootstrapping.\n\n\n\nCode\n# A special thanks to Joy Shi, Sean McGrath and Tom Palmer for providing this code for free. Link here: https://remlapmot.github.io/cibookex-r/standardization-and-the-parametric-g-formula.html#program-13.4\n\nlibrary(boot)\n\n# Function, altered from the above link for our use \n\nstandardization <- function(data, indices) {\n  \n  # We need to first make the datasets that we need\n  \n  # 1st copy: our original\n  \n  d <- data[indices, ] \n  \n  # 2nd copy: Same as original but with group = 0 and outcome as missing\n  \n  d0 <- d \n  d0$group <- 0 # setting group to 0\n  d0$plant_length <- NA # setting plant length (our outcome) to missing \n  \n  # 3rd copy: Same as original but with group = 1 and outcome as missing\n  \n  d1 <- d \n  d1$group <- 1 # setting group to 1\n  d1$plant_length <- NA # setting plant length (our outcome) to missing \n  \n  # Making one sample\n  \n  d.onesample <- rbind(d, d0, d1) # combining datasets\n  \n  # Fitting a model for each iteration\n  \n  fit <- glm(\n    plant_length ~ group + age + indoors + hrs_sunlight + watered,\n    data = d.onesample\n  )\n  \n  # Using model to predict the outcome\n  \n  d.onesample$predicted_meanY <- predict(fit, d.onesample)\n  \n  # Calculate the mean for each of the groups. The third calculation is for the difference in group O (0) vs group A (1)\n  \n  return(c(\n    mean(d.onesample$predicted_meanY[d.onesample$group == 0]),\n    mean(d.onesample$predicted_meanY[d.onesample$group == 1]),\n    # Treatment - No Treatment\n    \n    mean(d.onesample$predicted_meanY[d.onesample$group == 1]) -\n    mean(d.onesample$predicted_meanY[d.onesample$group == 0])\n  ))\n}\n\n\n# Now we can bootstrap this statistic using our dataset\n\nresults <- boot(data = df,\n                statistic = standardization,\n                R = 5)\n\n# Using the bootstrapped sample (titled result), we can calculate the confidence intervals. We take the standard deviation of the sampling distribution. This will give us our standard error\n\nse <- c(sd(results$t[, 1]),\n        sd(results$t[, 2]),\n        sd(results$t[,3]))\n\n\nmean <- results$t0 # calculate mean \nll <- mean - qnorm(0.975) * se # lower limits\nul <- mean + qnorm(0.975) * se # upper limits\n\nfinalresults <- data.frame(\n  result_title = c(\"Group O\", \"Group A\", \"Difference\"),\n  mean = round(mean,3),\n  ci = paste0(\"95% CI: \", round(ll, 3), \" - \", round(ul, 3))\n)\n\n\nFinally we have our results! We end up with a mean difference of -1.43 (95% CI: -0.96 to -1.90). Finally, we can put to rest that the people who received the lesson (the kids) grew shorter plants than those who didn’t have the lesson. Feeling smug, the parents start to gloat before one of the kids walks up and casually asks “Are these really valid? What kind of assumptions are you making?”"
  },
  {
    "objectID": "posts/standardization/standardization.html#assumptions-for-standardization",
    "href": "posts/standardization/standardization.html#assumptions-for-standardization",
    "title": "Standardize Your Way to Causal Inference",
    "section": "Assumptions for Standardization",
    "text": "Assumptions for Standardization\nThe kid brings up an excellent point, one that we should always keep in mind when trying to make a causal inference. So when are these estimates valid? We can group the assumptions into three groups: identifability conditions, measurement of variables and specification of the model (Hernan and Robins 2021, 168). The identifability conditions are exchangeability, positivity and consistency. Positivity can be checked empirically but the other two are opinion based. For a more detailed version of these, check out my other blog post “Intro to Causal Inference”.\nThe second condition is the variables used in the analysis need to be correctly measured. Measurement error in the treatment, outcome or the confounders will generally result in bias (see chapter 9 of Hernan and Robins (2021) for more specifics).\n\nFinally, all models need to be correctly specified (see chapter 11 of Hernan and Robins (2021) for more specifics). Of course, all of these will never hold perfectly but some remain a matter of judgement which means it can be open to criticism. It’s important to keep these assumptions in mind and the validity of them. For example, a critique in our example could be that our intervention is not well-defined (which I’d agree with and we could make our intervention definition more defined).\nWe need to make sure all of these conditions hold, at least in approximately since in the real-world it is difficult (i.e., in practice there is most likely some form of model misspecification). Some of these assumptions are based on judgement, which is important to note.\n\n\n\n\n\n\nPositivity Assumption\n\n\n\nThe positivity assumption is a key assumption for causal inference. Simply put, no individual can have a probability of 0 for the treatment. Now, for standardization, we can still use this method if this assumption isn’t met. However, we need to be willing to reply on extrapolation (parametric extrapolation to be exact) that will smooth over the strata for those with a probability of 0 of receiving treatment. If we do this, we’ll introduce bias, specifically the 95% CIs will cover the true effect less than 95% of the time. For more details, please see (Hernan and Robins 2021, 162)."
  },
  {
    "objectID": "posts/standardization/standardization.html#what-about-other-measures",
    "href": "posts/standardization/standardization.html#what-about-other-measures",
    "title": "Standardize Your Way to Causal Inference",
    "section": "What about other measures?",
    "text": "What about other measures?\nOur example used a continuous outcome, so we used risk difference as our causal estimand. Of course, there are other causal estimands of interest as well. I won’t bore you with another example here (maybe in the future). I’d highly recommend checking out Lee and Lee (2022) if you are interested in other estimands including relative risk and odds ratios. The process is very similar."
  },
  {
    "objectID": "posts/standardization/standardization.html#ip-weighting-or-standardization",
    "href": "posts/standardization/standardization.html#ip-weighting-or-standardization",
    "title": "Standardize Your Way to Causal Inference",
    "section": "IP Weighting or Standardization?",
    "text": "IP Weighting or Standardization?\nIf we were to do both without using any models (i.e., nonparametrically), then we would expect both methods to give the exact same result. This is because they are modelling different things (treatment for IPW, outcome for standardizaiton) and we can always expect some level of misspecification of a model in practice. So if they don’t give the same answer then how do we pick? The short answer is we don’t.\n\nLarge differences between them will let us know that there is some serious misspecificaiton in at least one of the estimates. Small differences may still indicate there’s a problem but not as serious misspecification.\n\nBasically what I’m trying to say is when both methods can be used, just use both."
  },
  {
    "objectID": "posts/standardization/standardization.html#the-g-formula",
    "href": "posts/standardization/standardization.html#the-g-formula",
    "title": "Standardize Your Way to Causal Inference",
    "section": "The G-Formula",
    "text": "The G-Formula\nEnough teasing already! What does the parametric g-formula have to do with this!! You may be screaming at your screen right now. If you are, I don’t blame you. Alright, so first the equation for the g-formula is formally expressed as (Naimi, Cole, and Kennedy 2017):\n\\[\nE(Y^{a_0, a_1}) = \\sum_{z_1}E(Y|A_1 = a_1, Z_1 = z1, A_0 = a_0)P(Z_1=z_1|A_0=a_0)\n\\]\nAt this point you may be wondering “What in the world does this formula have to do with standardization?”. If you are thinking that, props to you! (side note: I personally always like to ask these types of questions). The answer is this is how we calculate our expected values (E). The g-formula has a broad use, specifically for time-varying applications. In our case, we can just ignore \\(A_1\\) since that refers to the point in time (we only have one time point for our example).\n\nSince we already have our values, from standardization, we are going to plug-in those values to this formula (technically we already did this, but you get the point). We say that we are using a plug-in g-formula estimator since we just…you guessed it….plug in the values! Standardization is a plug-in g-formula estimator however since, in our case, the estimate came from a parametric model we call it the parametric g-formula."
  },
  {
    "objectID": "posts/standardization/standardization.html#what-next",
    "href": "posts/standardization/standardization.html#what-next",
    "title": "Standardize Your Way to Causal Inference",
    "section": "What Next?",
    "text": "What Next?\nNow you are free to go standardize away out there in the real world!"
  },
  {
    "objectID": "posts/welcome/welcome.html",
    "href": "posts/welcome/welcome.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Overview\nThis is my first post in this blog! The aim of this blog is to describe methods related to real-world evidence, causal inference and statistical fun. My goal for this is to elaborate on methods I’ve come across while reading and hopefully be able to teach one person something new, or in a more intuitive way. A huge credit to the Count Bayesie blog (@willkurt on twitter). Reading his blog posts encouraged me to do the same.\nPlease note: we are all human. Sometimes due to a lack of sleep, caffeine, or being distracted everyone makes mistakes (myself included). If you notice a mistake on this blog, please reach out to me so I can correct it."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html",
    "href": "posts/or_rr_hr/or_rr_hr.html",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "",
    "text": "Let’s start with probability because that’s probably a good place to start. Probability is basically how likely something is to occur. For example, what are the chances that my favorite show will come on TV today? Or how likely is it that it will rain today? Based upon these probabilities you can make informed decisions. The neat thing about probability, is that you can incorporate it into different measures when conducting scientific research. An example of this is the odds that something will happen, similar to how a betting line works when gambling.\n\nTo convert from probability to odds, the formula is the probability of something happening divided by the probability of that something not happening. In mathematical terms:\n\\[\nodds = p/(1-p)\n\\]\nAn example always helps to contextualize the principles. What are the odds that my favorite candy will be available at the movie theatre tonight? Well, if the probability (p) is 0.70 that it will be there tonight, then the odds are:\n\\[\nodds = (0.70) / (1-0.70) = 0.70/0.30 = 2.33\n\\]\nThis means that the odds are 2.33:1 that my candy will be there tonight. Hooray!\nOdds are useful, however often times in research we want to know how the ratio of these two odds can be compared. For example, are women more likely to have a heart attack than men? To answer this, we turn to odds ratios (OR)."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html#odds-ratios",
    "href": "posts/or_rr_hr/or_rr_hr.html#odds-ratios",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "Odds Ratios",
    "text": "Odds Ratios\nOdds ratios essentially take two different odds, as explained above, and compared them to each other. For example, we have the below table (note: these numbers were simulated, not from an actual study). So the odds of men with a heart condition is 87/176 = 0.49, compared to women with a heart condition. If we do the same for the subgroup of patients with no heart condition: 101/147 = 0.69.\n\nNow, these odds are great but really we want to know, how do men and women compare? Well, we take the ratio of these two values. 0.49/0.69 = 0.71 and…dramatic pause… now you have an odds ratio!\n\n\nTable 1: Contingency Table for Heart Attack by Sex\n\n\n\nMen\nWomen\nTotal\n\n\nHeart Attack\n87\n176\n263\n\n\nNo Heart Attack\n101\n147\n248\n\n\nTotal\n188\n323\n511\n\n\n\n\nThe interpretation for this would be that men have a decreased odds of having a heart attack than women. If this is hard to interpret, personally I find it easier to contextualize ORs > 1 to other people, you can take the reciprocal of the OR and flip what is called the reference group. So women have increased odds, 1.41, of having a heart attack (1 / 0.71 = 1.41)."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html#relative-risks",
    "href": "posts/or_rr_hr/or_rr_hr.html#relative-risks",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "Relative Risks",
    "text": "Relative Risks\nRelative risks (RR) are similar to odds ratios, however the way they are calculated are different. Using the above example, Table 1, we can calculate the relative risk. The formula for relative risk is\n\\[\nRR = (87/263) / (101/248) = 0.33/0.407 = 0.81\n\\]\nAgain, this means that men with a heart condition have a decreased risk compared to women. It is important to note here that the relative risk is specific to the study that was conducted. In our example, RR = 0.81 for the 511 people included in our study.\n\nThis is a very important distinction to make because people have varying degrees of baseline risk. As such, sometimes ORs and RRs are used interchangeably however they are distinctly different. In an upcoming post, I will elaborate on transportability."
  },
  {
    "objectID": "posts/or_rr_hr/or_rr_hr.html#hazard-ratios",
    "href": "posts/or_rr_hr/or_rr_hr.html#hazard-ratios",
    "title": "OR/RR/HR: What’s the Difference?",
    "section": "Hazard Ratios",
    "text": "Hazard Ratios\nHazard ratios are typically reported from time-to-event analyses, whereas odds ratios and relative risks are calculated from binary events: yes/no. Due to this, hazards cannot be explained quite as simple using a table, however that isn’t a problem.\nA hazard is the probability that an individual has an event at that time t Clark et al. (2003). Alternatively, it represents the instantaneous event rate for an individual. This is commonly used in survival analyses, which we will touch on in a later post, however it doesn’t have to be for survival (sometimes a common misconception). The hazard is for the event. Since it is a time-to-event measure, it could be used to determine the probability that my socks will instantaneously fall off. The typical notation for a hazard is \\(h(t)\\) or \\(\\lambda(t)\\)\nAs with the previous two ratios, it is a ratio. So if we use the same heart attack example, Table 1, except this time we are analyzing the time-to-heart attack then we can calculate a hazard for each group. Unfortunately, the math is not quite as straightforward so I’ll save you the pain. Using an appropriate model, a subsequent post will touch on models for time-to-event analysis but a common methods you may have heard of is Cox Proportional Hazards Model (note: if proportional hazards assumption is held, then hazard is cumulative Clark et al. (2003).\nUsing this imaginary model, we calculated the hazard for men \\(h(men)\\) as 0.20 and the hazard for women \\(h(women)\\) as 0.369. Now, we take the ratio of these two hazards and….*I’ll pause while you roll your eyes*….bingo! We have a hazard ratio (HR):\n\\[\nHR = h(men)/h(women) = 0.20/0.369 = 0.542.\n\\]\nThe interpretation for this is that men, yet again, have a lower probability of having an instantaneous heart attack than women (HR = 0.542)."
  },
  {
    "objectID": "posts/p-value-power/p-value-power.html",
    "href": "posts/p-value-power/p-value-power.html",
    "title": "P-Values and Power: Please Explain",
    "section": "",
    "text": "P-values are a term commonly heard in scientific literature, however often they are misconstrued, misunderstood or misinterpreted. The p in p-value stands for probability, as a lot of statistics begins with. What exactly does it tell us? Let’s find out\n\n\n\n\nHypotheses are the best place to start for any science experiment or project. A hypothesis is what you are wanting to know. First you need to start with your research question. For example, are people who love lollipops older than people who do not? To do this, formally in statistics, we need a null hypothesis (\\(H_0\\)) and an alternative hypothesis (\\(H_A\\)). We will use two abbreviations here: LL for lollipop lickers and NLL for not lollipop lickers. We want to know “Do lollipop lickers have more sore tongues than those who do not lick lollipops?”\nRe-phrasing this in statistically terms, \\(H_0\\) is that people do not have more sore tongues if they lick lollipops compared to if they do. The question now is what is our alternative hypothesis? Well there are three options to choose from:\n\nWe think LL have more sore tongues than NLL\nWe think LL have less sore tongues than NLL\nWe don’t know, but we don’t think they are the same!\n\n1 and 2 are what are called one-sided hypothesis. A one-sided hypothesis, means you are assuming the difference is bigger or smaller. A two-sided hypothesis means we simply don’t know! We will assume we don’t know and use \\(p\\) to denote the proportion of lollipop sore tongues.\n\\[\nH_0: p_{LL-ST} = p_{NLL-ST}\n\\]\n\\[\nH_A: p_{LL-ST} \\neq p_{NLL-ST}\n\\]\nNow, before we continue we need to review how okay we are with being wrong. Being wrong is okay, but there are two different types of being wrong.\n\n\n\nA type I error, sometimes referred to as \\(\\alpha\\) error, is the error of saying something happened when it did not. For example, the error of me assuming you bought a balloon when you did not. A type II error, sometimes referred to as \\(\\beta\\) error, is the error of saying something did not happen when it did. Using our balloon example, me assuming you didn’t buy a balloon when you did.\n\n\n\nThe power of a statistical test is the probability that it correctly rejects the null hypothesis (Gelman and Carlin 2014). It is tied to the type II error, through the below formula.\n\\[\nPower = 1 - \\beta\n\\]\nRemember our lollipop example? For that it would be the probability that we can say lollipop lickers do not have the same amount of sore tongues as non lollipop lickers.\n\n\n\nAlright, I know at this point I’m boring you. One last definition and then we can get to the good stuff. A test statistic is exactly what it sounds like. A statistic used for tests! What kind of tests you may ask? Hypothesis tests like we have above. Now, the test statistic in itself may follow a variety of distributions depending on the test. I’ll leave distributions to another post.\n\n\n\nThe probability of such an extreme value of the test statistic occurring if the null hypothesis were true is often called the P-Value (Bland 2015).\n\n\n\nRemember \\(\\alpha\\) from before? The type I error is sometimes referred to as the significance level. The number that is picked for the type I error is usually 0.05, however it is completely arbitrary to pick 0.05. It could be 0.20 or 0.02.\n\n\n\n\n\n\nImportant\n\n\n\nThere is no such thing as more significant. It is either significant at the level that was pre-specified before the analysis or not. For example: a p-value of 0.01 is not more significant than a p-value of 0.05\n\n\n\n\n\n\nFinally, after all those boring definitions we are back to where we started. How are we going to determine whether people who are lollipop lickers have more sore tongues? This requires us to do a bit more thinking to determine the right statistical tool for the toolbox.\nLet’s think about this question a bit more first. Lollipop lickers tend to be younger right? Do boys like lollipops more than girls? Let’s assume that both of those things are true. We will want to control for both of those variables. Control in this sense, means adding the variable in the equation like below.\n\\[\nST = LL + age + sex\n\\]\nNow, the type of regression we will use is a generalized linear model (GLM). Generalized here just means that you can apply to almost any situation, hence general. Another post will cover the differences between GLMs and regular old regression (OLS).\n\n\n\nNow we know our question, have our tool and want to find out the answer. What are we missing…..the most important thing of all! Data! Luckily, there is a database that collects such data. It has data on 422 people. Below are the summary statistics for our groups\n\n\n\n\n\n\n\n\n\nSore Tongues\n(n = 181)\nNot Sore Tongues\n(n = 241)\n\n\n\n\nLollipop Lickers, n (%)\n146 (80.7%)\n60 (24.9%)\n\n\nAge, mean (SD)\n17.9 (7.30)\n48.0 (17.8)\n\n\nFemale, n (%)\n107 (59.1%)\n52 (21.6%)\n\n\n\n\n\n\nUsing our handy dandy formula from above, and our tool from our toolbox (GLM). Below are the results.\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nSE\nStatistic\n(Z-Value)\nPr(>|z|)\n(P-Value)\n\n\n\n\n(Intercept)\n6.31\n1.20\n5.25\n1.53e-7\n\n\nLL\n2.77\n0.484\n5.73\n1.00e-8\n\n\nAge\n-0.317\n0.0499\n-6.36\n2.04e-10\n\n\nSex\n2.15\n0.493\n4.36\n1.32e-5\n\n\n\n\n\n\nThe test statistic for regression is defined by \\(\\beta / SE(\\beta)\\). Beta here is the coefficient. For example, the test statistic for age is (note, close to -6.36. We will chalk up the difference to a rounding error):\n\\[\n-0.317/0.0499 = -6.35\n\\]\nNow, this number on it’s own is pretty useless. We want to know, what is our p-value?! Well, to do that we turn to the normal distribution.\n\n\n\nUsing the normal distribution, since our sample size is bigger than 30 (a wonderfully random number. Honestly, I have no idea why 30 is the cutoff), we can determine our almighty p-value.\n\n\nCode\n2*pnorm(-6.358)\n\n\n[1] 2.043975e-10\n\n\nTa-da! We have our p-value of 2.04e-10. This is less than our pre-specified value of 0.05. What does it mean though? From earlier, it is the probability of such an extreme value of the test statistic (-6.35) occurring if the null hypothesis were true. Finally, we can use significant in our interpretation right? Technically yes, however there are some strong caveats we need to go over first\n\n\n\nP-values are typically over emphasized. They are a piece of the puzzle, however they are not the whole puzzle. Arguably, a more important piece of the puzzle is the effect size. For our example, lollipop lickers have increased odds of a sore tongue compared to non-lollipop lickers (OR = 5.87, 95% CI: 2.53 to 13.87). The effect, in this case OR, should be another piece of the puzzle that is considered. Finally, there are two other errors we should go over that highlight how not confident we can be in the p-value.\n\n\nOften type I and type II errors are highlighted however type M and type S errors are important to note as well. A type S error is the probability of an estimate being in the wrong direction (Gelman and Carlin 2014). A type M error is the factor by which the magnitude of an effect might be overestimated (Gelman and Carlin 2014). Definitions are always great but an example helps to better understand.\nUsing our example from before, and the function outlined in Gelman and Carlin (2014):\n\n\nCode\nretrodesign <- function(A, s, alpha=.05, df=Inf, n.sims=10000){\n  z <- qt(1-alpha/2, df)\n  p.hi <- 1 - pt(z-A/s, df)\n  p.lo <- pt(-z-A/s, df)\n  power <- p.hi + p.lo\n  typeS <- p.lo/power\n  estimate <- A + s*rt(n.sims,df)\n  significant <- abs(estimate) > s*z\n  exaggeration <- mean(abs(estimate)[significant])/A\n  return(list(power=power, typeS=typeS, exaggeration=exaggeration))\n}\n\nretrodesign(-0.31703, 0.04986)\n\n\nThe type S error is 0.99, while the type M error is -0.997. This means that there is a 99% probability that the estimate is the wrong sign, and that the value is slightly underestimated (0.997 of what it should be). Doesn’t sound good right?\n\n\n\n\nP-values have a place in science, however they should be interpreted with caution and as a piece to the puzzle. Other pieces include the effect size, study design, sample that was studied and a myriad of other components."
  },
  {
    "objectID": "posts/glm/glm.html",
    "href": "posts/glm/glm.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "Regression is a commonly used tool in inferential statistics to build a model to make inferences about a super-population. Before we start however, I’d like to highlight that regression is not always necessary or even the best approach. For example, sometimes descriptive statistics are a more appropriate tool. With that being said, regression is a key tool in a statistician’s toolbox. Before we get started, I’d like to revisit one of my favorite quotes by George Box, a British statistician: “All models are wrong, some are useful”.\nWith this in mind, let’s get started! The real question is where should we start? Typically, most courses start with ordinary least squares (OLS) regression. Seems like a reasonable place to me to begin as well! However, we will back up a little bit and start with the equation of a line."
  },
  {
    "objectID": "posts/glm/glm.html#equation-of-a-line",
    "href": "posts/glm/glm.html#equation-of-a-line",
    "title": "Generalized Linear Models",
    "section": "Equation of a Line",
    "text": "Equation of a Line\nFrom high school math, you probably remember the equation of a line as\n\\[\ny = mx +b\n\\]\nwhere \\(y\\) is the dependent variable, \\(m\\) is the slope of the line, \\(x\\) is the independent variable and \\(b\\) is the y-intercept. A key exercise that is often used in math, using graph paper, is to first plot data points based on their coordinates (x, y), then draw a line of best fit through them. Once you draw the line, you can calculate the slope and extend the line through the y-intercept. This gives you the equation of your line which you can use to predict values of y! When drawing the line of best first, the goal is to capture as many data points on the line as possible, or closest to the line. OLS regression is similar to this."
  },
  {
    "objectID": "posts/glm/glm.html#plain-old-ordinary-regression",
    "href": "posts/glm/glm.html#plain-old-ordinary-regression",
    "title": "Generalized Linear Models",
    "section": "Plain Old Ordinary Regression",
    "text": "Plain Old Ordinary Regression\nOLS regression tries to minimize the sum of squares of the differences between the observed variable and those predicted by the model (hence the least squares). Put another way: by minimizing the residuals. A way to conceptualize this is by drawing the line of best fit.\n\nFor example, imagine we have a database that we can use to ask a burning question we have: Does a person’s age effect the number of balloons they own? Now, if we draw a line of best fit through the data, using number of balloons as the outcome/y/dependent variable, and age as the predictor/x/independent variable… we get the figure below.\n\n\nCode\nset.seed(2228) # August 28, 2022\n\nlibrary(tidyverse)\nlibrary(ggxmean)\n\nn.id = 250\n\ndf <- data.frame(\n  age = runif(n = n.id, min = c(5, 18), max = c(30, 85)),\n  sex = rbinom(n = n.id, size = 1, prob = c(0.60,0.20)),\n  candy_lover = rbinom(n = n.id, size = 1, prob = c(0.64, 0.45)),\n  \n  # Beta Coefficients\n  \n  beta_age = runif(n = n.id, min = 0, max = 0.10),\n  beta_sex = runif(n = n.id, min = 0, max = 0.30),\n  beta_candy = runif(n = n.id, min = 0, max = 0.40)\n) %>% \n  dplyr::mutate(\n    num_ballons = round((beta_age*age + beta_sex*sex + beta_candy*candy_lover)*3,0) # number of ballons that you own\n  )\n\nggplot2::ggplot(data = df, mapping = aes(x = age, y = num_ballons)) +\n  ggplot2::geom_point(color = \"red\") +\n  geom_lm() +\n  ggxmean::geom_lm_residuals(linetype = \"dashed\") +\n  labs(x = \"Age\", y = \"Number of Balloons Owned\") +\n  theme_minimal() + \n  ggtitle(\"Number of Balloons by Age\") +\n  scale_x_continuous(limits = c(0, 100)) + \n  scale_y_continuous(limits = c(0, 30)) + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nLet’s break down this figure into digestible chunks. First, let’s focus on the distance from the red dot to the blue line for each data point. This measurement is from the observed value (red dot) to the predicted value (point on the blue line). This is also known as the residual; in the figure, the dashed black line. By drawing the line of best fit, we are aiming to minimize the residuals. Once we have that, we can determine the equation of the line and predict some values!\nOf course, this is great in theory to draw the line of best fit, however it can get more complicated when there are additional variables we’d like to adjust for. There are two extra variables in our database that we think could be important: sex and candy lover (yes/no). We may want to adjust for whether a person loves candy or not since candy buyers tend to buy more balloons. Furthermore, we may also want to adjust for sex. How do we draw a line of best fit for all of these? The simple answer is we don’t. A more useful approach is to use a mathematical equation.\n\nUsing Equations and Matrices\nFirst, we need to expand our equation to align with our research question\n\\[\n\\text{number of balloons} = \\beta_0 + \\beta_1*age + \\beta_2*sex + \\beta_3*\\text{candy lover}\n\\]\nNow we need to estimate the parameters (i.e., \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)). Instead of drawing a line, we can find the estimated values of the parameters using the below equation. Note that these variables are referring to the matrix, rather than a specific variable.\n\\[\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n\\]\nwhere \\(\\hat{\\beta}\\) is the ordinary least squares estimator, \\(X\\) is the matrix containing the predictor variables and \\(y\\) is the vector of the response variable. For our example, \\(X\\) is the matrix containing the values of an intercept and values for the following variables: age, sex and candy lover.\n\n\n\n\n\n\nNote\n\n\n\nFor those not familiar with linear algebra, the superscript T is called the transpose. Similarly, the -1 superscript is referred to as the inverse of a matrix.\n\n\nAbstract concepts can be helpful however an example using data is always better. Using our data from before on balloons and age, we can create our matrix of predictors, \\(X\\). In our matrix, we will also include a column of all 1s to reflect the intercept. Looking at the first six individuals, we get a sense of what the matrix looks like.\n\n\nCode\nX = as.matrix(cbind(1, df$age, df$sex, df$candy_lover)) \n\nX.df = X %>% as.data.frame() \ncolnames(X.df) = c(\"intercept\", \"age\", \"sex\", \"candy lover\")\n\nhead(X.df)\n\n\n  intercept      age sex candy lover\n1         1 29.23506   1           0\n2         1 54.38400   0           1\n3         1 27.16523   0           0\n4         1 44.35563   0           0\n5         1 29.51780   1           1\n6         1 75.95165   0           1\n\n\nAs you can see, the first column for the intercept is all 1s. The second is the value for age, third is sex (1 = female, 0 = male) and fourth is the value for candy lover (1 = yes, 0 = no). Similarly, for the \\(y\\) matrix\n\n\nCode\ny = as.matrix(df$num_ballons)\ny.df = y %>% as.data.frame()\ncolnames(y.df) <- c(\"number of ballons owned\")\n\nhead(y.df)\n\n\n  number of ballons owned\n1                       2\n2                      10\n3                       2\n4                      12\n5                       6\n6                       2\n\n\nWe can see that the \\(y\\) matrix is just the values for the outcome of interest. Now, how do we estimate the parameters? For that, we turn back to our handy dandy formula.\n\nEstimating Parameters\nRemember our formula from before? If you don’t, no problem I will add it below again just for good measure. Using our handy dandy formula and plugging in our matrices\n\\[\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n\\]\n\n\nCode\n# Doing it step by step:\n\nstep.1 <- t(X)%*%X\nstep.2 <- solve(step.1) # solve will return the inverse of a\nstep.3 <- step.2%*%t(X)\nstep.4 <- step.3%*%y\nbeta <- step.4\n\n# Alternatively, can do in one messy looking code: \n\nbeta <- solve(t(X)%*%X)%*%t(X)%*%y\n\n# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code all in one\n\nbeta %>% \n  as.data.frame() %>% \n  dplyr::mutate(\n    variable = c(\"intercept\", \"age\", \"sex\", \"candy lover\")\n  ) %>% \n  dplyr::rename(\n    estimate = V1\n  ) %>% \n  dplyr::select(\n    variable, estimate\n  )\n\n\n     variable    estimate\n1   intercept  0.09488954\n2         age  0.14316794\n3         sex -0.09891130\n4 candy lover  0.96749456\n\n\nGreat Scott! We have some results! The estimates are great but you may be wondering, “what about that standard error tho?” If you were thinking that, great point! First, we’ll need to get the variance from the variance-covariance matrix. How can we determine this variance-covariance matrix? Another formula!\n\n\nEstimating Variance\nIn order to calculate the variance-covariance matrix, we first the residuals. We can do that using the below formula (how many times have I said formula so far in the post?)\n\\[\nResiduals = y - \\beta_1 - \\beta_2*age - \\beta_3*sex - \\beta_4*\\text{candy lover}\n\\]\nOnce we know the residuals, we can calculate the variance-covariance matrix as\n\\[\nVCov = \\frac{1}{n-k}(RES^TRES)*(X^TX)^{-1}\n\\]\nwhere \\(n, k, RES, X\\) are the number of observations, number of parameters estimated, matrix of residuals and matrix of values for the predictor variables. Back to our example\n\n\nCode\n# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code \n\n# Calculating residuals as y-intercept-beta_1*X1-beta_2*X2\n\nres <- as.matrix(y-beta[1]-beta[2]*X[,2]-beta[3]*X[,3] - beta[4]*X[,4])\n\n# Note the above is really:\n# res <- as.matrix(y-beta[1]-beta[n]*X[,n]) for as many n's as there are \n\n# Variance-Covariance Matrix (VCV) \n\n# Formula for VCV: (1/df)*t(res)*res*[t(X)(X)]^-1\n\nn = nrow(df)\nk = ncol(X)\n\nVCV <- (1/(n-k))*as.numeric(t(res)%*%res)*solve(t(X)%*%X)\n\nVCV.df <- VCV %>% as.data.frame() \ncolnames(VCV.df) <- c(\"intercept\", \"age\", \"sex\", \"candy lover\")\nrownames(VCV.df) <- c(\"intercept\", \"age\", \"sex\", \"candy lover\")\nVCV.df\n\n\n               intercept           age          sex   candy lover\nintercept    0.376249319 -0.0054645875 -0.156856159 -0.1350433698\nage         -0.005464587  0.0001331024  0.002296164  0.0002074815\nsex         -0.156856159  0.0022961636  0.294962047 -0.0373553074\ncandy lover -0.135043370  0.0002074815 -0.037355307  0.2375580016\n\n\nTa-da! We now have our variance-covariance matrix! You may be asking yourself “so what?”. Well, besides that being rude, this matrix is how we can determine the standard error. The diagonals of the matrix are the variance for each of the variables. If we take the square root of the variance, we will get the standard deviation. In this case the standard deviation of the sampling distribution, aka the standard error.\n\n\nCode\nse <- sqrt(diag(VCV))\n\nse\n\n\n[1] 0.6133917 0.0115370 0.5431041 0.4873992\n\n\nThe last piece to our puzzle is to add some p-values. While we’re at it, let’s make it a little easier to read by adding in some formatting and text to help the reader make sense of our results.\n\n\nCode\np_value <- rbind(\n  2*pnorm(abs(beta[1]/se[1]), lower.tail = FALSE),\n  2*pnorm(abs(beta[2]/se[2]), lower.tail = FALSE),\n  2*pnorm(abs(beta[3]/se[3]), lower.tail = FALSE),\n  2*pnorm(abs(beta[4]/se[4]), lower.tail = FALSE)\n)\n\n# Output \n\noutput <- as.data.frame(\n  cbind(\n    c(\"Intercept\", \"Age\", \"Sex\", \"Candy Lover\"),\n    round(beta,5),\n    round(se, 5),\n    round(p_value, 4)\n)\n)\n\nnames(output) <- c(\n  \"Coefficients\",\n  \"Estimate\",\n  \"Std. Error\",\n  \"Pr(>{Z}\"\n)\n\noutput\n\n\n  Coefficients Estimate Std. Error Pr(>{Z}\n1    Intercept  0.09489    0.61339  0.8771\n2          Age  0.14317    0.01154       0\n3          Sex -0.09891     0.5431  0.8555\n4  Candy Lover  0.96749     0.4874  0.0471\n\n\n\n\nComparing to lm()\nGreat so we have worked through calculating OLS regression by hand, now what? Glad you asked! How do we know it worked? One way to check is to compare the values we got with those if we use the lm() function in R.\n\n\nCode\nmod.fit <- lm(num_ballons ~ age + sex + candy_lover, \n              data = df)\n\nsummary(mod.fit)\n\n\n\nCall:\nlm(formula = num_ballons ~ age + sex + candy_lover, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9362 -2.0570  0.2046  1.6230 11.9744 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.09489    0.61339   0.155   0.8772    \nage          0.14317    0.01154  12.409   <2e-16 ***\nsex         -0.09891    0.54310  -0.182   0.8556    \ncandy_lover  0.96749    0.48740   1.985   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.732 on 246 degrees of freedom\nMultiple R-squared:  0.423, Adjusted R-squared:  0.4159 \nF-statistic:  60.1 on 3 and 246 DF,  p-value: < 2.2e-16\n\n\nReviewing the results above, these look very similar to those obtained from working through step by step (except for some rounding differences). Now that we have covered linear regression, we are ready to move onto generalized linear models (GLM).\n\n\n\n\n\n\nNote\n\n\n\nThis post focuses on how the estimates are derived for linear regression, not the assumptions or diagnostics that can be used to check these assumptions and model fit. It’s always important to check the assumptions to see if any of them are violated, for example the assumption of homoscedasticity for the residuals. A good resource for reviewing the assumptions of OLS is by Jim Frost (OLS Linear Regression Assumptions)"
  },
  {
    "objectID": "posts/glm/glm.html#generalized-linear-model",
    "href": "posts/glm/glm.html#generalized-linear-model",
    "title": "Generalized Linear Models",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nFor any statistical method there are certain assumptions that have to be met. For example, with OLS regression one of the key assumptions is homoscedasticity for the residuals. Sometimes these assumptions are not met. Using homoscedasticity as an example, the variance of the errors is not consistent across observations and is rarely met. Due to this violation, using OLS regression may provide inaccurate results. One solution is to use a generalized linear model (GLM), however GLMs have their own set of assumptions. But first, we need to review what a GLM is!\n\n\n\n\n\n\nNote\n\n\n\nWhy GLMs over OLS regression? A few of the reasons [STAT 504, Section 6.1]:\n\nSince the model uses MLE instead of OLS, parameter estimates and likelihood functions benefit from asymptotic normal and chi-square distributions\nHomoscedasticity is not required for GLMs\nNo need to transform the outcome variable to have a normal distribution\nChoosing the link has nothing to do with what you choose for the random component. For example, you can use a binomial distribution but choose to use a logit link or probit.\nThere is no separate error term\n\n\n\n\nDo Candy Lovers Own More Balloons?\nFor our next question, we want to know if candy lovers own more balloons than those who do not. Luckily, we can use the same database that we did for our regression analysis! First, we need to go through what a GLM is before we jump into getting answers.\n\n\nMain Parts of GLM\nThere are three components to any GLM [STAT 504, Section 6.1]:\n\nRandom Component. This refers to the response variable. We need to make an assumption about the probability distribution of the response variable (i.e., normal, binomial, Poisson, etc.). Note: this is the only random component in the model (i.e., there is not a separate error term like there is in OLS regression). For our example, we assume that candy lovers is from a binomial distribution.\nSystematic Component. This outlines the explanatory variables and how they are related. Again, using the example above, the systematic component is linear since it will be \\(\\beta_0 + \\beta_1*age + \\beta_2*sex + \\beta_3*\\text{number of balloons}\\)\nLink Function. The link function is a crucial component of GLMs which differentiates it from OLS regression. This specifies how the random and systematic components are connected. For our example, the link function is logit. If our outcome variable were continuous it would be the identity link function (why it’s called the identity link function has never made sense to me. Think of it as the one you are used to: y = mx + b).\n\nLike any statistical technique, there are assumptions as well [STAT 504, Section 6.1]:\n\nThe data are independently distributed\nThe dependent variable typically assumes a distribution from an exponential family (i.e., normal, binomial, Poisson, etc.)\nA linear relationship between the transformed expected response in terms of the link function and explanatory variables (however not in terms of the response and explanatory variables).\nErrors need to be independent but not normally distributed (this is a key difference between OLS regression and GLMs)\n\n\n\nMaximum Likelihood Estimation\nOne of the key differences between OLS and GLM is the way that parameters (aka coefficients) are estimated. While OLS uses ordinary least squares, GLMs use something called maximum likelihood estimation (MLE). MLE is like what it sounds like: it’s about maximizing the likelihood function so that the model uses values that make the observed data most probable (aka most likely).\nThere are different methods to determine this value using MLE including solving the derivative of the likelihood funciton then setting to 0 (where the maxima occurs, from calculus), or more iterative procedures such as the Gradient descent method or the Newton-Raphson method. Here we will focus on iteratively reweighted least squares (IWLS) since that is what R uses by default in the glm() function.\n\nIteratively Reweighted Least Squares\nIWLS, is an algorithm that is used to determine the parameters and standard errors of the parameters. We’ll use logistic regression to walk through the steps, although for other link functions it is a similar process. The steps for IWLS are outlined below (Fox 2014)\n\nSet the regression coefficients to some initial value. For our example, we will start with 0\nFor each iteration, t, calculate the fitted probabilities, \\(\\mu\\), variance-function values, \\(v\\), working-response values, \\(z\\), and weights, \\(w\\).\n\\(\\mu_i^{(t)} = [1 + exp(-\\eta_i^{(t)})]^{-1}\\)\n\\(v_i^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})\\)\n\\(z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)})/v_i^{(t)}\\)\n\\(w_i^{(t)} = n_i*v_i\\)\nNote: \\(n_i\\) represents the binomial denominator for the ith observation. For binary data, all of the \\(n_i\\) are 1.\nRegress the working response on the predictors using weighted least squares, minimizing the weighted residual sum of squares. To do that, you can use the following formula:\n\\(\\sum\\limits_{i = 1}^{n}w_i^{(t)}(z_i^{(t)} - x_i^{'}\\beta)^2\\)\nwhere \\(x_i^{'}\\) is the ith row of the model matrix.\nRepeat steps 2 and 3 until the regression coefficients stabilize at the maximum-likelihood estimator \\(\\hat\\beta\\)\nCalculate the estimated asymptotic covariance matrix of the coefficients using the below formula\n\\(\\hat{V}(\\hat{\\beta}) = (X^{'}WX)^{-1}\\)\nwhere \\(W = \\text{diag}\\text{(}w_i\\text{})\\) is the diagonal matrix of weights from the last iteration and \\(X\\) is the model matrix.\n\nReading through steps can be helpful but an example is always better. Let’s work through this in R. First we’ll want to make a function to calculate IWLS implementing these steps (credit to Michael Clark for code for implementing IWLS, link here)\n\n\nCode\niwls <- function(X, y, tol = 1e-7, iter = 500){\n  \n  # Note: tol = 1e-7 is used by the lsfit function\n  \n  # First we need to start with some inital values\n  \n  int = log(mean(y)) / (1-mean(y)) # intercept\n  beta = c(int, rep(0, ncol(X) -1))\n  currtol = 1\n  it = 0\n  ll = 0 # log likelihood\n  \n  # As long as the tolerance calculate is greater than what we will allow we want the code to repeat\n  \n  while(currtol > tol && it < iter){\n    it = it + 1\n    ll_old = ll\n    \n    eta = X %*% beta\n    mu = plogis(eta)[,1]\n    s = mu*(1-mu)\n    S = diag(s)\n    z = eta + (y-mu)/s\n    beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z))\n    var = solve((t(X) %*% S %*% X))\n    \n    ll = sum(\n      dbinom(\n        y, \n        prob = plogis(X%*% beta), \n        size = 1, \n        log = TRUE)\n      )\n    \n    currtol = abs(ll - ll_old)\n  }\n  \n  list(\n    beta = beta, \n    var = var, \n    se = diag(sqrt(var)), # SE = sqrt(var) but we want diagnoals of the variance-covariance matrix \n    iter = it, \n    tol = currtol, \n    loglik = ll, \n    weights = plogis(X %*% beta) * (1 - plogis(X %*% beta))\n  )\n  \n}\n\n\n\n\n\nComparing our method to glm()\nNow that we’ve written a function that calculates estimates of the parameters and standard errors, we need to see if it works! What’s a better way to check than comparing with a well-established method? We’ll compare our function to that from using glm(), although admittedly our output is not as clean.\n\n\nCode\nlibrary(tidyverse)\n\nX <- cbind(1, df$age, df$sex, df$num_ballons) %>% as.matrix()\ny <- df$candy_lover %>% as.matrix()\n\nour.way <- iwls(X, y)\n\nour.way$beta\n\n\n            [,1]\n[1,]  0.23378250\n[2,] -0.01323819\n[3,]  0.68071575\n[4,]  0.06809751\n\n\nCode\ndiag(our.way$se)\n\n\n         [,1]        [,2]      [,3]      [,4]\n[1,] 0.300512 0.000000000 0.0000000 0.0000000\n[2,] 0.000000 0.008062909 0.0000000 0.0000000\n[3,] 0.000000 0.000000000 0.3057791 0.0000000\n[4,] 0.000000 0.000000000 0.0000000 0.0354048\n\n\nCode\nglm.way <- glm(candy_lover ~ age + sex + num_ballons, \n               family = binomial(link = \"logit\"), \n               data = df)\n\n\n\nOutput from our function compared to glm()\n\n\n\n\n\n\n\nParameter\nEstimate (SE)\nglm()\nEstimate (SE)\nour method\n\n\n\n\nIntercept\n0.233782 (0.300512)\n0.233783 (0.300512)\n\n\nAge\n-0.013238 (0.008063)\n-0.013238 (0.008063)\n\n\nSex\n0.680716 (0.305779)\n0.680716 (0.305779)\n\n\nNumber of Balloons\n0.068098 (0.35405)\n0.068098 (0.035405)\n\n\n\nLooking at the estimates from the glm() function to our function…nearly identical results! Yippee! This is also what we’d expect. We can also look at the weights from the last iteration for both the glm() method and using our function. The code is in the below snippet, however rather than boring you with weights for 250 observations, I will leave that up to you to review if you are interested (tldr: they are quite similar).\n\n\nCode\nresult.weights <- cbind(glm.way$weights, our.way$weights) %>% as.data.frame()"
  },
  {
    "objectID": "posts/glm/glm.html#whats-next",
    "href": "posts/glm/glm.html#whats-next",
    "title": "Generalized Linear Models",
    "section": "What’s Next?",
    "text": "What’s Next?\nSince every good story must come to an end, so too does our GLM by hand exercise…but fear not! You can now use this to foray into the world of GLM with a better understanding of how these parameters are calculated!"
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html",
    "href": "posts/causal_inference_intro/causal_inference.html",
    "title": "Intro to Causal Inference",
    "section": "",
    "text": "I was first introduced to causal inference by Hernan and Robins (2021), in the summer of 2021 prior to starting my PhD. This book was tremendous and changed my thinking about “correlation doesn’t equal causation”.\n\nFrom introductory science, as scientists, we are taught that correlation doesn’t equal causation. I had never questioned this until I was in my MSc program and started thinking about the association between lung cancer and smokers. Stellman et al. (2001) reported an odds ratio of 40.4 (95% CI: 21.8-79.6), which to me seemed high.\nNow there are two ways to interpret this. On one hand, if we were a tobacco company we could argue that the 95% CI is pretty wide, indicating that the estimate is not that precise and the statistical method used must be wrong, etc (by etc, I mean whatever excuses you want to use). On the other hand, even at the lower limit an odds ratio of 21.8 is pretty damn high. In my opinion, this would require at the very least more investigation to determine “does smoking cause lung cancer?”"
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html#common-ground",
    "href": "posts/causal_inference_intro/causal_inference.html#common-ground",
    "title": "Intro to Causal Inference",
    "section": "Common Ground",
    "text": "Common Ground\nPrior to diving straight into the world of causal inference, we first need to set some common ground rules.\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind causal inference is a broad field and will be the subject of many subsequent posts, this is just an introductory post or wetting your beak if you will.\n\n\n\nConfound It!\nIn order to get to the bottom of “does X cause Y?” we need to outlined some key methodological concepts. First, we need to outline what confounding is and clearly define what we mean.\nDifferent disciplines used different terminology, however confounding in clinical epidemiology refers to a variable that impacts both the exposure (i.e., smoking) and outcome (i.e., lung cancer). A useful way to visualize this is using directed acyclic graphs (DAGs). I’ll defer to Hernan and Robins (2021) for more detail about DAGs. However, the below illustrates what confounding is.\n\n\n\n\n\nFigure 1: Coffee Drinkers, Smoking and Lung Cancer.\n\n\n\n\nFigure 1 shows that coffee drinkers are associated with smoking. Logically, this makes sense because many smokers enjoy a cup of coffee with their cigarette. In fact, a lot of people enjoy coffee. A group of these people who are coffee lovers may also develop lung cancer. Based on this, you could say that people who drink coffee develop lung cancer. The question is, is this true? Or is it just that a lot of people like drinking coffee? Confounding bias, makes it difficult to tease out what coffee actually causes.\n\n\nRandomized Controlled Trials (RCTs)\nRandomized controlled trials are typically considered the gold standard in research. However, sometimes people forget why they are considered the gold standard. There are of course numerous reasons but I will touch on a few here. Firstly, an RCT is randomized meaning that patient’s are randomly assigned to one of the treatment arms. In theory, this is to ensure that characteristics are balanced between the treatment arms. By balancing characteristics, in theory, both observed and unobserved confounders are equal between the groups, making them exchangeable (aka similar).\n\n\n\n\n\n\nNote\n\n\n\nRandom is important to note here. There are different methods which are truly random, such as a random number generator, while there are others that are not.\nFor example, if you were picking players for a baseball team and wanted to assign people to two teams, team A & team B, how would you do it? Based on the color shirt they are wearing, hair color, eye color? None of those methods would be random.\n\n\nAnother key strength of RCTs is the nature of the intervention. For example, a well designed RCT will be very clear about what treatment is received by the participants and that all participants will receive the same treatment. Furthermore, one participant receiving a treatment won’t affect the other participant receiving treatment. It is also worth noting here that all participants in a RCT will receive some form of exposure (typically either treatment or placebo).\n\n\nObservational Data\nObservational data in research often gets a bad wrap. Since the gold standard is a RCT, people often view observational data as second-tier compared to an RCT. However, it all depends on how well the study is conducted. If the study is conducted rigorously, which is subjective but we will elaborate more below, then a study using observational data can be almost as good as an RCT."
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html#conditions-required-for-causal-inference",
    "href": "posts/causal_inference_intro/causal_inference.html#conditions-required-for-causal-inference",
    "title": "Intro to Causal Inference",
    "section": "Conditions Required for Causal Inference",
    "text": "Conditions Required for Causal Inference\nReal-life is very complicated. For any method or analysis, such as designing a house, assumptions are required. This is is no different for causal inference. Key assumptions for causal inference include (Hernán 2012):\n\nExchangeability\nPositivity\nConsistency\nNo versions of treatment\nNo interference\n\n\n\n\n\n\n\nNote\n\n\n\nCollectively, consistency, no versions of treatment and no interference have been referred to as the stable-unit-treatment-value assumption (SUTVA) (Hernán 2012).\n\n\nLet’s break down these assumptions down one by one.\n\nExchangeability\nRemember confounding from earlier? Well that pesky bias is back again to haunt us.\n\n\n\n\n\n\nNote\n\n\n\nRecommend to pause here and re-read Section 2.1 if needed\n\n\nThe goal, although not always, of a lot of research is to compare two different exposures. An exposure could be a treatment, for example drug A to placebo, or to compare purple Popsicle eaters with water drinkers. An example always helps.\nLet’s pretend we want to compare purple Popsicle eaters with water drinkers, to see if there is a difference in who is hungrier. Using an existing database of hungry people, we decide we want to compare these two groups of people. In the database, there are 591 people who have reported hunger.\n\n\nTable 1: Purple Popsicle Eaters vs Water Drinkers: Demographic Characteristics\n\n\n\n\n\n\n\nDemographic Variable\nPurple Popsicle Eaters\n(n = 250)\nWater Drinkers\n(n = 341)\n\n\n\n\nAge, mean (SD)\n9.18 (3.39)\n42.58 (22.03)\n\n\nFemale, n (%)\n190 (76.0%)\n145 (42.5%)\n\n\n\n\nTable 1 shows the average age and proportion of females in each of the two groups. Now, after looking at these two groups, you may think to yourself “Wait a minute…water drinks on average are 42 while purple Popsicle eaters are an average age of 9? These can’t possibly be compared!”.\nYour inclination was right. These two groups are quite different and may vary in more ways than what is shown. This lack of similarity could be reworded as a lack of exchangeability. Luckily, there are fancy stats methods to fix this, which will be the topic of other posts.\n\n\n\n\n\n\nImportant\n\n\n\nWhile we can adjust for observed confounding, it is not possible to adjust for unobserved confounding. We simply don’t know about it, for example how many people in each group enjoy the taste of grape crush.\n\n\n\n\nPositivity\nPositivity refers to the condition that every individual has a greater than 0 probability of being assigned to each of the treatment levels (Hernan and Robins (2021), pp. 30). Logically, this makes sense because if we think about our purple Popsicle eaters and water drinks, if someone in that group had a less than 0 probability they couldn’t possibly be in that group!\n\n\nConsistency\nConsistency means that the observed outcome for every treated individual equals her outcome if she received treatment and that the observed outcome for every untreated individual equals her outcome if she had remained untreated, that is \\(Y^a = Y\\) for every individual with \\(A = a\\) (Hernan and Robins (2021), pp. 31).\nOn the surface, this seems intuitive but lets dive a little deeper. There are two components to consistency:\n\nA precise definition of the counterfactual outcome \\(Y^a\\) via a detailed specification of the superscript \\(a\\) ( Hernan and Robins (2021)\nLinkage of the counterfactual outcomes to the observed outcomes ( Hernan and Robins (2021))\n\nThe first bullet point is we essentially want to have a well defined intervention. If we think back to our purple Popsicles, we want to make sure that it is the same size, shape and brand that each participant receives. Doing this allows us to make sure that each participant is getting the same treatment.\nFor the second bullet point, remember we are using observational data for our purple Popsicle eaters. We have to make sure that for the analysis, we only treat people receiving our intervention (purple Popsicle eaters) as treated, or as Popsicle eaters, and the others as not Popsicle eaters.\n\n\nNo Versions of Treatment\nAs discussed in Section 3.3, we cannot have multiple versions of the same treatment. This would muddy our findings if we were giving different people different Popsicle. Plus, this would make it hard to determine the effect of purple Popsicles on hunger. Imagine, you gave some people a much bigger Popsicle than others. Of course they’d be less hungry!\n\n\nNo Interference\nInterference here refers to the interference of the exposure. Exposure could be treatment, infectious disease or Popsicles. For our example, and most examples in this blog, the condition of no interference will be assumed to be met. Examples where this is not met is infectious disease studies (Hernan and Robins (2021))."
  },
  {
    "objectID": "posts/causal_inference_intro/causal_inference.html#when-to-use-cause",
    "href": "posts/causal_inference_intro/causal_inference.html#when-to-use-cause",
    "title": "Intro to Causal Inference",
    "section": "When to use Cause",
    "text": "When to use Cause\nFinally, all we wanted to do was get to use the word cause! Why? Because it’s important to use. If the above conditions hold, three main ones: exchangeability, positivity and consistency, then you can use the term cause. Besides, wouldn’t you want to know if Popsicles caused hunger?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RWE Blog",
    "section": "",
    "text": "Instrumental Variable Estimation\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStandardize Your Way to Causal Inference\n\n\nStandardization and the Parametric G-Formula\n\n\n\n\nStandardization\n\n\nParametric G-Formula\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nGeneralized Linear Models\n\n\n\n\n\n\n\nGLM\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nInverse Probability Weighting\n\n\n\n\n\n\n\nIPW\n\n\nIPTW\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nMarginal vs Conditional Effects\n\n\n\n\n\n\n\nMarginal\n\n\nConditional\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nReal-World Evidence: What’s all the hype?\n\n\n\n\n\n\n\nRWE\n\n\nObservational Data\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nP-Values and Power: Please Explain\n\n\n\n\n\n\n\nP-Values\n\n\nHypothesis Tests\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nIntro to Causal Inference\n\n\n\n\n\n\n\nCausal Inference\n\n\nRCT\n\n\nObservational Data\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nOR/RR/HR: What’s the Difference?\n\n\n\n\n\n\n\nOR\n\n\nRR\n\n\nHR\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2022\n\n\nRyan Batten\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2022\n\n\nRyan Batten\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My undergraduate degree is in civil engineering, with a focus on structural engineering. After graduating, I decided that while I loved math, which is why I went into engineering in the first place, perhaps the career wasn’t for me. I began looking for other opportunities/career paths when I had a meeting with my now supervisor. He introduced me to clinical epidemiology, which led me to start my MSc in 2016.\nA few different work experiences later, I’ve been able to apply my clinical epidemiology training + LOVE of statistics + passion for math/big data to everyday life. As a result, I started my PhD in January 2022 in clinical epidemiology focusing on causal inference and real-world evidence.\n\nExperience\nI’ve had a lot of different jobs relevant to research and not so relevant to research. There has been a wide range of jobs, varying from working as an engineering student, to as an estimator, to at a Canadian hardware store chain. Research relevant experience? I’ve worked as a student, consultant, on grants, for the Public Health Agency of Canada, EVERSANA and most recently at PHASTAR. In my recent role, as a Biostatistician III I’ve been able to do what I love. Diving into various statistical methodologies, while working with talented people. More importantly, working with caring/nice people.\n\n\nPersonal\nOkay so you made it this far, I’m impressed! I probably wouldn’t have read it this far to be honest so props to you. I LOVE statistics, figured I’d mention it again in case you didn’t know that yet, but have outside work interests as well. I’m an avid sports fan. A Buffalo Sabres fan, which has been rough as of late and a Kansas City Chiefs fan (2023 Super Bowl Champs!!).\nI am currently living in Halifax, Nova Scotia with my wife and daughter, and appreciate a good cider. While I live in Nova Scotia, I’m originally from Newfoundland (highly recommend people visit when the weather is nice, key is when)."
  },
  {
    "objectID": "posts/iv-methods/iv_methods.html",
    "href": "posts/iv-methods/iv_methods.html",
    "title": "Instrumental Variable Estimation",
    "section": "",
    "text": "No, not intravenous, IV as in instrumental variable. An instrumental variable is"
  },
  {
    "objectID": "posts/iv-methods/iv_methods.html#iv-methods",
    "href": "posts/iv-methods/iv_methods.html#iv-methods",
    "title": "Instrumental Variable Estimation",
    "section": "IV Methods",
    "text": "IV Methods\nTo perform their magic, IV methods need an instrumental variable or an instrument. It needs to meet three instrumental conditions:\n\nZ is associated with A\nZ does not affect Y except through its potential affect on A\nZ and Y do not share causes\n\nFor example, in a double-blind trial, the randomization is an instrument because: i) trial participants are more likely to receive treatment if they were assigned to treatment, ii) expected by double-blind design, iii) random assignment of Z.\nusing Hernan & Robins Ch 16 and can perhaps use Ding et al. 2017"
  }
]