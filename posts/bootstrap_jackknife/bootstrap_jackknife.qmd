---
title: "Resampling Magic"
subtitle: "The Wonders of Bootstrapping and Jackknifing Explained"
author: "Ryan Batten"
date: "2023-07-06"
categories: [Resampling, Bootstrapping, Jackknifing]
bibliography: bootstrap-jackknife.bib
draft: true
format: 
  html:
    code-fold: true
---

## Uncertainty

A key part of any analysis is to determine the uncertainty associated with an estimate. Sometimes this can be especially tricky with certain methods such as

## Vary Away!

So what you're telling me is I can just use bootstrapping all the time for every test? Well I guess you could but I'm going to say nah. It is a useful tool but like all tools, whether statistical or not, there is a time and a place to use them. Would you use a hammer as a fly swatter? I guess you could, but why not just use a fly swatter.

## Bootstrapping: the Magic Bag Trick

Imagine you're a magician with a bag full of different colored marbles. You want to know what the average color looks like, but you can only pull out one marble at a time, look at it, then put it back. Bootstrapping is like doing this over and over again, taking notes each time, and then using those notes to make a pretty good guess about the average color.

Bootstrapping is a statistical method that helps us understand the uncertainty of our estimates. By creating many "resamples" from our original data, we can calculate the standard errors, confidence intervals, and even perform hypothesis testing!

Here's a simple R code example using bootstrapping to estimate the average height of kids:

## Ol' Faithful

Bootstrapping comes up a lot. It's very useful, altho it can't solve every problem, it can work in a lot of scenarios where potentially other methods don't work.

## Pirates of the Caribbean?

No, this post will not be focusing on Pirates of the Caribbean but does have bootstrapping and jackknifing! However...perhaps we will

## Variance Estimation

A key aspect of conducting statistical analyses is to identify the most appropriate way to estimate variance. Whether it be the variance for a descriptive or inferential statistic the variance can tell us a lot.

Variance is a measure of how spread out a set of data is. It can tell us how much variation or dispersion there is from the average or mean of the data set. Variance can help us understand how much risk is associated with a particular investment or how much variation there is in a population. It can also help us identify outliers in a data set.

## Types of Variance Estimation

### Parametric Variance Estimators

what are the different parametric variance estimators

1.  Maximum Likelihood Estimator (MLE)
2.  Method of Moments Estimator (MME)
3.  Bayesian Estimator
4.  Restricted Maximum Likelihood Estimator (REML)
5.  Generalized Method of Moments Estimator (GMM)
6.  Empirical Bayes Estimator
7.  Bootstrap Estimator
8.  Jackknife Estimator

for the population mean.

The parametric variance estimator for the population mean is a statistical method used to estimate the variance of a population mean. It is based on the assumption that the population follows a normal distribution. The estimator uses the sample mean and sample variance to calculate the population mean and variance. The formula for the parametric variance estimator is:

σ2 = (1/n) \* Σ(x - x̄)2

where n is the sample size, x is the individual sample values, and x̄ is the sample mean.

### Nonparametric Variance Estimators

what are the types of nonparametric variance estimators

1.  Bootstrap Variance Estimator
2.  Jackknife Variance Estimator
3.  Permutation Variance Estimator
4.  Subsampling Variance Estimator
5.  Kernel Variance Estimator
6.  Rank Variance Estimator
7.  Resampling Variance Estimator
8.  Empirical Variance Estimator

## Jackknife

what is jackknife resampling

Jackknife resampling is a statistical technique used to estimate the bias and standard error of a statistic. It involves repeatedly leaving out one observation from a dataset and calculating the statistic of interest on the remaining data. The results are then averaged to obtain an estimate of the statistic's bias and standard error.

## 
