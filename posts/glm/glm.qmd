---
ttitle: "Generalized Linear Model"
author: "Ryan Batten"
date: "2022-08-28"
categories: [GLM, Regression]
# image: "iptw.jpeg"
bibliography: glm.bib
draft: true
format: 
  html:
    code-fold: true
---

## Regression: Swiss Army Knife of Statistics

Regression is a commonly used tool in statistics. Typically, the starting point is with ordinary least squares (OLS) regression. This is where we will also start. Well? Let's get started!

## Equation of a Line

From high school math, you probably remember the equation of a line as:

$$
y = mx +b
$$

where $m$ is the slope of the line, $x$ is the value of the dependent variable and $b$ is the y-intercept of the line. We can build upon this simple equation for linear regression.

## Ordinary Regression

The typical place to start with regression is ordinary least squares. This method tries to minimize the sum of squares of the differences between the observed variable and those predicted by the model (hence the *least squares*). Put otherwise: by minimizing the residuals. Another way to conceptualize this is by drawing the line of best fit.\
\
For example, imagine we have a database and want to examine the association between age and the number of balloons that a person owns. Our database has four variables: age, sex, candy lover (yes/no) and the number of balloons the person owns. Now, if we draw a line of best fit through the data, we'll get the below figure.

```{r, message = FALSE, warning = FALSE}

set.seed(2228) # August 28, 2022

library(tidyverse)
library(ggxmean)

n.id = 250

df <- data.frame(
  age = runif(n = n.id, min = c(5, 18), max = c(30, 85)),
  sex = rbinom(n = n.id, size = 1, prob = c(0.60,0.20)),
  candy_lover = rbinom(n = n.id, size = 1, prob = c(0.64, 0.45)),
  
  # Beta Coefficients
  
  beta_age = runif(n = n.id, min = 0, max = 0.10),
  beta_sex = runif(n = n.id, min = 0, max = 0.30),
  beta_candy = runif(n = n.id, min = 0, max = 0.40)
) %>% 
  dplyr::mutate(
    num_ballons = round((beta_age*age + beta_sex*sex + beta_candy*candy_lover)*3,0) # number of ballons that you own
  )

ggplot2::ggplot(data = df, mapping = aes(x = age, y = num_ballons)) +
  ggplot2::geom_point(color = "red") +
  geom_lm() +
  ggxmean::geom_lm_residuals(linetype = "dashed") +
  labs(x = "Age", y = "Number of Ballons Owned") +
  theme_minimal()


```

If we look at the distance from the red dot to blue line for each point, this is the measurement from the observed value (red dot) to the predicted value (on the blue line). This is also known as the residuals (in the figure the dashed black line).

Of course this is great in theory to draw the line of best fit, however there is also a mathematical equation that is more useful in practice for determining this.

### Using Equations and Matrices

We can find the estimated values of the parameters (i.e., $\beta_1$, $\beta_2$, $\beta_3$) using the below equation. Note that these variables are referring to the matrix, rather than a specific variable.

$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

where $\hat{\beta}$ is the ordinary least squares estimator, $X$ is the matrix containing the predictor variables and $y$ is the vector of the response variable. For our example, $X$ is the matrix containing the values of an intercept and values for the following variables: age, sex and candy lover.

::: callout-note
For those not familiar with linear algebra, the superscript T is called the transpose. Similarly, the -1 superscript is referred to as the inverse of a matrix.
:::

Abstract concepts can be helpful however an example is always better. Using our data (showing the first 6 rows):

```{r, message = FALSE, warning = FALSE}

X = as.matrix(cbind(1, df$age, df$sex, df$candy_lover))

head(X)

```

As you can see, the first column is all 1s. The second is the value for age, third is sex (1 = female, 0 = male) and fourth is the value for candy lover (1 = yes, 0 = no). Now for the y matrix:

```{r, message = FALSE, warning = FALSE}

y = as.matrix(df$num_ballons)

head(y)
```

#### Estimating Parameters

We can estimate the parameters using the below formula and plugging in our matrices.

$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

If we use this equation with our example:

```{r, message = FALSE, warning = FALSE}

# Doing it step by step:

step.1 <- t(X)%*%X
step.2 <- solve(step.1) # solve will return the inverse of a
step.3 <- step.2%*%t(X)
step.4 <- step.3%*%y
beta <- step.4

# Alternatively, can do in one messy looking code: 

beta <- solve(t(X)%*%X)%*%t(X)%*%y

# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code all in one

beta
```

Now we are ready to move onto calculating the variance covariance matrix, which we can use to derive our standard errors.

#### Estimating Variance

The first step is to calculate the residuals. We can do that using the below formula:

$$
Residuals = y - \beta_1 - \beta_2*age - \beta_3*sex - \beta_4*\text{candy lover}
$$

Once we know the residuals, as a matrix, we can calculate the variance-covariance matrix as

$$
VCov = \frac{1}{n-k}(RES^TRES)*(X^TX)^{-1}
$$

where $n, k, RES, X$ are the number of observations, number of parameters estimated, residual matrix and matrix of values for the predictor variables. Back to our example:

```{r, message = FALSE, warning = FALSE}

# Calculating residuals as y-intercept-beta_1*X1-beta_2*X2

res <- as.matrix(y-beta[1]-beta[2]*X[,2]-beta[3]*X[,3] - beta[4]*X[,4])

# Note the above is really:
# res <- as.matrix(y-beta[1]-beta[n]*X[,n]) for as many n's as there are 

# Variance-Covariance Matrix (VCV) 

# Formula for VCV: (1/df)*t(res)*res*[t(X)(X)]^-1

n = nrow(df)
k = ncol(X)

VCV <- (1/(n-k))*as.numeric(t(res)%*%res)*solve(t(X)%*%X)

```

#### Standard Error

We can calculate the standard error from a variance covariance matrix as the square root of the diagonal values. Doing this

```{r, message = FALSE, warning = FALSE}

se <- sqrt(diag(VCV))

```

So using the equations and matrices results in the following output

```{r, message = FALSE, warning = FALSE}

p_value <- rbind(
  2*pnorm(abs(beta[1]/se[1]), lower.tail = FALSE),
  2*pnorm(abs(beta[2]/se[2]), lower.tail = FALSE),
  2*pnorm(abs(beta[3]/se[3]), lower.tail = FALSE),
  2*pnorm(abs(beta[4]/se[4]), lower.tail = FALSE)
)

#... Output ----

output <- as.data.frame(
  cbind(
    c("Intercept", "Age", "Sex"),
    round(beta,5),
    round(se, 5),
    round(p_value, 4)
)
)

names(output) <- c(
  "Coefficients",
  "Estimate",
  "Std. Error",
  "Pr(>{Z}"
)

output

```

#### Comparing to lm()

Comparing these values to if we use the *lm()* function in R

```{r, warning = FALSE, error = FALSE}

mod.fit <- lm(num_ballons ~ age + sex + candy_lover, 
              data = df)

summary(mod.fit)

```

Now that we have covered linear regression, we are ready to move onto generalized linear models (GLM).

## Generalized Linear Model

### Main Parts of GLM

There are three components to any GLM [@stat504]:

1.  Random Component. This specifies the probability distribution of the response variable. Essentially, we are selecting the distribution for the

2.  Systematic Component

3.  Link Function

Another way to think of these three components are as the response variable (aka y), explanatory variables (aka x) and how they are connected.

There are some assumptions as well:

-   The data are independently distributed

-   The dependent variable typically assumes a distribution from an exponential family (i.e., normal, binomial, Poisson, etc.)

-   A linear relationship between the transformed expected response in terms of the link function and explanatory variables (however ***not*** in terms of the response and explanatory variables)

-   Errors need to be independent but not normally distributed

### Maximum Likelihood Estimation

One of the key differences between OLS and GLM is the way that parameters (aka coefficients) are estimated. While OLS uses ordinary least squares, GLMs use something called maximum likelihood estimation (MLE). MLE is like what it sounds: it maximizing the likelihood function so that the model uses values that make the observed data most probable (aka most likely). There are different methods to determine this value for MLE including solving the derivative of the likelihood funciton where it is 0, hence the maxima, or more iterative procedures such as Gradient descent method or Newton-Raphson method.

Here we will focus on iteratively reweighted least squares since that is what R uses by default.

#### Iteratively Reweighted Least Squares

Iteratively reweighted least squares (IWLS), is an algorithm that is used to determine the parameters and standard errors of the parameters. We'll use logistic regression to walk through the steps, although for other link functions it is a similar process. The steps for IWLS are outlined below [@fox2014]

1.  Set the regression coefficients to initial values. For our example, we will start with 0

2.  For each iteration, *t,* calculate the fitted probabilities, $\mu$, variance-function values, $v$, working-response values, $z$, and weights, $w$.

    $\mu_i^{(t)} = [1 + exp(-\eta_i^{(t)})]^{-1}$

    $v_i^{(t)} = \mu_i^{(t)}(1-\mu_i^{(t)})$

    $z_i^{(t)} = \eta_i^{(t)} + (y_i - \mu_i^{(t)})/v_i^{(t)}$

    $w_i^{(t)} = n_i*v_i$

    Note: $n_i$ represents the binomial denominator for the ith observation. For binary data, all of the $n_i$ are 1.

3.  Regress the working response on the predictors using weighted least squares, minimizing the weighted residual sum of squares

    $\sum\limits_{i = 1}^{n}w_i^{(t)}(z_i^{(t)} - x_i^{'}\beta)^2$

    where $x_i^{'}$ is the *i*th row of the model matrix.

4.  Repeat steps 2 and 3 until the regression coefficients stabilize at the maximum-likelihood estimator $\hat\beta$

5.  Calculate the estimated asymptotic covariance matrix of the coefficients as

    $\hat{V}(\hat{\beta}) = (X^{'}WX)^{-1}$

    where $W = \text{diag}\text{(}w_i\text{})$ is the diagonal matrix of weights from the last iteration and $X$ is the model matrix.

Reading through steps can be helpful but an example is always better. Let's work through this in R. First we'll want to make a function to calculate IWLS implementing these steps (credit to Michael Clark for code for implementing IWLS, link in below code snippet)

```{r, message = FALSE, warning = FALSE}

# Link to code that this is built off of: 

# https://m-clark.github.io/models-by-example/newton-irls.html#comparison-42

iwls <- function(X, y, tol = 1e-7, iter = 500){
  
  # Note: tol = 1e-7 is used by the lsfit function
  
  # First we need to start with some inital values
  
  int = log(mean(y)) / (1-mean(y)) # intercept
  beta = c(int, rep(0, ncol(X) -1))
  currtol = 1
  it = 0
  ll = 0 # log likelihood
  
  # As long as the tolerance calculate is greater than what we will allow we want the code to repeat
  
  while(currtol > tol && it < iter){
    it = it + 1
    ll_old = ll
    
    eta = X %*% beta
    mu = plogis(eta)[,1]
    s = mu*(1-mu)
    S = diag(s)
    z = eta + (y-mu)/s
    beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z))
    var = solve((t(X) %*% S %*% X))
    
    ll = sum(
      dbinom(
        y, 
        prob = plogis(X%*% beta), 
        size = 1, 
        log = TRUE)
      )
    
    currtol = abs(ll - ll_old)
  }
  
  list(
    beta = beta, 
    var = var, 
    se = diag(sqrt(var)), # SE = sqrt(var) but we want diagnoals of the variance-covariance matrix 
    iter = it, 
    tol = currtol, 
    loglik = ll, 
    weights = plogis(X %*% beta) * (1 - plogis(X %*% beta))
  )
  
}
```

### Comparing our method to glm()

```{r, message = FALSE, warning = FALSE}

library(tidyverse)

X <- cbind(1, df$age, df$sex, df$num_ballons) %>% as.matrix()
y <- df$candy_lover %>% as.matrix()

our.way <- iwls(X, y)

our.way$beta

summary(our.way)

diag(our.way$se)

glm.way <- glm(candy_lover ~ age + sex + num_ballons, 
               family = binomial(link = "logit"), 
               data = df)

glm.way$coefficients
summary(glm.way)
```

If we compare the estimates from the *glm()* function to the our function, the results are fairly similar. For example, sex has an estimate of 0.680716 (SE: 0.305779) with the glm function and 0.68071575 with ours (SE: 0.30577906). Nearly identical results! Yippee!

## Conclusion

Since every good story must come to an end, so too does our glm by hand exercise...but fear not! You can now use this to foray into the world of glm with better knowledge!
