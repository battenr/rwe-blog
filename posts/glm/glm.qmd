---
ttitle: "Generalized Linear Model"
author: "Ryan Batten"
date: "2022-08-13"
categories: [GLM, Regression]
# image: "iptw.jpeg"
# bibliography: ip_weighting.bib
draft: true
---

## Regression: Swiss Army Knife of Statistics

Regression is a commonly used tool in statistics. Typically, the starting point is with

What does GLM actually stand for? Well, it stands for

## Main Parts of GLM

Re-weighting in this context has nothing to do with weight. Instead it is a statistical method that is used to adjust for effect modification and confounding to ensure exchangeability. This method can be helpful for answering questions about the ***marginal or conditional*** causal effect.

Now, what is a better way to get started than with probability yet again. In this case, we are going to talk about the probability of receiving treatment.

::: callout-note
This post will draw heavily from Chapter 12 of What If [@hernanwhatif]
:::

## Propensity Score

A term that is commonly used in clinical epidemiology is the ***propensity score***. The propensity score is the conditional probability of receiving treatment [@hernanwhatif], or in mathematical notation:

$$
Pr[A = 1| L = l]
$$

Where A is treatment and L is covariates.

Imagine we have a clinical trial with two groups. The treatment group get to bounce on a trampoline for 10 minutes, while the control group have to sit down on a chair for 10 minutes. In this scenario, the propensity score would be the conditional probability of getting to bounce on the trampoline.

While theoretical discussions are helpful, numbers always help to really drive a point home. Luckily, once again there is a database that has already collected data on such a scenario.

## Pseudo-Population

The goal of reweighing is to make a pseudo-population where exchangeability holds. To do this, we fit a logistic regression where the outcome is treatment.

## General Formula for IP Weighting

The below formula is from [@hernanwhatif, pp.153].

$$
W^a = \frac{p}{f[A|L]}
$$

where $p$ is the unconditional probability of treatment. Note: $0< p \leq 1$, whereas $f[A|L]$ is the probability of treatment based on covariates L. A common choice is to use $Pr[A = 1]$ for $p$ in the treated and $Pr[A=0]$ for $p$ in the untreated. $Pr$ in this case means the proportion. More compactly:

$$
\frac{f(A)}{f[A|L]}
$$

## Stabilized Weight

$$
SW^a = \frac{f(A)}{f[A|L]}
$$

Mean of the stabilized weights should be 1. This is important to check when conducting an analysis using stabilized weights.

## Stabilized or Nonstabilized?

At this point, you may be wondering to yourself if we should be using stabilized or nonstabilized weights. One reason is stabilized weights result in narrower 95% CIs. You should use stabilized weights when your model is not saturated (i.e., there are the same amount of unknowns on both sides of the equations)

## Example

An example always helps. Let's jump right into our made up dataset. We have 5 variables: sex, sunglasses, age, love for disney princess and ice cream eaters. Now for this dataset, we want to know if loving Disney movie causes people to become ice cream lovers. Below is a summary table of

```{r, cache = TRUE, include = FALSE, message = FALSE, echo - FALSE}

library(tidyverse)

set.seed(202208)

df.disney.lover <- data.frame(
  sex = rbinom(577, size = 1, prob = 0.19), # 1 = female
  age = runif(577, min = 5, max = 60),
  disney_lover = 1,
  sunglasses = rbinom(577, size = 1, prob = 0.37),
  ice_cream = rbinom(577, size = 1, prob = 0.22)
)

df.disney.hater <- data.frame(
  sex = rbinom(206, size = 1, prob = 0.61), # 1 = female
  age = runif(206, min = 25, max = 80),
  disney_lover = 0,
  sunglasses = rbinom(206, size = 1, prob = 0.50),
  ice_cream = rbinom(206, size = 1, prob = 0.22)
)

# Age

cbind(mean(df.disney.lover$age), sd(df.disney.lover$age))
cbind(mean(df.disney.hater$age), sd(df.disney.hater$age))

# Sex

n.female.dl = df.disney.lover %>% dplyr::filter(sex == 1) %>%  count()
cbind(n.female.dl, round(n.female.dl/577, 3)*100)

n.female.ndl = df.disney.hater %>% dplyr::filter(sex == 1) %>%  count()
cbind(n.female.ndl, round(n.female.ndl/206, 3)*100)

# Sunglasses

n.sg.dl = df.disney.lover %>% dplyr::filter(sunglasses == 1) %>%  count()
cbind(n.sg.dl, round(n.sg.dl/577, 3)*100)

n.sg.ndl = df.disney.hater %>% dplyr::filter(sunglasses == 1) %>%  count()
cbind(n.sg.ndl, round(n.sg.ndl/206, 3)*100)

# Ice Cream Lovers

n.ic.dl = df.disney.lover %>% dplyr::filter(ice_cream == 1) %>%  count()
cbind(n.ic.dl, round(n.ic.dl/577, 3)*100)

n.ic.ndl = df.disney.hater %>% dplyr::filter(ice_cream == 1) %>%  count()
cbind(n.ic.ndl, round(n.ic.ndl/206, 3)*100)

```

+-------------------------+--------------+------------------+
| Variables               | Disney Lover | Not a Disney Fan |
|                         |              |                  |
|                         | (n = 577)    | ( n = 206)       |
+:=======================:+:============:+:================:+
| Age, mean (SD)          | 31.8 (16.3)  | 54.1 (16.0)      |
+-------------------------+--------------+------------------+
| Female, n (%)           | 110 (19.1%)  | 119, (57.8)      |
+-------------------------+--------------+------------------+
| Sunglasses, n (%)       | 209 (36.2%)  | 98 (47.6%)       |
+-------------------------+--------------+------------------+
| Ice Cream Lovers, n (%) | 126 (21.8%)  | 51 (24.8%)       |
+-------------------------+--------------+------------------+
