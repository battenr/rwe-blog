---
ttitle: "Generalized Linear Model"
author: "Ryan Batten"
date: "2022-08-28"
categories: [GLM, Regression]
# image: "iptw.jpeg"
bibliography: glm.bib
draft: true
format: 
  html:
    code-fold: true
---

## Regression: Swiss Army Knife of Statistics

Regression is a commonly used tool in inferential statistics. Before we start however, I'd like to highlight that regression is not always necessary. Sometimes, descriptive statistics are a preferred tool as opposed to a formal model that is testing a hypothesis to make inferences about a super-population. With that being said, regression can be used in multiple situations and flexible to a wide range of datasets. One last point before we get started, my favorite quote by George Box "All models are wrong, some are useful".

Without further ado, let's get started. The real question is where to start? Typically, when most people initially begin learning about regression they think about, or are taught, ordinary least squares (OLS) regression. Seems like a reasonable place to me to begin as well! However, we will back up a little bit and start with the equation of a line.

## Equation of a Line

From high school math, you probably remember the equation of a line as:

$$
y = mx +b
$$

where $m$ is the slope of the line, $x$ is the value of the dependent variable and $b$ is the y-intercept of the line. A key exercise that is often used in math, using graph paper, is to first plot data points based on their coordinates (x, y), then draw a line of best fit through them. The goal of drawing this line is capture as many data points on the line as possible, or closest to the line. OLS regression is similar to this.

## Plain Old Ordinary Regression

OLS regression tries to minimize the sum of squares of the differences between the observed variable and those predicted by the model (hence the *least squares*). Put otherwise: by minimizing the residuals. Another way to conceptualize this is by drawing the line of best fit. \
\
For example, imagine we have a database that we can use to ask a burning question we have: is the number of balloons a person owns affected by their age? Our database has four variables: age, sex, candy lover (yes/no) and the number of balloons the person owns. Now, if we draw a line of best fit through the data, using number of balloons as the outcome/y/dependent variable, and age as the predictor/x/independent variable... we get the below figure.

```{r, message = FALSE, warning = FALSE}

set.seed(2228) # August 28, 2022

library(tidyverse)
library(ggxmean)

n.id = 250

df <- data.frame(
  age = runif(n = n.id, min = c(5, 18), max = c(30, 85)),
  sex = rbinom(n = n.id, size = 1, prob = c(0.60,0.20)),
  candy_lover = rbinom(n = n.id, size = 1, prob = c(0.64, 0.45)),
  
  # Beta Coefficients
  
  beta_age = runif(n = n.id, min = 0, max = 0.10),
  beta_sex = runif(n = n.id, min = 0, max = 0.30),
  beta_candy = runif(n = n.id, min = 0, max = 0.40)
) %>% 
  dplyr::mutate(
    num_ballons = round((beta_age*age + beta_sex*sex + beta_candy*candy_lover)*3,0) # number of ballons that you own
  )

ggplot2::ggplot(data = df, mapping = aes(x = age, y = num_ballons)) +
  ggplot2::geom_point(color = "red") +
  geom_lm() +
  ggxmean::geom_lm_residuals(linetype = "dashed") +
  labs(x = "Age", y = "Number of Balloons Owned") +
  theme_minimal() + 
  ggtitle("Number of Balloons by Age") +
  scale_x_continuous(limits = c(0, 100)) + 
  scale_y_continuous(limits = c(0, 30)) + 
  theme(plot.title = element_text(hjust = 0.5))


```

Let's break down this figure a little. First, let's focus on the distance from the red dot to the blue line for each data point. This measurement is from the observed value (red dot) to the predicted value (on the blue line). This is also known as the ***residual*** (in the figure the dashed black line). By drawing the line of best fit, we are aiming to minimize the residuals.

Of course, this is great in theory to draw the line of best fit, however it can get more complicated when there are additional variables we'd like to adjust for. For example, we may also want to adjust for whether a person loves candy or not since candy buyers tend to buy more balloons. Furthermore, we may also want to adjust for sex. To do this, a more useful approach is to use a mathematical equation.

### Using Equations and Matrices

Instead of drawing a line, we can find the estimated values of the parameters (i.e., $\beta_1$, $\beta_2$, $\beta_3$) using the below equation. Note that these variables are referring to the matrix, rather than a specific variable.

$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

where $\hat{\beta}$ is the ordinary least squares estimator, $X$ is the matrix containing the predictor variables and $y$ is the vector of the response variable. For our example, $X$ is the matrix containing the values of an intercept and values for the following variables: age, sex and candy lover.

::: callout-note
For those not familiar with linear algebra, the superscript T is called the transpose. Similarly, the -1 superscript is referred to as the inverse of a matrix.
:::

Abstract concepts can be helpful however an example using data is always better. Using our data from before on balloons and age, we can create our matrix of predictors, $X$. In our matrix, we will also include a column of all 1s to reflect the intercept.

Looking at the first six individuals, we get a sense of what the matrix looks like.

```{r, message = FALSE, warning = FALSE}

X = as.matrix(cbind(1, df$age, df$sex, df$candy_lover))

head(X)

```

As you can see, the first column is all 1s. The second is the value for age, third is sex (1 = female, 0 = male) and fourth is the value for candy lover (1 = yes, 0 = no). Similarly, for the $y$ matrix:

```{r, message = FALSE, warning = FALSE}

y = as.matrix(df$num_ballons)

head(y)
```

We can see that the $y$ matrix is essentially just the outcome variable of interest. Now, how do we estimate the parameters? For that, we turn to our handy dandy formula.

#### Estimating Parameters

We can estimate the parameters using our handy dandy formula below and plugging in our matrices where applicable.

$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

Using the equation above:

```{r, message = FALSE, warning = FALSE}

# Doing it step by step:

step.1 <- t(X)%*%X
step.2 <- solve(step.1) # solve will return the inverse of a
step.3 <- step.2%*%t(X)
step.4 <- step.3%*%y
beta <- step.4

# Alternatively, can do in one messy looking code: 

beta <- solve(t(X)%*%X)%*%t(X)%*%y

# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code all in one

beta %>% 
  as.data.frame() %>% 
  dplyr::mutate(
    variable = c("intercept", "age", "sex", "candy lover")
  ) %>% 
  dplyr::rename(
    estimate = V1
  ) %>% 
  dplyr::select(
    variable, estimate
  )
```

The next step is to determine the standard error for our estimates. To do this, we first need the variance which we can get from the variance-covariance matrix. How can we determine this variance-covariance matrix? Another formula!

#### Estimating Variance

In order to calculate the variance-covariance matrix, we first need to calculate the residuals. We can do that using the below formula:

$$
Residuals = y - \beta_1 - \beta_2*age - \beta_3*sex - \beta_4*\text{candy lover}
$$

Once we know the residuals, as a matrix, we can calculate the variance-covariance matrix as

$$
VCov = \frac{1}{n-k}(RES^TRES)*(X^TX)^{-1}
$$

where $n, k, RES, X$ are the number of observations, number of parameters estimated, matrix of residuals and matrix of values for the predictor variables. Back to our example, since working through an example is always best:

```{r, message = FALSE, warning = FALSE}

# credit to https://economictheoryblog.com/2016/02/20/rebuild-ols-estimator-manually-in-r/ for the code 

# Calculating residuals as y-intercept-beta_1*X1-beta_2*X2

res <- as.matrix(y-beta[1]-beta[2]*X[,2]-beta[3]*X[,3] - beta[4]*X[,4])

# Note the above is really:
# res <- as.matrix(y-beta[1]-beta[n]*X[,n]) for as many n's as there are 

# Variance-Covariance Matrix (VCV) 

# Formula for VCV: (1/df)*t(res)*res*[t(X)(X)]^-1

n = nrow(df)
k = ncol(X)

VCV <- (1/(n-k))*as.numeric(t(res)%*%res)*solve(t(X)%*%X)

VCV
```

Ta-da! We have our variance-covariance matrix! You may be asking yourself "so what?", well this matrix is how we can determine the standard error. The diagonals of the matrix are the variance for each of the variables. If we take the square root of the variance, we will get the standard deviation. In this case the standard deviation of the sampling distribution, or the standard error.

```{r, message = FALSE, warning = FALSE}

se <- sqrt(diag(VCV))

```

The last piece to our puzzle is to add some p-values. While we're at it, let's make it a little easier to read by adding in some formatting and text to help the reader make sense of our results.

```{r, message = FALSE, warning = FALSE}

p_value <- rbind(
  2*pnorm(abs(beta[1]/se[1]), lower.tail = FALSE),
  2*pnorm(abs(beta[2]/se[2]), lower.tail = FALSE),
  2*pnorm(abs(beta[3]/se[3]), lower.tail = FALSE),
  2*pnorm(abs(beta[4]/se[4]), lower.tail = FALSE)
)

#... Output ----

output <- as.data.frame(
  cbind(
    c("Intercept", "Age", "Candy Lover"),
    round(beta,5),
    round(se, 5),
    round(p_value, 4)
)
)

names(output) <- c(
  "Coefficients",
  "Estimate",
  "Std. Error",
  "Pr(>{Z}"
)

output

```

#### Comparing to lm()

Great so we have worked through calculating OLS regression by hand, now what? Glad you asked! How do we know it worked? One way to check is to compare the values we got with those if we use the *lm()* function in R.

```{r, warning = FALSE, error = FALSE}

mod.fit <- lm(num_ballons ~ age + sex + candy_lover, 
              data = df)

summary(mod.fit)

```

| Variable    | Estimate (SE) by hand | Estimate (SE) using *lm()* |
|-------------|-----------------------|----------------------------|
| Intercept   |                       |                            |
| Age         |                       |                            |
| Sex         |                       |                            |
| Candy Lover |                       |                            |

: Linear Regression by Hand versus lm()

As you can see above, the results obtained by calculating by hand are very similar to those using the *lm()* function. Now that we have covered linear regression, we are ready to move onto generalized linear models (GLM).

::: callout-note
This post focuses on how the estimates are derived for linear regression, not the assumptions or diagnostics that can be used to check these assumptions. It's always important to check the assumptions to see if any of them are violated, for example if the assumption of homoscedasticity for the residuals. A good resource for reviewing the assumptions of OLS is by Jim Frost (https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)
:::

## Generalized Linear Model

For any statistical method there are certain assumptions that have to be met. For example, with OLS regression one of the key assumptions is homoscedasticity for the residuals (see list of assumptions here https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/). Sometimes these assumptions are not met. Using homoscedasticity as an example, sometimes the variance of the errors is not consistent across observations. Due to this violation, using OLS regression may provide inaccurate results. One solution to this is to use a generalized linear model (GLM), however GLMs have their own set of assumptions. But first, we need to review that a GLM is!

::: callout-note
Why GLMs over OLS? Well there are a few reasons [@stat504]:

-   Models are fitted via MLE instead of OLS. This relaxes some of the assumptions required for OLS such as homogeneity of the residuals and the need for residuals to be normally distributed.

-   No need to transform the outcome variable to have a normal distribution

-   Choosing the link has nothing to do with what you choose for the random component. For example, you can use a binomial distribution but choose to use a logit link or probit.
:::

### Main Parts of GLM

There are three components to any GLM [@stat504]:

1.  Random Component. This specifies the probability distribution of the response variable. Essentially, we are selecting the distribution for the outcome variable of interest. For example, assume we want to examine whether candy lovers own more balloons. For this question, we would assume that candy lovers is a binomial distribution (i.e., yes/no). Note that there is no separate error term for a GLM.

2.  Systematic Component. This outlines the explanatory variables. Again, using the example above, the systematic component is linear since it will be $\beta_0 + \beta_1*age + \beta_2*sex + \beta_3*\text{number of balloons}$

3.  Link Function. The link function is a crucial component of GLMs. This specifies how the random and systematic components are connected. For our example, the link function is logit. If our outcome variable were continuous it would be the identity link function (why it's called the identity link function has never made sense to me. Think of it as the one you are used to: y = mx + b).

Another way to think of these three components are as the response variable (aka y), explanatory variables (aka x) and how they are connected. Like any statistical technique, there are assumptions as well [@stat504]:

-   The data are independently distributed

-   The dependent variable typically assumes a distribution from an exponential family (i.e., normal, binomial, Poisson, etc.)

-   A linear relationship between the transformed expected response in terms of the link function and explanatory variables (however ***not*** in terms of the response and explanatory variables)

-   Errors need to be independent but not normally distributed (this is a key difference between OLS regression and GLMs)

### Maximum Likelihood Estimation

One of the key differences between OLS and GLM is the way that parameters (aka coefficients) are estimated. While OLS uses ordinary least squares, GLMs use something called maximum likelihood estimation (MLE). MLE is like what it sounds like: it's about maximizing the likelihood function so that the model uses values that make the observed data most probable (aka most likely). There are different methods to determine this value for MLE including solving the derivative of the likelihood funciton where it is 0, hence the maxima, or more iterative procedures such as the Gradient descent method or the Newton-Raphson method. Here we will focus on iteratively reweighted least squares (IWLS) since that is what R uses by default.

#### Iteratively Reweighted Least Squares

IWLS, is an algorithm that is used to determine the parameters and standard errors of the parameters. We'll use logistic regression to walk through the steps, although for other link functions it is a similar process. The steps for IWLS are outlined below [@fox2014]

1.  Set the regression coefficients to initial values. For our example, we will start with 0

2.  For each iteration, *t,* calculate the fitted probabilities, $\mu$, variance-function values, $v$, working-response values, $z$, and weights, $w$.

    $\mu_i^{(t)} = [1 + exp(-\eta_i^{(t)})]^{-1}$

    $v_i^{(t)} = \mu_i^{(t)}(1-\mu_i^{(t)})$

    $z_i^{(t)} = \eta_i^{(t)} + (y_i - \mu_i^{(t)})/v_i^{(t)}$

    $w_i^{(t)} = n_i*v_i$

    Note: $n_i$ represents the binomial denominator for the ith observation. For binary data, all of the $n_i$ are 1.

3.  Regress the working response on the predictors using weighted least squares, minimizing the weighted residual sum of squares

    $\sum\limits_{i = 1}^{n}w_i^{(t)}(z_i^{(t)} - x_i^{'}\beta)^2$

    where $x_i^{'}$ is the *i*th row of the model matrix.

4.  Repeat steps 2 and 3 until the regression coefficients stabilize at the maximum-likelihood estimator $\hat\beta$

5.  Calculate the estimated asymptotic covariance matrix of the coefficients as

    $\hat{V}(\hat{\beta}) = (X^{'}WX)^{-1}$

    where $W = \text{diag}\text{(}w_i\text{})$ is the diagonal matrix of weights from the last iteration and $X$ is the model matrix.

Reading through steps can be helpful but an example is always better. Let's work through this in R. First we'll want to make a function to calculate IWLS implementing these steps (credit to Michael Clark for code for implementing IWLS, link in below code snippet)

```{r, message = FALSE, warning = FALSE}

# Link to code that this is built off of: 

# https://m-clark.github.io/models-by-example/newton-irls.html#comparison-42

iwls <- function(X, y, tol = 1e-7, iter = 500){
  
  # Note: tol = 1e-7 is used by the lsfit function
  
  # First we need to start with some inital values
  
  int = log(mean(y)) / (1-mean(y)) # intercept
  beta = c(int, rep(0, ncol(X) -1))
  currtol = 1
  it = 0
  ll = 0 # log likelihood
  
  # As long as the tolerance calculate is greater than what we will allow we want the code to repeat
  
  while(currtol > tol && it < iter){
    it = it + 1
    ll_old = ll
    
    eta = X %*% beta
    mu = plogis(eta)[,1]
    s = mu*(1-mu)
    S = diag(s)
    z = eta + (y-mu)/s
    beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z))
    var = solve((t(X) %*% S %*% X))
    
    ll = sum(
      dbinom(
        y, 
        prob = plogis(X%*% beta), 
        size = 1, 
        log = TRUE)
      )
    
    currtol = abs(ll - ll_old)
  }
  
  list(
    beta = beta, 
    var = var, 
    se = diag(sqrt(var)), # SE = sqrt(var) but we want diagnoals of the variance-covariance matrix 
    iter = it, 
    tol = currtol, 
    loglik = ll, 
    weights = plogis(X %*% beta) * (1 - plogis(X %*% beta))
  )
  
}
```

### Comparing our method to glm()

Now that we've written a function that calculates estimates of the parameters and standard errors, we need to see if it works! What's a better way to check than comparing with a well-established method? We'll compare our function to that from glm, although admittedly our output is not as clean.

```{r, message = FALSE, warning = FALSE}

library(tidyverse)

X <- cbind(1, df$age, df$sex, df$num_ballons) %>% as.matrix()
y <- df$candy_lover %>% as.matrix()

our.way <- iwls(X, y)

our.way$beta

summary(our.way)

diag(our.way$se)

glm.way <- glm(candy_lover ~ age + sex + num_ballons, 
               family = binomial(link = "logit"), 
               data = df)
```

+---------------------------+----------------------------------+------------------------------+
| Parameter                 | Estimate (SE)                    | Estimate (SE)                |
|                           |                                  |                              |
|                           | *glm()*                          | our method                   |
+:=========================:+:================================:+:============================:+
| Intercept                 | 0.233782 (0.300512)              | 0.233783 (0.300512)          |
+---------------------------+----------------------------------+------------------------------+
| Age                       | -0.013238 (0.008063)             | -0.013238 (0.008063)         |
+---------------------------+----------------------------------+------------------------------+
| Sex                       | 0.680716 (0.305779)              | 0.680716 (0.305779)          |
+---------------------------+----------------------------------+------------------------------+
| Number of Balloons        | 0.068098 (0.35405)               | 0.068098 (0.035405)          |
+---------------------------+----------------------------------+------------------------------+

: Output from our function compared to glm()

Looking at the estimates from the *glm()* function to our function...nearly identical results! Yippee! This is also what we'd expect. We can also look at the weights from the last iteration for both the *glm()* method and using our function. The code is in the below snippet, however rather than boring you with weights for 250 observations, I will leave that up to you to review if you are interested (tldr: they are quite similar).

```{r, message = FALSE, warning = FALSE}

result.weights <- cbind(glm.way$weights, our.way$weights) %>% as.data.frame()

```

## Conclusion

Since every good story must come to an end, so too does our GLM by hand exercise...but fear not! You can now use this to foray into the world of GLM with a better understanding of how these parameters are calculated!
