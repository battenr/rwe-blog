---
title: "Resampling Magic"
subtitle: "The Wonders of Bootstrapping and Jackknifing Explained"
author: "Ryan Batten"
date: "2023-08-01"
categories: [Resampling, Bootstrapping, Jackknifing]
bibliography: bootstrap.bib
draft: true
format: 
  html:
    code-fold: true
---

## Certainly Uncertain

A key part of any analysis is to determine the uncertainty associated with an estimate. This has to do with determining the variance of the estimate, which we can use to calculate the standard error. Sometimes this can be especially tricky with certain methods such as inverse probability weighting and standardization. One way to estimate this is using something called bootstrapping.

## What is bootstrapping?

No, not like Bootstrap Bill (Pirates of the Caribbean), bootstrapping is a statistical technique that can be quite useful. We can use is to estmate confidence intervals, for hypothesis testing, model validation and estimating treatment effects just to name a few things. So how exactly does it work?

## How does it work?

Bootstrapping works by drawing repeated samples, with replacement, from our observations. Imagine that we have have a big tub of ice cream that looks like a rainbow, with a bunch of colors all mixed together. We want to know what the whole tub looks like, so what we do is take one scoop and look at the colors in that scoop. Then we put it back, mix it up and take another scoop. We can do this over and over again to try and understand what the whole tub looks like!\
\
This is what bootstrapping it. We take repeated samples to try to understand what the whole picture looks like. Let's take a different example and put some numbers to it!

## Mini-Golf Examples

Let's use mini-golf as an example. Imagine that you play mini-golf with your friends 100 times over a summer. Each time you play, you get a different score (the lower the better). Sometimes you do well, sometimes you play awful.\
\
Now you want to know: "am I any good at mini-golf? What would my score be on average?" So you decide to use bootstrapping to answer this. Here's the following steps

1.  Look at 10 scores on random days and write them down.
2.  Put them back.
3.  Draw another 10 scores and write them down.
4.  Put them back.
5.  Repeat steps 1-4

Now we have our plan, let's start implementing it!

## Mini-Golf - Average Score

So we know our average score, if we just take the mean. But what is the confidence interval for this mean? Well, from statistics we know that the standard error is the standard deviation of it's *sampling distribution*. The sampling distribution is the probability distribution of a statistic. This may sound like a bit of jargon. In simpler \[RYAN: consider adding a graph\]

For example, if we have a bunch of mean values (say 1000) then that is our sampling distribution.

```{r scores}

library(tidyverse)

# Your 100 mini-golf scores
original_scores <- sample(40:60, 100, replace = TRUE)

# Initialize a variable to store the bootstrap estimates
bootstrap_estimates <- numeric()

# Number of bootstraps
n_bootstraps <- 1000

# Bootstrapping
for (i in 1:n_bootstraps) {
  bootstrap_sample <- sample(original_scores, size = 10, replace = TRUE)
  bootstrap_mean <- mean(bootstrap_sample)
  bootstrap_estimates <- c(bootstrap_estimates, bootstrap_mean)
}

# Estimate the 95% confidence interval
estimated_ci <- quantile(bootstrap_estimates, probs = c(0.05, 0.95))

estimated_mean <- mean(original_scores)

```

Now we know that our average score is `r estimated_mean` and our confidence interval is `r estimated_ci`

However, this is the mean. It doesn't account for the number of obstacles on the course or if it was a weekend (perhaps we'd feel more rushed if we were playing on the weekend). Let's do this example again but account for those additional variables.

## Mini-Golf: GLM

We'll use a generalized linear model for our example. Side note: there is no rationale for picking a GLM, other than it's a method that I'm familiar with. In reality, picking the model to use isn't always so straightforward.\
\
Now, let's simulate some data!

```{r golf-glm}
set.seed(123) # For reproducibility

# Number of games
n_games <- 100

# Number of obstacles (simulated)
obstacles <- round(runif(n_games, 5, 15))

# Weekend (1 if weekend, 0 otherwise)
weekend <- sample(0:1, n_games, replace = TRUE)

# Mini-golf scores (simulated)
scores <- 50 + 2 * obstacles + 5 * weekend + rnorm(n_games, 0, 5)

# Data frame
mini_golf_data <- data.frame(scores, obstacles, weekend)

```

Firs things first with our data, let's fit a GLM.

Now that we have our data, we can create some bootstrap samples.

```{r bootstrap}

# We'll use the tidymodels package here

library(tidymodels)

boots <- bootstraps(mini_golf_data, times = 1000, apparent = TRUE)

# Now we'll create a function for fitting our model. In this case a GLM

fit_glm <- function(df){
  glm(scores ~ obstacles + weekend, 
      family = gaussian(link = "identity"),
      data = df 
      )
}

# Now let's fit this model for each bootstrap sample

boot_glms <- boots %>% 
  dplyr::mutate(
    model = map(splits, fit_glm),
    coef_into = map(model, tidy)
  )


boot_coefs <- 
  boot_glms %>% 
  unnest(coef_into)








```

```{r boot-ci}

# Now we can calculate the confidence interval by taking the 0.025 and 0.95 quantiles. 

int_pctl(boot_glms, coef_into)
```

## Why not use bootstrapping all the time?

Like any method, bootstrapping isn't perfect. There's benefits and drawbacks to using it. For example, it can give different results depending on the seed that is chosen. Below are a list of pro/cons:

Pros:

-   

Cons:

-   While it's a nonparametric method, we are still assumption that the distribution is a certain shape. If it is skewed, then our result won't be correct. One way to solve this is to use a bias-corrected and accelerated bootstrap

## Vary Away!

"So what you're telling me is I can just use bootstrapping all the time for every test?" Well, no. It is a useful tool but like all tools, whether statistical or not, there is a time and a place to use them. Would you use a hammer as a fly swatter? I guess you could, but why not just use a fly swatter.

## What's Next?

Bootstrapping is a wide field. I highly recommend the textbook by Hinkely et al. and to try using bootstrapping yourself!

## From ChatGPT \[Need to Edit\]

## **Pros and Cons of Bootstrapping**

Bootstrapping is a widely-used technique in statistics for its simplicity and versatility. However, like any method, it has its strengths and weaknesses. Below, we explore some of the pros and cons of using bootstrapping.

### **Pros**

1.  **Simplicity**: Bootstrapping is conceptually simple and easy to implement. It doesn't require strong assumptions about the underlying distribution of the data.

2.  **Versatility**: It can be applied to a wide range of statistics and models, making it useful for various types of data analysis, including causal inference.

3.  **Small Sample Sizes**: Bootstrapping can be particularly useful when you have a small sample size, as it allows you to make more robust inferences.

4.  **Non-Parametric**: It's a non-parametric method, meaning it doesn't assume a specific form for the distribution of the data. This makes it flexible and widely applicable.

5.  **Computational Efficiency**: With modern computing power, bootstrapping can be done quickly, even for large datasets.

6.  **Confidence Intervals**: Easily compute confidence intervals for complex estimators where analytical solutions may not be available.

7.  **Model Validation**: Can be used for internal validation of models, assessing the stability of results.

### **Cons**

1.  **Not Always Accurate**: The accuracy of bootstrapping depends on the data and the statistic being estimated. It may not perform well for highly skewed data or for statistics that are sensitive to outliers.

2.  **Computational Cost**: While generally efficient, bootstrapping can be computationally expensive if the dataset is large or if a large number of bootstrap samples are needed.

3.  **Independence Assumption**: Assumes that the observations are independent. If this assumption is violated (e.g., in time-series data), then bootstrapping may give misleading results.

4.  **Limited External Validity**: Bootstrapping only resamples from the existing data, so it can't capture variations or features not present in the original sample. This limits its external validity.

5.  **Pseudo-Replication**: Since bootstrapping involves resampling with replacement, it creates "pseudo-replicates" rather than true replicates, which may not fully capture the uncertainty in some cases.

6.  **Boundary Estimates**: For some statistics, bootstrapping can produce biased confidence intervals that do not contain the true parameter value, especially for small sample sizes.

7.  **Not a Substitute for Real Data**: While bootstrapping can enhance an analysis, it's not a substitute for collecting more or better-quality data.

### **Conclusion**

Bootstrapping is a powerful tool but should be used thoughtfully. Understanding its limitations is crucial for interpreting the results correctly, especially in complex analyses like causal inference. Given its pros and cons, it often serves as a useful complement to other statistical methods.
