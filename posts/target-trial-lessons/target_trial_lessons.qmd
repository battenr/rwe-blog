---
title: "Emulating a Target Trial" # November Post 
subtitle: "Lessons Learned" # Titles are placeholders
author: "Ryan Batten"
date: "2023-12-28"
categories: [Target Trial, Lessons Learned]
bibliography: tt-lessons.bib
#image : tt-lessons.jpeg
format: 
  html:
    toc: true
    toc-title: Contents
    toc-location: right
    toc-depth: 4
    code-fold: true
---

# What is a Target Trial, Why Emulate One?

A target trial is the trial that we'd like to conduct under ideal circumstances. Emulating a target trial consists of trying to mimic this trial as much as possible given the constraints (i.e., ethics, data availability, etc). For example, the target trial may be randomizing patients but we can't randomize patients that are in an observational study. For a brief overview of it, I'd recommend reading @hernan2016using or @fu2023target.

Targe trial emulation has drastically improved the quality of studies using observational data in recent years. It can prevent a lot of biases that are "self-inflicted" such as immortal time bias [@hernan2016specifying]. Although the concept may seem simple at first (or so I thought), in practice there are some issues that can arise. This post focuses on my personal experience with emulating target trials and what I've learned.

# Lessons Learned

I've been involved with a few projects where the goal is to emulate a target trial. Part of my PhD project, currently underway, is to emulate a target trial using electronic health records. There are some key aspects/lessons that I've learned while working on projects emulating a target trial. To keep it somewhat organized, I'll group them (in no particular order):

-   Inclusion/Exclusion Criteria

-   Data Sources

-   Data Quality

-   Missing Data

-   Choosing an Index Date

-   Causal Estimands

# Inclusion/Exclusion Criteria

Inclusion/exclusion criteria, or sometimes referred to as eligibility criteria, are need to ensure that we are including patients needed to answer our question. I/E criteria are used in RCTs, but there is more flexibility than when using real-world data. For example, lab test 1 to be measured within 30 days of starting the patient on treatment. Real-world data might not have a lab test measured within 30 days.

With this in mind there are a few important things I've learned to consider about the I/E criteria when emulating a target trial with RWD.

## Applying Every I/E Criteria is Unlikely

The goal is to apply all I/E criteria that is specified in the target trial protocol. Unfortunately, this is unlikely to be the case. What can further complicate it, is that different data sources may have different variables which will allow for differing I/E criteria to be applied.

An approach that I found useful was the one outlined in @gatto2022structured. This helps rank which criteria are most important so be applied to allow for a better decision.

## How Can We Alter Criteria?

Certain criteria we can apply but they may need to be altered slightly. This often requires input from an expert (i.e., a clinician). For example, the ideal criteria might be to take a lab value the day before randomization but this isn't available in the RWD. So what's a reasonable alternative? 14 days? A week? A month?

The goal is to operationalize the criteria to be as similar as the ideal criteria while not sacrificing the validity of the study.

# Index Dates

Choosing an index date can be fairly straightforward. Based on emulating a trial, if the goal is to estimate the per-protocol effects, then we can select the index date as 1 day after receiving treatment or whatever will align with our target trial....but what if there are multiple options?

## Too many options!

Certain studies can result in there being multiple possible index dates. Prior to coming across this issue, I had naively assumed there'd only be one index date. This becomes problematic. Do we pick the first index date? The last? Randomly close our eyes and pick one while hoping for the best?\
\
Like much of science the answer is "it depends". Part of the reason is that it depends on the outcome. If it's a time-to-event outcome, choosing the first or last index date could impact the results. My approach now when I come across this problem is to simulate the scenario, similar to @hatswell2022approaches.

# Missing Data

If you're working with real-world data, there will 100% be missing data.

## Need a Plan

Working with real-world data, you will need a plan for missing data. Regardless of how high quality the data is you will need a plan. Personally, I tend to seperate these into two main categories then three within that. The two main categories are: outcome and covariates. The reason

Part of what can make this challenging (not just for emulating a target trial but missing data in general, including trials) is that you don't see the data a priori. So this is mostly educated guesses based on similiar data or similar experience.\

## Missing Data Mechanism for Each Variable

The missing data mechanism can be different for each variable. This was something that I hadn't thought about before working on a project where the team specified a different missing data mechanism (MCAR, MAR, MNAR) for each variable. To me, this became a fantastic approach because a "one-size approach fits all" mentality is dangerous to apply to all variables.

## Covariates vs Outcomes

Missing data, in my opinion, should be stratified by covariates and outcomes. This can further be broken down by missing data mechanism but these two groups are a good place to start. Why? Regulatory bodies can have certain guidances to follow for each of these. For example, imputing best/worst

# Causal Estimands

Estimands are often used in clinical trials, ICH E9 (R1)

-   ATE, ATT, ATU, ATO

-   Important for conclusions/refining research question

-   Almost important/guides statistical method

-   Even more important for observational studies (RCTs do ATE by default)

# Data Quality

-   Quality over quantity

-   "Quality" should include ability to answer research question/emulate the target trial

# Data Sources

For a trial, we typically just have one data source however when using real-world data to derive causal inferences we have several options. A key

-   EHR, Claims, Registry, Wearables

-   Different data for different purpose

-   Making sure it can answer the question of interest

## 
