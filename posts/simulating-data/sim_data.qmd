---
title: "Mastering Statistics Through Make-Believe" 
subtitle: "Simulated Data in R"
author: "Ryan Batten"
date: "2023-09-24"
categories: [Simulating Data]
bibliography: sim_data.bib
# image : "standards.png" - use graphic from LinkedIn post about simulation
draft: true
format: 
  html:
    toc: true
    toc-title: Contents
    toc-location: right
    toc-depth: 4
    code-fold: true
---

## Simulating Data, Why?

Simulating data wasn't something that I was taught in school. I've learned it since graduating/during my PhD, in my pursuit of improving my stats knowledge (the more I learn the more I feel I don't know, weird feeling). It's been unbelievably useful.

I wanted to write this post to help anyone else who isn't familiar with simulating data. Before, I just want to give a few use cases where I've found simulating data helpful:

-   Showing bias (confounder, collider, etc)

-   Understanding how methods work

-   Comparing different methods (1:1 matching vs IPTW, etc)

-   Understand data generating mechanisms

::: callout-note
## Stats for non-stats

I'm a tradesman biostatistician \[NOTE: RYAN CHECK THIS TERM FROM HIS TWEET\] (credit to PhDemetri for this term). Basically, I've done a few statistics courses but don't have a degree in it (I do in clinical epidemiology which overlaps substantially with biostatistics).

If you do, hopefully this is a good refresher. If you don't, don't worry! I don't either so hopefully this helps out a little as an intro.
:::

## How to Simulate Data in R

Before we dive into some R code, which I do love, it's useful to first understand at a high level how this works. Before you scramble to close this, wait! I promise it isn't going to be technical or boring. Alright, let's get started.

### Continuous Outcomes

To simulate data in R, we can use a family of functions that start with r. For example, if we want to simulate data from a normal distribution we can use the *rnorm* function. Let's simulate an outcome and exposure that we will assess with a generalized linear model.

Let's use hours that a four month old slept as the outcome and number of pacifiers in their bed.

```{r continuous_outcome, message = FALSE}

library(tidyverse)

set.seed(123) # we need to set a seed prior to simulating data. This allows us to replicate the data. For more details check out this blog post: [insert blog post about using different seeds for simulations]

n.babies = 128 # note to get this value I used runif(1, min = 100, max = 500) then picked closest number divisible by 2

num_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)
age_months = rnorm(n.babies, mean = 9, sd = 2)
hours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months 

df = data.frame(
  num_pacifiers, 
  age_months,
  hours_slept
)
```

Okay, so we have our data. Now let's do something with it! Let's try fitting a GLM

```{r model_fit, message = FALSE}
mod = glm(hours_slept ~ num_pacifiers + age_months, 
          family = gaussian(), # we know this because we simulated the data. In reality, you have to use a combination of visualizing the data, understanding the data generating mechanism (aka what distribution it came from and the best model fit (i.e., Poisson vs negative binomial))
          data = df 
          )

summary(mod)

```

Now, as you can see our estimates were pretty accurate. Let's try this again, but this time we'll assume that pacifiers doesn't matter, it's only the age. We can still adjust for both. Let's try two models: one where we adjust for both variables and one where we adjust for only age.

```{r two_models}

num_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)
age_months = rnorm(n.babies, mean = 9, sd = 2)
hours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months 

df = data.frame(
  num_pacifiers, 
  age_months,
  hours_slept
)

# Could alternatively try adjusting for the wrong variable (i.e., pacifiers but not age)

mod1 <- glm(hours_slept ~ age_months, 
            family = gaussian(link = "identity"), 
            data = df)

mod2 <- glm(hours_slept ~ age_months + num_pacifiers, 
            family = gaussian(link = "identity"), 
            data = df)

summary(mod1)
summary(mod2)
```

Now you see how the impact of adjusting for a variable that doesn't affect the outcome, at least in this case. For our model where we adjust for only age, the result is `r coef(mod1[2])`. When we adjust for the "correct" variables , we end up with a coefficient of `r coef(mod2)[2]`.

This is how you simulate a continuous outcome, but what about a binary outcome?

::: callout-note
## Continuous Distributions distributions

For the above example, we used a normal distribution. However, this doesn't need to be the case. If we are dealing with age we may want to use a uniform distribution (using runif) and specifying the minimum and maximum. For example, if we are thinking of a variable where it may not make sense to have a value below a certain value (i.e., age where people are between 18 and 65).

For simplicity here, and because a number of statistical methods assume normality (*cough* also the central limit theorem *cough*), we will use *rnorm*.
:::

### Binary Outcomes

Simulating a binary outcome is similar to simulating a continuous outcome except we need to put these variables in the probability argument. We can convert linear predictors to probabilities for the logistic distribution (since we'll be fitting a logistic regression) using the *plogis* function.

```{r binary_outcome}

n.parents = n.babies*1.5

coffee_consumption = rnorm(n = n.parents, mean = 3, sd = 0.5)
hours_baby_slept = rnorm(n = n.parents, mean = 6, sd = 2)
cold_room = rbinom(n = n.parents, size = 1, prob = 0.5)

prob_tired = plogis(0.5*hours_baby_slept + cold_room*1.5 + coffee_consumption*0.25)
  
tired_parents = rbinom(n = n.parents, size = 1, prob = prob_tired)

df = data.frame(
  cold_room,
  coffee_consumption,
  hours_baby_slept,
  tired_parents
)
 
```

Now we can fit a logistic regression model

```{r logistic_regression}

mod = glm(tired_parents ~ coffee_consumption + hours_baby_slept + cold_room,
          family = binomial(link = "logit"), 
          data = )
```

## Simulating Causal Concepts

## Causal Concepts

Now, this is great and all but this blog is about causal inference! So let's incorporate some into this post shall we? Instead of looking at GLMs, let's demonstrate how confounding and colliders can introduce bias. We'll calculate bias as [@morris2019]:

$$
E[\hat{\theta}] - \theta
$$

where $E[\hat{\theta]}$ is the expected, or average, estimated value and $\theta$ is the "actual" value. I'll explain why we use the average later on in the Over and Over and Over Again section \[TRY TO ADD CROSS REFERENCE\]

## Showing Confounding

Let's assume that we want to test how not adjusting for a confounder can lead to confounding bias. For this situation let's look at the number of pacifiers in a babies crib and the hours slept. We can draw a DAG, including our confounder which is weather.

\# Set seed for reproducibility

set.seed(111)

\# Number of observations

n \<- 100

\# Generate confounder - Baby's Age in months

baby_age \<- rnorm(n, mean = 6, sd = 2)

\# Generate number of pacifiers influenced by Baby's Age

num_pacifiers \<- 1 + 0.3 \* baby_age + rnorm(n, mean = 0, sd = 0.2)

\# Generate hours slept influenced by Baby's Age

hours_slept \<- 10 + 0.5 \* baby_age + rnorm(n, mean = 0, sd = 1)

\# Combine into a data frame

data \<- data.frame(baby_age, num_pacifiers, hours_slept)

For this, our question is "does hot coffee cause rainbows?". Rainy days lead to drinking more coffee, but rainy days also lead to rainbows.\
\

```{r}
library(tidyverse)
library(ggdag)

theme_set(theme_dag())

dag = ggdag::dagify(
  hot_coffee ~ rain, 
  rainbow ~ rain + hot_coffee, 
  exposure = "hot_coffee",
  outcome = "rainbow",
  labels = c(
    hot_coffee = "Hot\nCoffee",
    rainbow = "Rainbow", 
    rain = "Rain"
  )
) 

dag %>% 
  ggdag::ggdag(layout = "tree", 
               text = FALSE,
               use_labels = "label")

```

g at how long goldfish grow as an example. There's a few factors that affect the size (in this example length and size will be used interchangeably): feeding_frequency (number of times per day), water temperate, and how much food.

\# Simulate data for 100 goldfish

set.seed(123)

n \<- 100

\# Nutritional factors

food_quality \<- rnorm(n, mean = 5, sd = 1)

feeding_frequency \<- rnorm(n, mean = 3, sd = 0.5)

\# Environmental factors

water_quality \<- rnorm(n, mean = 7, sd = 1)

water_temp \<- rnorm(n, mean = 20, sd = 2)

\# Growth rate as the dependent variable

growth_rate \<- 0.5 + 0.2\*food_quality + 0.1\*feeding_frequency + 0.3\*water_quality + 0.4\*water_temp + rnorm(n, mean = 0, sd = 0.5)

\# Combine into a data frame

goldfish_data \<- data.frame(food_quality, feeding_frequency, water_quality, water_temp, growth_rate)

Let's use how long goldfish grow as an example. For ease of this example, let's

### 

This type of simulation is called a Monte Carlo Simulation. The reason it's so useful is because

of coffee beans. These range in shape and size. Suppose we know that the average weight is 135 milligrams and 16 (10 to 22/64 of an inch). Now, does this mean that every bean will be that? Let's pull one out and see

## Simulating Data

We'll start with continuous data. To simulate this we need two pieces of information, also known as parameters: mean and standard deviation.

::: callout-note
## TTE Outcome

This post will focus on continuous and binary outcomes. A future blog post will go over how to simulate time-to-event, sometimes called survival, data.
:::

NOTE TO SELF: explain that regression is finding the mean (keith mcnulty's post)

Now, if we pick one

# Old Text

Learning to simulate data was one of the best things that I've stumbled into. It can help improve your knowledge of statistics and inform decision making. One of the things I found difficult when starting was for a resource that gave some examples of what I really wanted to know. Hopefully this post will provide a decent start to someone.

## Why Simulate?

At first, I had heard about simulation studies but wasn't exactly sure the purpose of them.

## Steps to Simulate

\@morris2019 use the acronym ADEMP: Aims, Data-generating mechanisms, Methods, Estimands and Performance measures. This is a great acronym which we'l'' break down a little bit .

## Over and Over and Over Again

Now we've simulated data. Great! But how can we trust this? We only did it once. What if the sample of the same size (N = X \[RYAN: INSERT NUMBER\]) gave a different answer? To account for this, we need to repeat this multiple times. So, let's do that!

::: callout-note
## Selecting n for Monte Carlo simulation

To select the n required, there are several resources. I won't go into detail here, but some personal references I use are \[INSERT Morris et al. and others\]
:::

Monte Carlo simulation is

## Resources for Simulating

There are a ton of good resources when it comes to design aspects of simulation study. Two that help me think about simulation studies are \@morris2019 and \@burton2006.
