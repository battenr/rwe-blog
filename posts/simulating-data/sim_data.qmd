---
title: "Mastering Statistics Through Make-Believe" 
subtitle: "Simulated Data in R"
author: "Ryan Batten"
date: "2023-09-30"
categories: [Simulating Data]
bibliography: sim_data.bib
image : sim_data.jpg
format: 
  html:
    toc: true
    toc-title: Contents
    toc-location: right
    toc-depth: 4
    code-fold: true
---

## Why Simulate Data?

Simulating data wasn't something that I was taught in school. I've learned it since graduating/during my PhD, in my pursuit of improving my stats knowledge (the more I learn the more I feel I don't know, weird feeling). It's been unbelievably useful.

I wanted to write this post to help anyone else who isn't familiar with simulating data. Before, I just want to give a few use cases where I've found simulating data helpful:

-   Showing bias (confounder, collider, etc)

-   Understanding how methods work

-   Comparing different methods (1:1 matching vs IPTW, etc)

-   Understand data generating mechanisms

Enough about how it's helpful, how do we do it!

::: callout-note
## R Code

I primarily use R for my coding, so this post focuses on using R. However, the methodology apply to whatever program you use for analysis. If you prefer to use MS Excel, you can do this using Excel as well, although I'd suggest a different software.

I'm more of a journeyman tradesman biostatistician. What I mean by that is that I've done a few statistics courses but don't have a degree in it (my MSc and PhD are in clinical epidemiology).\
\
If you do have a background in statistics, hopefully this is a good refresher. If you don't, don't worry! I don't either, so hopefulyl
:::

## How to Simulate Data in R

Before we dive into some R code, which I do love, it's useful to first understand at a high level how this works. Before you scramble to close this, wait! I promise it isn't going to be technical or boring....or at least I'll try my best to not make it boring.\
\
For simplicity, we'll categorize outcomes as continuous, binary or time-to-event (TTE). This post won't tackle TTE, but a future blog post will! Alright, let's get started.

### Continuous Outcomes

To simulate data in R, we can use a family of functions that start with r. For example, if we want to simulate data from a normal distribution we can use the *rnorm* function. Let's simulate an outcome and exposure that we will assess with a generalized linear model.

Let's use hours that a four month old slept as the outcome and number of pacifiers in their bed.

```{r continuous_outcome, message = FALSE}

library(tidyverse)

set.seed(123) # we need to set a seed prior to simulating data. This allows us to replicate the data. For more details check out this blog post: [insert blog post about using different seeds for simulations]

n.babies = 128 # note to get this value I used runif(1, min = 100, max = 500) then picked closest number divisible by 2

num_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)
age_months = rnorm(n.babies, mean = 9, sd = 2)
hours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months 

df = data.frame(
  num_pacifiers, 
  age_months,
  hours_slept
)
```

Okay, so we have our data. Now let's do something with it! Let's try fitting a GLM

```{r model_fit, message = FALSE}
mod = glm(hours_slept ~ num_pacifiers + age_months, 
          family = gaussian(), # we know this because we simulated the data. In reality, you have to use a combination of visualizing the data, understanding the data generating mechanism (aka what distribution it came from and the best model fit (i.e., Poisson vs negative binomial))
          data = df 
          )

summary(mod)

```

Now, as you can see our estimates were pretty accurate. Let's try this again, but this time we'll assume that pacifiers doesn't matter, it's only the age. We can still adjust for both. Let's try two models: one where we adjust for both variables and one where we adjust for only age.

```{r two_models}

num_pacifiers = rnorm(n = n.babies, mean = 8, sd = 3)
age_months = rnorm(n.babies, mean = 9, sd = 2)
hours_slept = 4 + 0.5*num_pacifiers + 0.25*age_months 

df = data.frame(
  num_pacifiers, 
  age_months,
  hours_slept
)

# Could alternatively try adjusting for the wrong variable (i.e., pacifiers but not age)

mod1 <- glm(hours_slept ~ age_months, 
            family = gaussian(link = "identity"), 
            data = df)

mod2 <- glm(hours_slept ~ age_months + num_pacifiers, 
            family = gaussian(link = "identity"), 
            data = df)

summary(mod1)
summary(mod2)
```

Now you see how the impact of adjusting for a variable that doesn't affect the outcome, at least in this case. For our model where we adjust for only age, the result is `r coef(mod1[2])`. When we adjust for the "correct" variables , we end up with a coefficient of `r coef(mod2)[2]`.

This is how you simulate a continuous outcome, but what about a binary outcome?

::: callout-note
## Continuous Distributions distributions

For the above example, we used a normal distribution. However, this doesn't need to be the case. If we are dealing with age we may want to use a uniform distribution (using runif) and specifying the minimum and maximum. For example, if we are thinking of a variable where it may not make sense to have a value below a certain value (i.e., age where people are between 18 and 65).

For simplicity here, and because a number of statistical methods assume normality (*cough* also the central limit theorem *cough*), we will use *rnorm*.
:::

### Binary Outcomes

Simulating a binary outcome is similar to simulating a continuous outcome except we need to put these variables in the probability argument. We can convert linear predictors to probabilities for the logistic distribution (since we'll be fitting a logistic regression) using the *plogis* function.

```{r binary_outcome}

# Double check this example (using ChatGPT and another way/blog post/expert)

n.parents = n.babies*1.5

coffee_consumption = rnorm(n = n.parents, mean = 3, sd = 0.5)
hours_baby_slept = rnorm(n = n.parents, mean = 4, sd = 0.25)
cold_room = rbinom(n = n.parents, size = 1, prob = 0.5)

linpred = 0.25*hours_baby_slept 

prob_tired = plogis(linpred)

# linpred = 3 + 0.25*hours_baby_slept + 0.005*cold_room + 0.05*coffee_consumption
# 
# prob_tired = plogis(linpred)
  
tired_parents = rbinom(n = n.parents, size = 1, prob = prob_tired)

df = data.frame(
  cold_room,
  coffee_consumption,
  hours_baby_slept,
  tired_parents
)
 
```

Now we can fit a logistic regression model!

```{r logistic_regression}

mod = glm(tired_parents ~ hours_baby_slept,
          # tired_parents ~ coffee_consumption + hours_baby_slept + cold_room,
          family = binomial(link = "logit"), 
          data = df)

summary(mod)
```

Based on this, we can see that our model doesn't give a great example but if we increase the sample size, what happens?

```{r binary_outcome_large_sample}

# Double check this example (using ChatGPT and another way/blog post/expert)

n.parents.large = n.parents*10

coffee_consumption = rnorm(n = n.parents.large, mean = 3, sd = 0.5)
hours_baby_slept = rnorm(n = n.parents.large, mean = 4, sd = 0.25)
cold_room = rbinom(n = n.parents.large, size = 1, prob = 0.5)

linpred = 0.25*hours_baby_slept 

prob_tired = plogis(linpred)

# linpred = 3 + 0.25*hours_baby_slept + 0.005*cold_room + 0.05*coffee_consumption
# 
# prob_tired = plogis(linpred)
  
tired_parents = rbinom(n = n.parents.large, size = 1, prob = prob_tired)

df = data.frame(
  cold_room,
  coffee_consumption,
  hours_baby_slept,
  tired_parents
)


mod = glm(tired_parents ~ hours_baby_slept,
          # tired_parents ~ coffee_consumption + hours_baby_slept + cold_room,
          family = binomial(link = "logit"), 
          data = df)

summary(mod)
```

So if we include 10 times the people, our result gets closer! If we include 100 times the people, it gets even closer.

::: callout-note
## TTE Outcome

At this point, you may be expecting a section on simulating TTE outcome. This will be the topics of a future blog post, since it's not quite as straight forward as continuous and binary outcomes.
:::

You might be thinking "this is great and all, but how does this apply to causal inference?"

## Simulating Causal Concepts

## Causal Concepts

Now, this is great and all but this blog is about causal inference! So let's incorporate some into this post shall we? Instead of looking at GLMs, let's demonstrate how confounding and colliders can introduce bias. We'll calculate bias as [@morris2019]:

$$
E[\hat{\theta}] - \theta
$$

where $E[\hat{\theta]}$ is the expected, or average, estimated value and $\theta$ is the "actual" value. I'll explain why we use the average later on in the Over and Over and Over Again section \[TRY TO ADD CROSS REFERENCE\].

::: callout-important
## Causal Estimand

We need to make sure we are looking at the same causal estimand when comparing methods. For example, if we want to compare PSM to IPTW that would result in different answer...because they target different estimands!
:::

## Showing Confounding

Let's assume that we want to demonstrate how confounding works. I'm an avid coffee lover, so let's use an example with coffee! Suppose that our research question is "Does consuming coffee cause you to be happy?" We can start by drawing a DAG with our three variables: happy, coffee and sleep.

```{r dags}
library(ggdag)
theme_set(theme_dag())

coffee_dag <- ggdag::dagify(
  happy ~ coffee + sleep,
  coffee ~ sleep,
  exposure = "coffee",
  outcome = "happy",
  labels = c(
    coffee = "Coffee",
    happy = "Happiness",
    sleep = "Sleep"
  )
)

ggdag::ggdag(coffee_dag, text = FALSE, use_labels = "label")
```

Now we have our DAG, let's get to simulating some data!

```{r}

# Look into Matthew Fox article for simulating data 

# Simulating Data, from ChatGPT

set.seed(123) # Setting a seed for reproducibility

# Number of observations
n <- 1000

# Simulating sleep hours (normal distribution with mean=7 and sd=1.5)
sleep_hours <- rnorm(n, mean = 7, sd = 1.5)

# Simulating coffee consumption based on sleep (negative correlation: less sleep -> more coffee)
coffee_consumption <- 5 - 0.5 * sleep_hours + rnorm(n, mean = 0, sd = 1)

# Simulating happiness based on both sleep (positive correlation: more sleep -> more happiness)
# and coffee consumption (positive correlation: more coffee -> more happiness)
happiness <- rbinom(n = n, size = 1, prob = plogis(0.3 * sleep_hours + 0.2 * coffee_consumption + rnorm(n, mean = 0, sd = 1)))

# Creating a data frame to hold the variables
df <- data.frame(sleep_hours, coffee_consumption, happiness)

# Displaying the first few rows of the data
head(df)
```

Now we have our data, let's show some confounding. How? We'll fit two models: 1) not adjusting for the confounder, 2) adjusting for the confounder.

```{r confounder-models}

mod1 <- glm(formula = happiness ~ coffee_consumption,
            family = binomial(link = "logit"),
            data = df)

mod2 <- glm(formula = happiness ~ coffee_consumption + sleep_hours,
            family = binomial(link = "logit"),
            data = df)

bias1 = coef(mod1)[2] - 0.2
bias2 = coef(mod2)[2] - 0.2

```

We can now calculate bias for each of these models. As we can see, the bias from model one (`r bias1`) is more than the bias from model two (`r bias2`). But how can we trust this? We only did it once. What if a sample of the same size (N = 250) gave a different answer? To account for this, we need to repeat this multiple times. So, let's do that! Let's repeat it a thousand times

```{r}

set.seed(123) # Setting a seed for reproducibility

simulation <- function() {
  # Number of observations
  n <- 250
  
  # Simulating sleep hours (normal distribution with mean=7 and sd=1.5)
  sleep_hours <- rnorm(n, mean = 7, sd = 1.5)
  
  # Simulating coffee consumption based on sleep (negative correlation: less sleep -> more coffee)
  coffee_consumption <- 5 - 0.5 * sleep_hours + rnorm(n, mean = 0, sd = 1)
  
  # Simulating happiness based on both sleep (positive correlation: more sleep -> more happiness)
  # and coffee consumption (positive correlation: more coffee -> more happiness)
  happiness <- rbinom(n = n, size = 1, prob = plogis(0.3 * sleep_hours + 0.2 * coffee_consumption + rnorm(n, mean = 0, sd = 1)))
  
  # Creating a data frame to hold the variables
  df <- data.frame(sleep_hours, coffee_consumption, happiness)
  
  # Building the models
  mod1 <- glm(formula = happiness ~ coffee_consumption,
              family = binomial(link = "logit"),
              data = df)
  
  mod2 <- glm(formula = happiness ~ coffee_consumption + sleep_hours,
              family = binomial(link = "logit"),
              data = df)
  
  # Calculating the biases
  coef1 <- coef(mod1)[2] 
  coef2 <- coef(mod2)[2] 
  
  coef_results <- data.frame(
    coef1, coef2
  )
  
  # Returning the biases as a named vector
  return(
    coef_results
  )
}

# Replicating the simulation 1000 times
output_list <- replicate(n = 1000, expr = simulation(), simplify = FALSE) 

# Bind all data frames in the list into a single data frame
df.out <- do.call(rbind, output_list)

bias1 = mean(df.out$coef1) - 0.2
bias2 = mean(df.out$coef2) - 0.2
```

Now we can look at

::: callout-note
## Picking a number of simulations

For our example we picked 1000 repetitions but where did this number come from? Truthfully, it was completely arbitrary. In practice, we need to carefully choose how many we need. I highly suggest @morris2019 as a reference for more information if you need to pick the number of simulations.
:::

# Other Use Cases

We can use this to show other concepts as well, test new ideas or learn methods ourselves. Recently, I used it to demonstrate to myself how collider bias is. Another example, that I'm currently exploring, is how different the bias is from confounding versus a collider bias.

or test them ourselves.

For example: collider bias.

## 
