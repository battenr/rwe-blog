---
title: "Missing Data" # November Post 
subtitle: "The Where's Waldo of Causal Inference" # Titles are placeholders
author: "Ryan Batten"
date: "2023-11-26"
categories: [Missing Data, MCAR, MAR, MNAR]
bibliography: missing-data.bib
draft: true
format: 
  html:
    code-fold: true
---

# Missing Data

Missing data are unavoidable when dealing data in general, but even more so with real-world data. So it's important to have a plan on what to do! To make valid causal inferences, we need to have a solution but what options do we have? A BUNCH! This post will focus on introducing five common methods:

-   Complete Case

-   Last Observation Carried Forward

-   Mean Value Imputation

-   Conditional Mean Imputation (aka using regression)

-   Multiple Imputation

Before we dive into these exciting methods, first we need to go over the different types of missing data mechanisms. Arguably the most important aspect of dealing with missing data is figuring this part out.

# Missing Data Mechanisms

"How did this data end up becoming missing?" is essentially what we are asking. Was this just random? Or is there a reason? Data can go missing for a variety of reasons. A patient refusing to respond to a certain question, being lost to follow-up, investigator error or certain tests not being order to name just a few.

Typically missing data are organized into three types of missing data mechanisms: missing completely at random (MCAR), missing at random (MAR) and missing not a random (MNAR) [@austin2021missing] . Let's break down each one a little further.

## Missing Completely at Random (MCAR)

Data are "missing completely at random" if the probability of a variable being missing is independent from both observed and unobserved variables for that person [@austin2021missing]. The key word here is "completely", implying that it has nothing to do with the other variables that we are interested in.

Let's use an example. Imagine that we are taking a survey of a class of kindergarten children on what type of ice cream they ate. One day a mischievous puppy runs into the classroom and knocks over the box, and eats the surveys that fall out. The surveys that are missing are MCAR because it the dog didn't choose which ones to eat! It was just an accident.

## Missing at Random (MAR)

Data are "missing at random" if the probability of a variable being missing, after accounting for **all** the observed variables is independent from the unobserved data [@austin2021missing]. So basically it can depend on the observed variables but has nothing to do with the unobserved variables.

Using our same example, imagine that kids who prefer mint or vanilla, decide to skip school and go to the beach.

## Missing Not at Random (MNAR)

Data are considered to be "missing not at random" if, they are neither MCAR or MAR. But that's not super helpful, let's try again. Data are MNAR if the probability of being missing, even after accounting for all the observevd variables, is based on the value of the missing variable [@austin2021missing].

An example of MNAR wou

::: callout-note
## MAR vs MNAR

Unfortunately, there is no way to test for MAR vs MNAR, so this needs to be based on expert knowledge [@austin2021missing].
:::

## Pirate Treasure?

Imagine that we have a collection of pirate treasure maps. The missing data mechanisms could be thought of like this:\
\
**MCAR**: Some maps are missing because they were randomly lost during a storm at sea

**MAR**: Maps are missing for treasures buried in a haunted island because superstitious pirates (whose superstitions are recorded) avoid those places.\
\
**MNAR**: Maps are missing for the most valuabel treasures because the pirates who buried them never shared the locations, fearing theft, and their level of paranoia isn't recorded. (So the most lucrative treasures are missing)

# So...what do we do?

Once we know, or think we know, what type of missing data mechanism we have we need to figure out how to deal with this! This post will go over five methods, although there a LOT more than that. These are just some common ones that I see/have come across in my field. Each section will cover: overview of what the method is, assumptions (what's required of the method), when to use it and then an example in R.

::: callout-warning
## Covariates vs Outcomes

This post focuses on imputation for covariates, rather than outcomes. The methods are the same, but the plausibility may be different for an outcome variable. Furthermore, it depends on the audience. For example, certain regulatory bodies may prefer several scenarios such as best-case and worst-case imputation.
:::

::: callout-important
## An Introduction to Methods for Missingness

This is meant to be an introduction to some of these methods. A few of them shouldn't have much additional detail to learn about that, such as last observation carried forward or complete cases. However others, such as multiple imputation, there is a litany of information about them. For example, there are entire textbooks on multiple imputation.

The goal of this post is more to be an introduction to missingness, and help identify some potential scenarios to use the different methods while avoiding pitfalls.
:::

# Pirate Treasure Example

Let's simulate some data for our pirate treasure example. We'll assume that

```{r pirates_example, message = FALSE}
library(tidyverse)

# Simulate a dataset
set.seed(123) # for reproducibility

n = 322

# Pirate Data

df <- data.frame(
  pirate_id = 1:n, 
  treasure_value = runif(n = n, min = 1000, max = 5000), 
  haunted_island = rbinom(n = n, size = 1, prob = 0.5),
  pirate_superstition = rbinom(n = n, size = 1, prob = 0.5)
) 

# MCAR

# Randomly select 50 observations

mcar_indices <- sample(1:nrow(df), 50) # randomly select 20 indices
df$treasure_value_mcar <- df$treasure_value
df$treasure_value_mcar[mcar_indices] <- NA # set these values to NA



# MAR (depends on the value of another variable)
df$treasure_value_mar <- df$treasure_value
is_mar <- df$haunted_island & df$pirate_superstition > 0.5
df$treasure_value_mar[is_mar] <- NA

# MNAR Variable 
df$treasure_value_mnar <- df$treasure_value
mnar_prob <- df$treasure_value / max(df$treasure_value) # higher value, higher chance of being NA
df$treasure_value_mnar[runif(nrow(df)) < mnar_prob] <- NA

# View the data
head(df)

# Writing a Function for Selecting Appropriate Data 

df_missing <- function(type){
  if(type == "mcar"){
    df |> 
      dplyr::select(
        pirate_id, 
        treasure_value, 
        haunted_island, 
        pirate_superstition,
        treasure_value_mcar
      )
  } else if (type == "mar"){
        df |> 
      dplyr::select(
        pirate_id, 
        treasure_value, 
        haunted_island, 
        pirate_superstition,
        treasure_value_mar
      )
  } else if (type == "mnar"){
        df |> 
      dplyr::select(
        pirate_id, 
        treasure_value, 
        haunted_island, 
        pirate_superstition,
        treasure_value_mar
      )
  } else{
    "Error please select one of the types of missing data mechnanism (MCAR, MAR, MNAR)"
  }
}

```

# Complete Cases (Exclude All Missing)

## What is it?

"Let's just get rid of the missing data! Then our problem will be solved!"...not quite. While getting rid of the missing data certainly is *a* way to deal with missing values, you are also losing good information. However, like any method, there is a time and place for it.

## When to Use

Complete case analysis can be valid when only the outcome variable is incomplete and we assume MAR [@austin2021missing], however when we are dealing with covariates there are several disadvantages to this approach.

Unless the data are MAR, the estimated stats and regression coefficients may be biased [@austin2021missing]. Even if the data are MCAR, by reducing the sample size, we are reducing the precision (i.e., confidence intervals will be wider). Another problem is that by using

## Example

```{r complete_cases}

# Note to self: maybe this should be MCAR? 

df.mar <- df_missing(type = "mar") 

cc <- df.mar |> 
  drop_na()

mean(df.mar$treasure_value)
mean(cc$treasure_value_mar)
```

We can see in this example, that removing all the missing values results in a difference of values. If you compare the datasets then you'll see the differences and the flaws in this. Futhermore, this can drastically reduce your dataset if you only keep the observations that has a value for every value (as a function of the number of variables in your dataset).

## Considerations

Complete case analysis can be convenient from a data quality point of view, however it can cause some issues. For example, is the missing data are MCAR the analysis will have reduced precision due to the reduced sample size but the observed data will not be biased [@jakobsen2017]. If the missing data are not MCAR, the estimate of the effect may be biased [@jakobsen2017].

It's also important to consider which patients are being removed and how many. Is this now the same target population that it was originally? Imagine if ended up removing 25% of our sample! How would that affect our generalizability?\
\
Complete cases can be used in the right context (i.e., if \<5% of sample is missing and data is MCAR \[RYAN ADD REF\]) however it requires some careful thought.

# Last Observation Carried Forward

## What is it?

Last observation carried forward (LOCF) is very much what it sounds like. We take the last value that was observed and use it.

## When to Use

LOCF assumes that the data is MCAR (REF?). If the data are MCAR, the absence of data is unrelated to the study question or the values of the missing data themselves. It can be quite useful when it is plausible, for example when we need to carry through the value of sex (male/female).

## Example

```{r locf}

# Here we need to have multiple measurements per pirate

# FROM CHATGPT

n_pirates <- 10   # Number of unique pirates
n_measurements <- 5  # Number of measurements per pirate

# Create the data frame
df.locf <- expand.grid(
  pirate_id = 1:n_pirates,
  measurement_id = 1:n_measurements
) %>%
  mutate(
    treasure_value = runif(n = n_pirates * n_measurements, min = 1000, max = 5000),
    haunted_island = rbinom(n = n_pirates * n_measurements, size = 1, prob = 0.5),
    pirate_superstition = rbinom(n = n_pirates * n_measurements, size = 1, prob = 0.5)
  )

# View the dataframe
head(df.locf)


# # MY CODE
# 
# df.mcar <- df_missing(type = "mcar")
# 
# # RYAN: NEED to ADD MORE ROWS, multiple per pirate_id
# 
# df.locf <- df.mcar %>% 
#   group_by(pirate_id) %>% 
#   dplyr::mutate(
#     treasure_value = ifelse(is.na(treasure_value_mcar), lag(treasure_value_mcar), treasure_value_mcar)
#   )
```

## Considerations

The plausibility of LOCF needs to be considered. For example, if we are studying pirates and have two categories: monsters vs humans, we can safely assume that the monster will still be a monster. An example where this might be a problem is a lab value. Is a lab value that is 30 days prior okay? What about 60, 90 or 400?

Other assumptions include:

-   No dropout

-   Stability of the condition over time

# Mean Value Imputation

## What is it?

Mean value imputation takes the mean of the values that we *do* have and uses this wherever there are missing values. As a side note, the mean is sometimes called the expected value in statistics.

## When to Use

You probably guessed this already, but we can use it when we have a mean! So this can be used for any variable that has a mean (typically a continuous variable).

## Example

```{r mean_value}

df.mcar <- df_missing(type = "mcar")

mean(df.mcar$treasure_value_mcar, na.rm = TRUE)

df.mean.imputation <- df.mcar %>% 
  dplyr::mutate(
    new_treasure_value = dplyr::case_when(
      is.na(treasure_value_mcar) ~ mean(df.mcar$treasure_value_mcar, na.rm = TRUE),
      !is.na(treasure_value_mcar) ~ treasure_value_mcar
    )
  )
```

## Considerations

This method is easy to implement and quick but there are some things we need to consider.

Mean value imputation is just what it sounds like. You replace the missing values with the mean. Now, this fairly straightforward but there are some assumptions that we need to consider. This method will give unbiased effects when the data is MCAR. It will lead to a decrease in the variability of the effect estimate though [@dziura2013, @austin2021missing]. It also ignores relationships with other variables, which may be the case [@austin2021missing]. It can be quite useful but needs to be used in the right circumstances.

# Condition mean Imputation (aka using Regression)

## What is it?

Conditional mean imputation is very similar to mean imputation but instead of using the mean, we'll use the conditional mean. How do we know the conditional mean? It's basically the result from our regression.\
\
Basically, you fit a regression model and then use this to predict what the value would be. A perk of this method is that you can fit a regression model including the variables that are associated with other variables.

## When to Use

We can use this in similar scenarios to the mean value imputation.

## Example

```{r conditional_mean}

df.mar <- df_missing(type = "mar")

mar.regress <- glm(
  treasure_value_mar ~ haunted_island + pirate_superstition, 
  family = gaussian(),
  data = df.mar
)

treasure_value_imputed = predict(mar.regress)

regress.imputed <- df.mar %>%
  mutate(
    treasure_value_imputed = case_when(
      is.na(treasure_value_mar) ~ predict(mar.regress, newdata = df.mar, type = "response"),
      TRUE ~ treasure_value_mar
    )
  )

mean(regress.imputed$treasure_value)
var.og = sd(regress.imputed$treasure_value)

mean(regress.imputed$treasure_value_imputed)
var.imputed = sd(regress.imputed$treasure_value_imputed)

mean(regress.imputed$treasure_value_mar, na.rm = TRUE)
sd(regress.imputed$treasure_value_mar, na.rm = TRUE)


```

As you can see, we are artificially reducing the variability in the variable. For example, `r var.og` is the original standard deviation, where as now the standard deviation is `r var.imputed`

## Considerations

"Conditional mean imputation" is similar to mean imputation however, a regression model is used to impute a single value for each missing value [@austin2021missing]. This will give an unbiased effect estimate when the missing data are MCAR or MAR [@dziura2013].

# Multiple Imputation

## What is it?

Multiple imputation (MI) imputes multiple values for each missing value. It picks a value from a list of options, the distribution, and selects a value to be imputed. This results in multiple complete data sets where the missing value has been filled in with plausible values [@austin2021missing]. Each of these data sets is used to conduct the analysis, for example fitting a generalized linear model, then the results are pooled. For our case, we'll focus on the method known as multivariate imputation by chained equations (MICE). For more detail about it, I highly recommend @austin2021missing.

## When to Use

## Example

```{r multiple_imputation}

library(mice) # package used for multiple imputation using chained equations

df.mcar <- df_missing(type = "mcar")

# Perform MICE imputation
mice_mod <- mice(df.mcar, m=5, maxit=50, meth='pmm', seed=500)

# Fit a GLM to each imputed dataset
glm_models <- with(mice_mod, glm(treasure_value_mcar ~ haunted_island + pirate_superstition, family = gaussian()))

# Pool the results of the GLM models
pooled_results <- pool(glm_models)



# Print the pooled results
broom::tidy(pooled_results) |> 
  select(term, estimate, std.error)



```

## Considerations

There are some considerations for using this. We need to decide if we think all the plausible values have been captured. MI only pulls from the existing values. So for example, if we have red, blue, yellow and green marbles in our population but only blue and green in our sample then red and yellow won't be imputed.

We also need to decide how large the number of imputed data sets are. There are many different ways to ascertain this but White et al. (ref from austin) suggested that as a rule of thumb it should be at least as large as the percentage of subjects with missing data. So if missingness is 22%, then we need at least 22 data sets.

::: callout-note
## MI is broad, this is an introduction

Multiple imputation is one of those methods that has an extensive amount of literature on it. This was meant to be an introduction but there are full textbooks that focus on the topic.
:::

# Summary of Methods

These are a few of the missing data methods that you may come across, however there is a VAST number of methods for missing data. The key takeaway should be to consider these, regardless of the method:

-   What kind of assumptions does it make? (i.e., MCAR, MNAR, MAR)

-   What are the tradeoffs? For example, increase in data quality but decreased in precision and generalizibility? Or increase in precision but decrease in replicability?

-   Practically speaking: is this the right use case for the audience? For example, a regulatory body may have different wants than an eighth grader wanting her help with analyzing her missing pets data.

This was just an introduction but there are a ton of great resources on each of these methods. There are also a bunch of other methods that are great to learn too (I'm currently still learning some of them so perhaps it will be a source of future posts!). This includes: K-Nearest Neighbour Matching, Predictive Mean Matching, Random Forest imputation and much more!

Hope this blog post was useful as an introduction to five methods to start, but this is only the beginning! Happy missing data exploring!
