---
title: "Missing Data" # November Post 
subtitle: "The Where's Waldo of Causal Inference" # Titles are placeholders
author: "Ryan Batten"
date: "2023-11-26"
categories: [Missing Data, MCAR, MAR, MNAR]
bibliography: missing-data.bib
draft: true
format: 
  html:
    code-fold: true
---

# Missing Data

Missing data are unavoidable when dealing with real-world data. Due to this, it's important to have a plan for dealing with missing data. There are tons of different methods that can be used for missing data but this post will focus on four different methods: multiple imputation, k nearest neighbour imputation, regression imputation and last observation carried forward. In order to make a valid causal inference we need to have an appropriate plan for what to do with this missing data.

Before we get started, we need to go over the different types of missing data mechanisms, arguably the most important aspect of dealing with missing data. Basically, what are the ways that data comes to be missing.

# Missing Data Mechanisms

"How did this data end up becoming missing?" is the best place to start. Was this just random? Or is there a reason? This can happen for a variety of reasons including the patient refusing to respond to a certain question, being lost to follow-up, investigator error or certain tests not being order. There are a myriad of reasons but these are commonly organized into three different types of data mechanisms: missing completely at random (MCAR), missing at random (MAR) and missing not a random (MNAR).

Each one of these requires making different assumptions, which in turn needs different methods to deal with. Let's break down each one a little further. For each of these.

## Missing Completely at Random (MCAR)

Data are considered to be "missing completely at random" if the probability of a variable being missing for a given subject, is independent from both observed and unobserved variables for that subject [@austin2021missing].

Missing completely at random is the just that. The key word is "completely". This means that it has nothing to do with the other variables that we are interested in. Let's use an example

## Missing at Random (MAR)

Data are considered to be "missing at random" if, after accounting for all the observed variables, the probability of a variable being missing is independent from the unobserved data [@austin2021missing].

## Missing Not at Random (MNAR)

Data are considered to be "missing not at random" if, they are neither MCAR or MAR. But that's not super helpful, let's try again. Data are MNAR if the probability of being missing, even after accounting for all the observevd variables, is based on the value of the missing variable [@austin2021missing].

An example of MNAR wou

::: callout-note
## MAR vs MNAR

Unfortunately, there is no way to test for MAR vs MNAR, so this needs to be based on expert knowledge [@austin2021missing].
:::

## Pirate Treasure?

Imagine that we have a collection of pirate treasure maps. The missing data mechanisms could be thought of like this:\
\
**MCAR**: Some maps are missing because they were randomly lost during a storm at sea

**MAR**: Maps are missing for treasures buried in a haunted island because superstitious pirates (whose superstitions are recorded) avoid those places.\
\
**MNAR**: Maps are missing for the most valuabel treasures because the pirates who buried them never shared the locations, fearing theft, and their level of paranoia isn't recorded. (So the most lucrative treasures are missing)

# So...what do we do?

Once we know, or think we know, what type of missing data mechanism we have we need to figure out how to deal with this! This post will go over five methods, although there a LOT more than that. These are just some common ones that I see/have come across in my field. Each section will cover: overview of what the method is, assumptions (what's required of the method), when to use it and then an example in R.

::: callout-warning
## Covariates vs Outcomes

This post focuses on imputation for covariates, rather than outcomes. The methods are the same, but the plausibility may be different for an outcome variable. Furthermore, it depends on the audience. For example, certain regulatory bodies may prefer several scenarios such as best-case and worst-case imputation.
:::

::: callout-important
## An Introduction to Methods for Missingness

This is meant to be an introduction to some of these methods. A few of them shouldn't have much additional detail to learn about that, such as last observation carried forward or complete cases. However others, such as multiple imputation, there is a litany of information about them. For example, there are entire textbooks on multiple imputation.

The goal of this post is more to be an introduction to missingness, and help identify some potential scenarios to use the different methods while avoiding pitfalls.
:::

# Pirate Treasure Example

Let's simulate some data for our pirate treasure example. We'll assume that

```{r, message = FALSE}
library(tidyverse)

# Simulate a dataset
set.seed(123) # for reproducibility

n = 322

# Pirate Data

df <- data.frame(
  pirate_id = 1:n, 
  treasure_value = runif(n = n, min = 1000, max = 5000), 
  haunted_island = rbinom(n = n, size = 1, prob = 0.5),
  pirate_superstition = rbinom(n = n, size = 1, prob = 0.5)
) 

# MCAR

# Randomly select 50 observations

mcar_indices <- sample(1:nrow(df), 50) # randomly select 20 indices
df$treasure_value_mcar <- df$treasure_value
df$treasure_value_mcar[mcar_indices] <- NA # set these values to NA



# MAR (depends on the value of another variable)
df$treasure_value_mar <- df$treasure_value
is_mar <- df$haunted_island & df$pirate_superstition > 0.5
df$treasure_value_mar[is_mar] <- NA

# MNAR Variable 
df$treasure_value_mnar <- df$treasure_value
mnar_prob <- df$treasure_value / max(df$treasure_value) # higher value, higher chance of being NA
df$treasure_value_mnar[runif(nrow(df)) < mnar_prob] <- NA

# View the data
head(df)

# Writing a Function for Selecting Appropriate Data 

df_missing <- function(type){
  if(type == "mcar"){
    df |> 
      dplyr::select(
        pirate_id, 
        treasure_value, 
        haunted_island, 
        pirate_superstition,
        treasure_value_mcar
      )
  } else if (type == "mar"){
        df |> 
      dplyr::select(
        pirate_id, 
        treasure_value, 
        haunted_island, 
        pirate_superstition,
        treasure_value_mar
      )
  } else if (type == "mnar"){
        df |> 
      dplyr::select(
        pirate_id, 
        treasure_value, 
        haunted_island, 
        pirate_superstition,
        treasure_value_mar
      )
  } else{
    "Error please select one of the types of missing data mechnanism (MCAR, MAR, MNAR)"
  }
}

```

# Complete Cases (Exclude All Missing)

## What is it?

"Let's just get rid of the missing data! Then our problem will be solved!"...not quite. While getting rid of the missing data certainly is *a* way to deal with missing values, you are also losing good information. However, like any method, there is a time and place for it.

## When to Use

Complete case analysis can be valid when only the outcome variable is incomplete and we assume MAR [@austin2021missing], however when we are dealing with covariates there are several disadvantages to this approach.

Unless the data are MAR, the estimated stats and regression coefficients may be biased [@austin2021missing]. Even if the data are MCAR, by reducing the sample size, we are reducing the precision (i.e., confidence intervals will be wider). Another problem is that by using

## Example

```{r}

# Note to self: maybe this should be MCAR? 

df.mar <- df_missing(type = "mar") 

cc <- df.mar |> 
  drop_na()

mean(df.mar$treasure_value)
mean(cc$treasure_value_mar)
```

We can see in this example, that removing all the missing values results in a difference of values. If you compare the datasets then you'll see the differences and the flaws in this. Futhermore, this can drastically reduce your dataset if you only keep the observations that has a value for every value (as a function of the number of variables in your dataset).

## Considerations

Complete case analysis can be convenient from a data quality point of view, however it can cause some issues. For example, is the missing data are MCAR the analysis will have reduced precision due to the reduced sample size but the observed data will not be biased [@jakobsen2017]. If the missing data are not MCAR, the estimate of the effect may be biased [@jakobsen2017].

It's also important to consider which patients are being removed and how many. Is this now the same target population that it was originally? Imagine if ended up removing 25% of our sample! How would that affect our generalizability?\
\
Complete cases can be used in the right context (i.e., if \<5% of sample is missing and data is MCAR \[RYAN ADD REF\]) however it requires some careful thought.

# Last Observation Carried Forward

## What is it?

Last observation carried forward (LOCF) is very much what it sounds like. We take the last value that was observed and use it.

## When to Use

LOCF assumes that the data is MCAR (REF?). If the data are MCAR, the absence of data is unrelated to the study question or the values of the missing data themselves. It can be quite useful when it is plausible, for example when we need to carry through the value of sex (male/female).

## Example

```{r}

# Here we need to have multiple measurements per pirate

# FROM CHATGPT

n_pirates <- 10   # Number of unique pirates
n_measurements <- 5  # Number of measurements per pirate

# Create the data frame
df <- expand.grid(
  pirate_id = 1:n_pirates,
  measurement_id = 1:n_measurements
) %>%
  mutate(
    treasure_value = runif(n = n_pirates * n_measurements, min = 1000, max = 5000),
    haunted_island = rbinom(n = n_pirates * n_measurements, size = 1, prob = 0.5),
    pirate_superstition = rbinom(n = n_pirates * n_measurements, size = 1, prob = 0.5)
  )

# View the dataframe
head(df)


# MY CODE

df.mcar <- df_missing(type = "mcar")

# RYAN: NEED to ADD MORE ROWS, multiple per pirate_id

df.locf <- df.mcar %>% 
  group_by(pirate_id) %>% 
  dplyr::mutate(
    treasure_value = ifelse(is.na(treasure_value_mcar), lag(treasure_value_mcar), treasure_value_mcar)
  )
```

## Considerations

The plausibility of LOCF needs to be considered. For example, if we are studying pirates and have two categories: monsters vs humans, we can safely assume that the monster will still be a monster. An example where this might be a problem is a lab value. Is a lab value that is 30 days prior okay? What about 60, 90 or 400?

Other assumptions include:

-   No dropout

-   Stability of the condition over time

# Mean Value Imputation

## What is it?

Mean value imputation takes the mean of the values that we *do* have and uses this wherever there are missing values. As a side note, the mean is sometimes called the expected value in statistics.

## When to Use

You probably guessed this already, but we can use it when we have a mean! So this can be used for

## Example

```{r}

df.mcar <- df_missing(type = "mcar")

mean(df.mcar$treasure_value_mcar, na.rm = TRUE)

df.mean.imputation <- df.mcar %>% 
  dplyr::mutate(
    new_treasure_value = dplyr::case_when(
      is.na(treasure_value_mcar) ~ mean(df.mcar$treasure_value_mcar, na.rm = TRUE),
      !is.na(treasure_value_mcar) ~ treasure_value_mcar
    )
  )
```

## Considerations

This method is easy to implement and quick but there are some things we need to consider.

Mean value imputation is just what it sounds like. You replace the missing values with the mean. Now, this fairly straightforward but there are some assumptions that we need to consider. This method assumes that the data is MCAR \[REF?\]. Using the mean can

It can be easy to implement

Pro

-   Easy to implement

-   Quick

Con

-   Artifically reduces variation in the data [@austin2021missing]

-   Ignores relationships with other variables [@austin2021missing], which is often true in practice

# Condition mean Imputation (aka using Regression)

## What is it?

Conditional mean imputation is very similar to mean imputation but instead of using the mean, we'll use the conditional mean. How do we know the conditional mean? It's basically the result from our regression.\
\
Basically, you fit a regression model and then use this to predict what the value would be. A perk of this method is that you can fit a regression model including the variables that are associated with other variables.

## When to Use

## Example

## Considerations

"Conditional mean imputation" is similar to mean imputation however, a regression model is used to impute a single value for each missing value [@austin2021missing].

# Multiple Imputation

## What is it?

## When to Use

## Example

## Considerations

# K Nearest Neighbour Imputation

## What is it?

## When to Use

## Example

## Considerations

K nearest neighbour imputation is a mouthful. So let's break it down. Essentially, what happens is you match people to the closest person. Based on this, you assume that their value is the same as this "closest" person. Why K? Well it's doesn't have to just be one person, it could be a couple (RYAN: how does this work)\>

# Summary of Methods

These are a few of the missing data methods that you may come across, however there is a VAST number of methods for missing data. The key takeaway should be to consider these, regardless of the method:

-   What kind of assumptions does it make? (i.e., MCAR, MNAR, MAR)

-   What are the tradeoffs? For example, increase in data quality but decreased in precision and generalizibility? Or increase in precision but decrease in replicability?

-   Practically speaking: is this the right use case for the audience? For example, a regulatory body may have different wants than an eighth grader wanting her help with analyzing her missing pets data.

Hope this blog post was useful as an introduction to five methods to start, but this is only the beginning! Happy missing data exploring!
