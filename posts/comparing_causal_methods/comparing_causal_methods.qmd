---
title: "Comparison is the Thief of Joy"
subtitle: "Unless it's comparing causal methods!"
author: "Ryan Batten"
date: "2024-07-16"
categories: [Simulation, Causal Methods]
#image: "cc_methods.jpeg"
bibliography: cc_methods.bib
draft: true
format: 
  html:
    code-fold: true
---

# Why Compare?

There are a ton of causal methods that are available and only increasing. To name a few: matching, inverse probability of treatment weighting, regression, machine learning methods, parametric g-formula, marginal structural models and many many more. This can make it tricky to figure out which to use. By comparing these quantitatively it can provide some additional evidence for choosing between methods for certain scenarios.

::: callout-note
## Side Note

This post will cover the basics of how to compare different methods used for causal inference. I can't recommend @morris2019using enough. It was extremely helpful when I was first learning how to do this and I continue to use it. As a result, you'll see this article reference throughout this post. A big thank you to Anthony Hatswell for the recommendation!
:::

# First, Walk the DAG

::: callout-note
## Familiar with DAGs?

If you are new to causal inference, I recommend you check out [@rohrer2018thinking] or the ggdag R package to learn more about directed acyclic graphs (DAGs). For the purposes of this post, you can continue reading! Just know that this DAG guides which variables we want to adjust for (and which ones we don't) to best mitigate certain types of bias.
:::

First things first. We need to draw a directed acyclic graph (DAG). This will be needed to determine which variables to adjust for, which ones not to and more. Alright, let's get started!

Wait, we need a research question! The fun part is we get to decide what we want to look at! For this post, we'll look at the effect of ice cream on happiness level. We'll assume that happiness is a continuous measure (since whether someone is happy or not isn't really binary). We also get to decide what kind of variables we want to include! Let's go with:

-   Ice Cream (the exposure)

-   Happiness (the outcome)

-   Sunshine (hours per day)

-   Time with Friends (hours per week)

-   Exercise (hours per day)

```{r, message = FALSE}
library(tidyverse) # ol faithful 
library(ggdag) # used for the DAG 

theme_set(theme_dag()) # setting the theme of the DAG 

# Actually creating the DAG. Adding labels so we know the name of each variable. 

dag <- ggdag::dagify(
  happiness ~ ice_cream + sunshine + time_with_friends + exercise, 
  time_with_friends ~ sunshine, 
  ice_cream ~ sunshine + exercise, 
  exposure = "ice_cream",
  outcome = "happiness",
  labels = c(
    happiness = "Happiness",
    ice_cream = "Ice Cream",
    sunshine = "Sunshine",
    time_with_friends = "Time with Friends",
    exercise = "Exercise"
  )
)

ggdag::ggdag(dag, text = FALSE, use_labels = "label") # actually showing the DAG
```

Great now we have our DAG! Now let's see what we have to adjust for. Luckily we can use the handy dandy ggdag package! Or if you don't use R, there's an online tool dagitty. Either way, let's have a look.

```{r, message = FALSE}

ggdag::ggdag_adjustment_set(dag)
```

Perfect! Looks like we have to adjust for sunshine and exercise. The next step is to think about what exactly we want to estimate.

# Why Causal Estimand...Why?

Individual treatment effects are the ultimate goal but it is very difficult to determine them (at least at the time of this post). Instead, we use an average of the effects. The question becomes the average of who? The overall sample? People who are like those who received the treatment? Choosing this is an important step because it guides who we make inferences about. Quickly, there are basically four different options:

-   Average treatment effect (ATE)

-   Average treatment effect in the treated (ATT)

-   Average treatment effect in the untreated (ATU)

-   Average treatment effect in the overlap (ATO) (aka for those patients that we don't don't know

For this let's use the ATT. In the real-world, we'd put thought into which estimand we want to estimate and not just pick one willy nilly. A great resource for this is @greifer2021choosing.

::: callout-important
## Not all estimands are applicable for all methods 

It's very important to consider this when comparing different methods. A great example of this is comparing propensity score matching (PSM) and inverse probability of treatment weighting (IPTW). PSM can only estimate the ATT because we're matching treated patients to those that are untreated. This results in a sample that looks similar to the treated patients.

If we were to compare this with IPTW (using the default setting in some packages), we could accidentally end up estimating the ATE. Comparing this result with PSM is problematic. Why? Because they're estimating different things!

Not all is lost, with IPTW you can estimate any of the four causal estimands. This isn't true for all methods, so we need to make sure it's the same causal estimand.
:::

# "Better" Scientifically

Great, so we're ready to compare! Right?! GIMME SOME DATA! Not quite yet but almost there! We need to decide how exactly we're going to quantify the difference. This essentially comes down to the research question we're asking. Do we want to know if one approach results in a model with a better fit? Or do we want to know which model has lower variance? Maybe we want to know which one has a higher power. For this example, we'll use the mean squared error (MSE) and relative percent increase in precision.

::: callout-note
## Choosing Measure

I highly, HIGHLY recommend the article by @morris2019using for more about choices in comparing methods. It's my personal go-to when I'm doing anything involving simulation.
:::

# Methods, Methods, Methods!

Methods, methods, methods! Unlike Beetlejuice, saying it three times won't make a method appear. So we need to pick the two methods that we want to compare. Now in reality, these don't usually happen sequentially. There may be two methods that you want to compare, so you then work backwards to draw a DAG a made-up situation (or one based on a real causal hypothesis). Regardless, it's important to know what two methods you are using and how they differ in how they work.

For example, it could be helpful to compare inverse probability of treatment weighting and the parametric g-formula. Both can estimate the average treatment effect, but they do so differently. IPTW models the treatment whereas the parametric g-formula models the outcome. It would be expected that these wouldn't give the exact same result. Although they could still be compared since they should be similar. Essentially, it's important to know how the methods may differ and what would be expected versus unexpected.

# Finally an Example! With Data!

Alright, let's put it all together with some simulation! We're going to use the DAG from before to compare IPTW and PSM.

## Simulating Data

Time to simulate some data! We'll use our handy dandy DAG from before to guide us on this.

```{r}



# arbitrary sample size. This can sometimes be altered as well, if we want to compare the difference in methods as a function of sample size. For example, maybe we want to see if PSM and IPTW become the same (or simliar enough) as sample size increases

n = 500 # arbitrary 
theta = 3 # this is what the actual effect is (we'll use this later)

df = data.frame(
  sunshine = rnorm(n = n, mean = 10, sd = 2),
  exercise = rnorm(n = n, mean = 1, sd = 0.25)
) %>% 
  dplyr::mutate(
    time_with_friends = 2 + 1.5*sunshine, # assume total hours per week 
    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),
    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise
  ) 

```

Alright now let's test fitting a version for each method. Personally, I do this first before getting into repeating it. This is just to make sure nothing is a miss

```{r}

library(WeightIt)
library(MatchIt)
library(broom)

matched <- MatchIt::matchit(ice_cream ~ sunshine, 
                 data = df, 
                 method = "nearest", 
                 estimand = "ATT")

mod <- stats::glm(
  happiness ~ ice_cream, 
  data = match.data(matched)
)

broom::tidy(mod)


ipw <- WeightIt::weightit(ice_cream ~ sunshine, 
                          data = df,
                          method = "glm", 
                          estimand = "ATT")

mod_ipw <- stats::glm(
  happiness ~ ice_cream, 
  data = df,
  weights = ipw$weights
)

```

## Repeat, Repeat, Repeat

Repeating for each of these. First calculating the results for IPTW, then for PSM

```{r}

# PSM

simulate_psm <- function(n, theta) {
  df = data.frame(
    sunshine = rnorm(n = n, mean = 10, sd = 2),
    exercise = rnorm(n = n, mean = 1, sd = 0.25)
) %>% 
  dplyr::mutate(
    time_with_friends = 2 + 1.5*sunshine, # assume total hours per week 
    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),
    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise
  ) 
  
  psm <- MatchIt::matchit(ice_cream ~ sunshine + exercise, 
                 data = df, 
                 method = "nearest", 
                 estimand = "ATT")
  
  outcome_model <- glm(happiness ~ ice_cream, 
                       data = match.data(psm))
  
  estimate <- broom::tidy(outcome_model) %>% 
    dplyr::filter(term == "ice_cream") %>% 
    pull(estimate)
  
  return(estimate)
}



psm_output <- replicate(1000, simulate_psm(500, 3), simplify = FALSE) 

psm_output <- do.call(rbind, psm_output) %>%  # reformatting 
  as.data.frame()

psm_output$theta_hat <- psm_output$V1 

psm_result <- psm_output %>% 
  mutate(
    error = theta_hat - theta,
    squared_error = error^2
  )

avg_theta = mean(psm_result$theta_hat)
mse = (1/1000)*sum(psm_result$squared_error)

psm_se_result = psm_result %>% 
  mutate(
    se_mse = mse,
    squared_diff = (theta_hat - avg_theta)^2
  )

psm_emp_se <- sqrt((1/(1000-1))*sum(psm_se_result$squared_diff))



```

```{r}

simulate_ipw <- function(n, theta) {
  df = data.frame(
    sunshine = rnorm(n = n, mean = 10, sd = 2),
    exercise = rnorm(n = n, mean = 1, sd = 0.25)
) %>% 
  dplyr::mutate(
    time_with_friends = 2 + 1.5*sunshine, # assume total hours per week 
    ice_cream = rbinom(n = n, size = 1, prob = 0.01 + 0.0167*sunshine + 0.2*exercise),
    happiness = theta*ice_cream + 2*sunshine + 1.5*time_with_friends + 2.5*exercise
  ) 
  
  ipw <- WeightIt::weightit(ice_cream ~ sunshine + exercise, 
                 data = df, 
                 estimand = "ATT",
                 method = "glm")
  
  outcome_model <- glm(happiness ~ ice_cream, 
                       data = df, 
                       weights = ipw$weights)
  
  estimate <- broom::tidy(outcome_model) %>% 
    dplyr::filter(term == "ice_cream") %>% 
    pull(estimate)
  
  return(estimate)
}



ipw_output <- replicate(1000, simulate_ipw(500, 3), simplify = FALSE) 

ipw_output <- do.call(rbind, ipw_output) %>%  # reformatting 
  as.data.frame()

ipw_output$theta_hat <- ipw_output$V1 

ipw_result <- ipw_output %>% 
  mutate(
    error = theta_hat - theta,
    squared_error = error^2
  )

avg_theta = mean(ipw_result$theta_hat)
mse = (1/1000)*sum(ipw_result$squared_error)

ipw_se_result = ipw_result %>% 
  mutate(
    se_mse = mse,
    squared_diff = (theta_hat - avg_theta)^2
  )

ipw_emp_se <- sqrt((1/(1000-1))*sum(ipw_se_result$squared_diff))

```

```{r}

increase_precision <- round(100*((ipw_emp_se/psm_emp_se)^2 - 1 ),2) # Increase in precision for IPTW vs PSM

corr = cor(ipw_result$theta_hat, psm_result$theta_hat)

ratio <- ipw_emp_se/psm_emp_se

se_increase_precision <- round(200*(ratio^2)*sqrt((1-(corr^2))/(1000-1)), 2)


```

Now we have some results! The MSE for PSM is `r round(psm_emp_se,2)` and `r round(ipw_emp_se,2)`for IPTW. The increase in precision is `r paste0(increase_precision, " (", se_increase_precision, ")")`. We need to unpack these results since there are a few things to keep in mind.

Firstly, we didn't assess the balance. When comparing two methods, we should also compare how the balance between the methods looks. Does one method result in better balance or no? This can be done graphically and/or using love plots (recommend the cobalt package for this). Secondly, the same size was 500. Perhaps, more importantly, is the number of patients getting matched. Do we have enough controls for our treatment? What if we didn't, what if we did? Thirdly, there were only two confounders. How would the results change if we included more?

When comparing results it's important to not just look numerically. There are scientific limitations of certain methods. For example, PSM only keeps patients with a match. Would we be sure that these patients match our inclusion/exclusion criteria? Or is selection bias being induced? We need to incorporate these pieces when comparing methods.

This isn't to say that the above is necessarily wrong. It's just important to consider the limitations of what we did do. As put in @morris2019using, starting simple is the best and I agree. We can now build on this!

::: callout-caution
## Metrics That Rely on Standard Error

We should use caution when using using metrics relying on standard error. For PS-based methods, we need to use appropriate variance estimators. This can make it tricky when using certain metrics. Imagine we are comparing IPTW and PSM based on power for nominal coverage. Both of these need SE. If we're using robust variance estimation (HC0) for PSM and M-estimation for IPTW, are these really comparing IPTW vs PSM? Or would the different variance estimation techniques have something to do with the differences? It's important to consider these differences.
:::

# Why Bother?

At this point, you may be thinking "why both to do this? This is a lot of work when I can just say one over the other". The answer is two-fold. Firstly, it's not always clear what the best approach is. It can depend on the setting and outcome (continuous, binary, time-to-event). It's helpful to quantify this when discussing with a team. Secondly, when we compare methods it's helpful to know "how much". One method may have higher precision but how much is "higher". Using different metrics also helps to figure out what should be more emphasized (i.e., precision vs bias, etc).

For me, learning how to simulate data and do these types of comparisons has helped tremendously. Not only in making decisions, but also in understanding methods. Hope you found this post helpful! Happy causal method comparing!
